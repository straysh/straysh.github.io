{"meta":{"title":"Straysh的小院","subtitle":"","description":"","author":"Straysh","url":"https://straysh.github.life","root":"/"},"pages":[{"title":"个人简介","date":"2019-12-01T10:39:42.000Z","updated":"2020-02-10T13:43:08.822Z","comments":false,"path":"about/index.html","permalink":"https://straysh.github.life/about/index.html","excerpt":"","text":"转眼到了2020年。 2017年是我职业生涯的一个转折，17年以前一直在做传统的PHP开发（Web + App），带着一个小规模的团队（3~5人）。埋头于功能的实现，少有性能的压力。 17年后，进入到区块链行业，熬了两个月，带着团队做出了第一款数字货币钱包。之后在币圈摸爬滚打，起起伏伏，无奈的是不到2年经历了3个东家。在这段时间，深入了Golang的开发，研究数字货币钱包。期间与众多大神共事，学习良多，感慨良多，加深了对基础的认知，算法、数据结构、数学知识、团队的协作、沟通的重要…… 最后随着币圈沉寂，回到传统互联网圈子，目前在出行领域。 而此时，我已不再年轻。甚幸的是刚结完婚，新房也快装修结束。 稍感中年危机，技术上也到了瓶颈期。一直从事0-1的产品实现，欠缺高质量的锤炼。希望下次能找到一个稳定的平台，给我北漂的职业生涯画上一个完美的句号！ 主子镇楼 曹庭汉，男，87年生，身高165，体重110，未婚，籍贯湖北，现居北京朝阳。 此前在湖北武汉从事网络维护，工作无甚压力，颇感蹉跎青春，激情渐消。2011年开始接触web开发，兴趣浓厚，亦有志于此。遂来京于【LAMP兄弟连】培训约半年。 2012.8-2013.9 在新影数讯科技有限公司工作。期间，担任PHP研发工程师一职。主要工作：后台管理系统（CMS）的研发，部分社交媒体、影视站点数据采集、分析，微信公众平台以及少量新浪微博APP的开发。 一年后，逐渐产生触到天花板的感觉，大数据这个方向并不适合我这样一个初出茅庐的coder。决定转向web开发。 2013.10-2015.9 经朋友推荐，入职中儒泰纳传媒。从一个普通的phper逐渐成长为Tech Leader，向web方向的fullstack developer迈进一步。期间，负责整个项目的技术选型/架构、网络爬虫、前端js开发、网站后端（php）、移动端API（php&amp;Nodejs），以及服务器的整体运维。 一转眼2年过去，天花板渐现，对于一个非计算机专业的coder，我已然非常清晰的认识到自己缺少的部分。一番准备后，参加了成考，期望进入北邮弥补基础理论的缺失。同时，跟随我们Team的一位股东sonic以及另一位IOS工程师xx11Dragon，重新起航，创办了共同的信仰“人生菜单（北京）科技有限公司”。 2015.9 至今。创业是艰难的。在这段不长的时间里，开始向“T型”工程师转变，一（一横）是涉猎广泛：linux、C/C++、Java、Python、Android、R、Go、Erlang、Lua，虽是泛泛，收货匪浅，一（一竖）是深研本职技术：php、js、nodejs。 写了4年代码，激情越盛。唯尚有一憾：惯性单身。 技能清单 熟练掌握Golang，Nodejs，PHP。 熟练掌握Gin(Golang后端Restfull框架)， express(NodeJs后端框架)，Laravel、Yii、ThinkPHP等PHP Web框架。 熟练掌握MySQL、MongoDB、Memcache、Ngnix、Redis、RabbitMQ。 熟练掌握MySQL事务、存储过程、索引优化、分区等相关技术。 熟练使用SVN、GIT版本控制器。 熟练掌握html、css、js(jQuery)以及ajax等前端技术。 熟练NodeJs后端技术栈， React-Native移动端技术栈。 熟悉Linux操作系统、bash脚本、lua脚本，能够在Linux环境下熟练开发。 联系方式： Mail:jobhancao@gmail.com Straysh的Github"},{"title":"categories","date":"2019-12-01T10:27:12.000Z","updated":"2019-12-01T10:28:53.214Z","comments":true,"path":"categories/index.html","permalink":"https://straysh.github.life/categories/index.html","excerpt":"","text":""},{"title":"books","date":"2019-12-01T10:31:23.000Z","updated":"2019-12-01T10:31:52.626Z","comments":false,"path":"books/index.html","permalink":"https://straysh.github.life/books/index.html","excerpt":"","text":""},{"title":"links","date":"2019-12-01T10:39:01.000Z","updated":"2019-12-01T10:39:18.921Z","comments":true,"path":"links/index.html","permalink":"https://straysh.github.life/links/index.html","excerpt":"","text":""},{"title":"repository","date":"2019-12-01T10:29:48.000Z","updated":"2019-12-01T10:31:00.490Z","comments":false,"path":"repository/index.html","permalink":"https://straysh.github.life/repository/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-12-01T10:28:45.000Z","updated":"2019-12-01T10:29:02.834Z","comments":true,"path":"tags/index.html","permalink":"https://straysh.github.life/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"使用Go操作文件","slug":"使用Go操作文件","date":"2022-03-14T12:11:25.000Z","updated":"2022-03-15T02:23:23.963Z","comments":true,"path":"2022/03/14/使用Go操作文件/","link":"","permalink":"https://straysh.github.life/2022/03/14/%E4%BD%BF%E7%94%A8Go%E6%93%8D%E4%BD%9C%E6%96%87%E4%BB%B6/","excerpt":"","text":"一切皆文件UNIX世界的一个基石观点是一切皆文件。我们不需要知道文件描述符映射的，由操作系统的设备驱动抽象出来的是什么。操作系统以文件的形式向我们提供了访问设备的接口。 Go语言中的读/写接口也类似。我们仅仅简单的读/写字节，并不需要理解reader从何处及如何读数据，或者writer将数据写入到了何处。在/dev下能找到可用的设备。有些可能需要提权访问。 1. 基础文件操作1.1 创建空文件1234567891011121314151617181920package mainimport ( &quot;log&quot; &quot;os&quot;)var ( newFile *os.File err error)func main() &#123; newFile, err = os.Create(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; log.Println(newFile) newFile.Close()&#125; 1.2 截断文件（Truncate）123456789101112131415161718package mainimport ( &quot;log&quot; &quot;os&quot;)func main() &#123; // 截断文件至100字节。如果文件小于100字节，则原始内容保留在前并在其后补null字节。 // 如果文件大于100字节，超过100字节的不删将丢弃。 // 那么不论哪种情况，我们都将得到长度为100字节的内容。 // 第二个参数传0，则将清空整个文件。 err := os.Truncate(&quot;test.txt&quot;, 100) if err != nil &#123; log.Fatal(err) &#125;&#125; 1.3 获取文件信息12345678910111213141516171819202122232425262728package mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;os&quot;)var ( fileInfo os.FileInfo err error)func main() &#123; // Stat returns file info. It will return // an error if there is no file. fileInfo, err = os.Stat(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;File name:&quot;, fileInfo.Name()) // File name: test.txt fmt.Println(&quot;Size in bytes:&quot;, fileInfo.Size()) // Size in bytes: 12 fmt.Println(&quot;Permissions:&quot;, fileInfo.Mode()) // Permissions: -rw-r--r-- fmt.Println(&quot;Last modified:&quot;, fileInfo.ModTime()) // Last modified: 2022-03-14 20:42:22.440137408 +0800 CST fmt.Println(&quot;Is Directory: &quot;, fileInfo.IsDir()) // Is Directory: false fmt.Printf(&quot;System interface type: %T\\n&quot;, fileInfo.Sys()) // System interface type: *syscall.Stat_t fmt.Printf(&quot;System info: %+v\\n\\n&quot;, fileInfo.Sys()) // System info: &amp;&#123;Dev:66306 Ino:11021648 ......&#125; 1.4 重命名/移动文件123456789101112131415package mainimport ( &quot;log&quot; &quot;os&quot;)func main() &#123; originalPath := &quot;test.txt&quot; newPath := &quot;test2.txt&quot; err := os.Rename(originalPath, newPath) if err != nil &#123; log.Fatal(err) &#125;&#125; 1.5 删除文件12345678910111213package mainimport ( &quot;log&quot; &quot;os&quot;)func main() &#123; err := os.Remove(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125;&#125; ## 1.6 打开/关闭文件123456789101112131415161718192021222324252627282930313233343536package mainimport ( &quot;log&quot; &quot;os&quot;)func main() &#123; // Simple read only open. We will cover actually reading // and writing to files in examples further down the page file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; file.Close() // OpenFile with more options. Last param is the permission mode // Second param is the attributes when opening file, err = os.OpenFile(&quot;test.txt&quot;, os.O_APPEND, 0666) if err != nil &#123; log.Fatal(err) &#125; file.Close() // Use these attributes individually or combined // with an OR for second arg of OpenFile() // e.g. os.O_CREATE|os.O_APPEND // or os.O_CREATE|os.O_TRUNC|os.O_WRONLY // os.O_RDONLY // Read only // os.O_WRONLY // Write only // os.O_RDWR // Read and write // os.O_APPEND // Append to end of file // os.O_CREATE // Create is none exist // os.O_TRUNC // Truncate file when opening&#125; 1.7 检查文件是否存在1234567891011121314151617181920212223package mainimport ( &quot;log&quot; &quot;os&quot;)var ( fileInfo *os.FileInfo err error)func main() &#123; // Stat returns file info. It will return // an error if there is no file. fileInfo, err := os.Stat(&quot;test.txt&quot;) if err != nil &#123; if os.IsNotExist(err) &#123; log.Fatal(&quot;File does not exist.&quot;) &#125; &#125; log.Printf(&quot;File does exist. File information:\\n%s&quot;, fileInfo)&#125; 1.8 检查读/写权限12345678910111213141516171819202122232425262728package mainimport ( &quot;log&quot; &quot;os&quot;)func main() &#123; // Test write permissions. It is possible the file // does not exist and that will return a different // error that can be checked with os.IsNotExist(err) file, err := os.OpenFile(&quot;test.txt&quot;, os.O_WRONLY, 0666) if err != nil &#123; if os.IsPermission(err) &#123; log.Println(&quot;Error: Write permission denied.&quot;) &#125; &#125; file.Close() // Test read permissions file, err = os.OpenFile(&quot;test.txt&quot;, os.O_RDONLY, 0666) if err != nil &#123; if os.IsPermission(err) &#123; log.Println(&quot;Error: Read permission denied.&quot;) &#125; &#125; file.Close()&#125; 1.9 修改权限/拥有者/时间戳123456789101112131415161718192021222324252627282930package mainimport ( &quot;log&quot; &quot;os&quot; &quot;time&quot;)func main() &#123; // Change perrmissions using Linux style err := os.Chmod(&quot;test.txt&quot;, 0777) if err != nil &#123; log.Println(err) &#125; // Change ownership err = os.Chown(&quot;test.txt&quot;, os.Getuid(), os.Getgid()) if err != nil &#123; log.Println(err) &#125; // Change timestamps twoDaysFromNow := time.Now().Add(48 * time.Hour) lastAccessTime := twoDaysFromNow lastModifyTime := twoDaysFromNow err = os.Chtimes(&quot;test.txt&quot;, lastAccessTime, lastModifyTime) if err != nil &#123; log.Println(err) &#125;&#125; 1.10 创建硬链接/软链接典型的文件就是一个指向硬盘某个位置的指针，我们称之为inode。硬链接会创建一个新的指针，指向相同的位置。只有当所有链接都删除时，文件才会真的被从硬盘上物理删除。硬链接只能在相同的文件系统中创建。符号链接或软链接，稍有不同，它并不直接指向磁盘上的位置。软链接仅仅通过文件名引用其他文件。它可以指向不同的文件系统上的文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( &quot;os&quot; &quot;log&quot; &quot;fmt&quot;)func main() &#123; // Create a hard link // You will have two file names that point to the same contents // Changing the contents of one will change the other // Deleting/renaming one will not affect the other err := os.Link(&quot;original.txt&quot;, &quot;original_also.txt&quot;) if err != nil &#123; log.Fatal(err) &#125;fmt.Println(&quot;creating sym&quot;) // Create a symlink err = os.Symlink(&quot;original.txt&quot;, &quot;original_sym.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; // Lstat will return file info, but if it is actually // a symlink, it will return info about the symlink. // It will not follow the link and give information // about the real file // Symlinks do not work in Windows fileInfo, err := os.Lstat(&quot;original_sym.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Link info: %+v&quot;, fileInfo) // Change ownership of a symlink only // and not the file it points to err = os.Lchown(&quot;original_sym.txt&quot;, os.Getuid(), os.Getgid()) if err != nil &#123; log.Fatal(err) &#125;&#125; 读/写文件2.1 复制文件1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;os&quot; &quot;log&quot; &quot;io&quot;)// Copy a filefunc main() &#123; // Open original file originalFile, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; defer originalFile.Close() // Create new file newFile, err := os.Create(&quot;test_copy.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; defer newFile.Close() // Copy the bytes to destination from source bytesWritten, err := io.Copy(newFile, originalFile) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Copied %d bytes.&quot;, bytesWritten) // Commit the file contents // Flushes memory to disk err = newFile.Sync() if err != nil &#123; log.Fatal(err) &#125;&#125; 2.2 跳至指定位置（Seek）12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( &quot;os&quot; &quot;fmt&quot; &quot;log&quot;)func main() &#123; file, _ := os.Open(&quot;test.txt&quot;) defer file.Close() // Offset is how many bytes to move // Offset can be positive or negative var offset int64 = 5 // Whence is the point of reference for offset // 0 = Beginning of file // 1 = Current position // 2 = End of file var whence int = 0 newPosition, err := file.Seek(offset, whence) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Just moved to 5:&quot;, newPosition) // Go back 2 bytes from current position newPosition, err = file.Seek(-2, 1) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Just moved back two:&quot;, newPosition) // Find the current position by getting the // return value from Seek after moving 0 bytes currentPosition, err := file.Seek(0, 1) fmt.Println(&quot;Current position:&quot;, currentPosition) // Go to beginning of file newPosition, err = file.Seek(0, 0) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Position after seeking 0,0:&quot;, newPosition)&#125; 2.3 向文件写入字节123456789101112131415161718192021222324252627package mainimport ( &quot;os&quot; &quot;log&quot;)func main() &#123; // Open a new file for writing only file, err := os.OpenFile( &quot;test.txt&quot;, os.O_WRONLY|os.O_TRUNC|os.O_CREATE, 0666, ) if err != nil &#123; log.Fatal(err) &#125; defer file.Close() // Write bytes to file byteSlice := []byte(&quot;Bytes!\\n&quot;) bytesWritten, err := file.Write(byteSlice) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Wrote %d bytes.\\n&quot;, bytesWritten)&#125; 2.4 快速写入文件12345678910111213package mainimport ( &quot;io/ioutil&quot; &quot;log&quot;)func main() &#123; err := ioutil.WriteFile(&quot;test.txt&quot;, []byte(&quot;Hi\\n&quot;), 0666) if err != nil &#123; log.Fatal(err) &#125;&#125; 2.5 使用缓冲写（Buffered Writer）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package mainimport ( &quot;log&quot; &quot;os&quot; &quot;bufio&quot;)func main() &#123; // Open file for writing file, err := os.OpenFile(&quot;test.txt&quot;, os.O_WRONLY, 0666) if err != nil &#123; log.Fatal(err) &#125; defer file.Close() // Create a buffered writer from the file bufferedWriter := bufio.NewWriter(file) // Write bytes to buffer bytesWritten, err := bufferedWriter.Write( []byte&#123;65, 66, 67&#125;, ) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Bytes written: %d\\n&quot;, bytesWritten) // Write string to buffer // Also available are WriteRune() and WriteByte() bytesWritten, err = bufferedWriter.WriteString( &quot;Buffered string\\n&quot;, ) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Bytes written: %d\\n&quot;, bytesWritten) // Check how much is stored in buffer waiting unflushedBufferSize := bufferedWriter.Buffered() log.Printf(&quot;Bytes buffered: %d\\n&quot;, unflushedBufferSize) // See how much buffer is available bytesAvailable := bufferedWriter.Available() if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Available buffer: %d\\n&quot;, bytesAvailable) // Write memory buffer to disk bufferedWriter.Flush() // Revert any changes done to buffer that have // not yet been written to file with Flush() // We just flushed, so there are no changes to revert // The writer that you pass as an argument // is where the buffer will output to, if you want // to change to a new writer bufferedWriter.Reset(bufferedWriter) // See how much buffer is available bytesAvailable = bufferedWriter.Available() if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Available buffer: %d\\n&quot;, bytesAvailable) // Resize buffer. The first argument is a writer // where the buffer should output to. In this case // we are using the same buffer. If we chose a number // that was smaller than the existing buffer, like 10 // we would not get back a buffer of size 10, we will // get back a buffer the size of the original since // it was already large enough (default 4096) bufferedWriter = bufio.NewWriterSize( bufferedWriter, 8000, ) // Check available buffer size after resizing bytesAvailable = bufferedWriter.Available() if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Available buffer: %d\\n&quot;, bytesAvailable)&#125; 最多读文件的n个字节1234567891011121314151617181920212223242526package mainimport ( &quot;os&quot; &quot;log&quot;)func main() &#123; // Open file for reading file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; defer file.Close() // Read up to len(b) bytes from the File // Zero bytes written means end of file // End of file returns error type io.EOF byteSlice := make([]byte, 16) bytesRead, err := file.Read(byteSlice) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Number of bytes read: %d\\n&quot;, bytesRead) log.Printf(&quot;Data read: %s\\n&quot;, byteSlice)&#125; 2.7 只读取文件的n个字节1234567891011121314151617181920212223242526package mainimport ( &quot;os&quot; &quot;log&quot; &quot;io&quot;)func main() &#123; // Open file for reading file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; // The file.Read() function will happily read a tiny file in to a large // byte slice, but io.ReadFull() will return an // error if the file is smaller than the byte slice. byteSlice := make([]byte, 2) numBytesRead, err := io.ReadFull(file, byteSlice) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Number of bytes read: %d\\n&quot;, numBytesRead) log.Printf(&quot;Data read: %s\\n&quot;, byteSlice)&#125; 2.8 至少读取文件的n个字节123456789101112131415161718192021222324252627package mainimport ( &quot;os&quot; &quot;log&quot; &quot;io&quot;)func main() &#123; // Open file for reading file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; byteSlice := make([]byte, 512) minBytes := 8 // io.ReadAtLeast() will return an error if it cannot // find at least minBytes to read. It will read as // many bytes as byteSlice can hold. numBytesRead, err := io.ReadAtLeast(file, byteSlice, minBytes) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Number of bytes read: %d\\n&quot;, numBytesRead) log.Printf(&quot;Data read: %s\\n&quot;, byteSlice)&#125; 2.9 读文件的全部字节1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;os&quot; &quot;log&quot; &quot;fmt&quot; &quot;io/ioutil&quot;)func main() &#123; // Open file for reading file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; // os.File.Read(), io.ReadFull(), and // io.ReadAtLeast() all work with a fixed // byte slice that you make before you read // ioutil.ReadAll() will read every byte // from the reader (in this case a file), // and return a slice of unknown slice data, err := ioutil.ReadAll(file) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Data as hex: %x\\n&quot;, data) fmt.Printf(&quot;Data as string: %s\\n&quot;, data) fmt.Println(&quot;Number of bytes read:&quot;, len(data))&#125; 2.10 快速读取全部文件到内存12345678910111213141516package mainimport ( &quot;log&quot; &quot;io/ioutil&quot;)func main() &#123; // Read file to byte slice data, err := ioutil.ReadFile(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Data read: %s\\n&quot;, data)&#125; 2.11 使用缓冲读（Buffered Reader）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( &quot;os&quot; &quot;log&quot; &quot;bufio&quot; &quot;fmt&quot;)func main() &#123; // Open file and create a buffered reader on top file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; bufferedReader := bufio.NewReader(file) // Get bytes without advancing pointer byteSlice := make([]byte, 5) byteSlice, err = bufferedReader.Peek(5) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Peeked at 5 bytes: %s\\n&quot;, byteSlice) // Read and advance pointer numBytesRead, err := bufferedReader.Read(byteSlice) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Read %d bytes: %s\\n&quot;, numBytesRead, byteSlice) // Ready 1 byte. Error if no byte to read myByte, err := bufferedReader.ReadByte() if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Read 1 byte: %c\\n&quot;, myByte) // Read up to and including delimiter // Returns byte slice dataBytes, err := bufferedReader.ReadBytes(&#x27;\\n&#x27;) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Read bytes: %s\\n&quot;, dataBytes) // Read up to and including delimiter // Returns string dataString, err := bufferedReader.ReadString(&#x27;\\n&#x27;) if err != nil &#123; log.Fatal(err) &#125; fmt.Printf(&quot;Read string: %s\\n&quot;, dataString) // This example reads a few lines so test.txt // should have a few lines of text to work correct&#125; 2.12 使用Scanner读bufio包有一个Scanner，它提供了一种方法使用指定的分隔符来逐项迭代文件内容。默认情况下，使用换行符newline来将文件拆分为若干行。在CSV文件中，使用逗号作为分隔符。os.File对象可以使用bufio.Scanner包装，用起来和缓存读相似。调用Scan函数读取下一行，调用Text()或Bytes()读取内容。分隔符不是一个简单的字节或字符，有一个特殊的函数用来决定符合分隔，计算从何处开始算作下一行，指针向前移动多少，返回什么数据。若未提供自定义的SplitFunc函数，默认使用ScanLines函数。另外bufio包提供了ScanRunes和ScanWords函数。 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;os&quot; &quot;log&quot; &quot;fmt&quot; &quot;bufio&quot;)func main() &#123; // Open file and create scanner on top of it file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; scanner := bufio.NewScanner(file) // Default scanner is bufio.ScanLines. Lets use ScanWords. // Could also use a custom function of SplitFunc type scanner.Split(bufio.ScanWords) // Scan for next token. success := scanner.Scan() if success == false &#123; // False on error or EOF. Check error err = scanner.Err() if err == nil &#123; log.Println(&quot;Scan completed and reached EOF&quot;) &#125; else &#123; log.Fatal(err) &#125; &#125; // Get data from scan with Bytes() or Text() fmt.Println(&quot;First word found:&quot;, scanner.Text()) // Call scanner.Scan() again to find next token&#125; 3. 归档3.1 Zip归档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// This example uses zip but standard library// also supports tar archivespackage mainimport ( &quot;archive/zip&quot; &quot;log&quot; &quot;os&quot;)func main() &#123; // Create a file to write the archive buffer to // Could also use an in memory buffer. outFile, err := os.Create(&quot;test.zip&quot;) if err != nil &#123; log.Fatal(err) &#125; defer outFile.Close() // Create a zip writer on top of the file writer zipWriter := zip.NewWriter(outFile) // Add files to archive // We use some hard coded data to demonstrate, // but you could iterate through all the files // in a directory and pass the name and contents // of each file, or you can take data from your // program and write it write in to the archive // without var filesToArchive = []struct &#123; Name, Body string &#125; &#123; &#123;&quot;test.txt&quot;, &quot;String contents of file&quot;&#125;, &#123;&quot;test2.txt&quot;, &quot;\\x61\\x62\\x63\\n&quot;&#125;, &#125; // Create and write files to the archive, which in turn // are getting written to the underlying writer to the // .zip file we created at the beginning for _, file := range filesToArchive &#123; fileWriter, err := zipWriter.Create(file.Name) if err != nil &#123; log.Fatal(err) &#125; _, err = fileWriter.Write([]byte(file.Body)) if err != nil &#123; log.Fatal(err) &#125; &#125; // Clean up err = zipWriter.Close() if err != nil &#123; log.Fatal(err) &#125;&#125; 3.2 Zip解压123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// This example uses zip but standard library// also supports tar archivespackage mainimport ( &quot;archive/zip&quot; &quot;log&quot; &quot;io&quot; &quot;os&quot; &quot;path/filepath&quot;)func main() &#123; // Create a reader out of the zip archive zipReader, err := zip.OpenReader(&quot;test.zip&quot;) if err != nil &#123; log.Fatal(err) &#125; defer zipReader.Close() // Iterate through each file/dir found in for _, file := range zipReader.Reader.File &#123; // Open the file inside the zip archive // like a normal file zippedFile, err := file.Open() if err != nil &#123; log.Fatal(err) &#125; defer zippedFile.Close() // Specify what the extracted file name should be. // You can specify a full path or a prefix // to move it to a different directory. // In this case, we will extract the file from // the zip to a file of the same name. targetDir := &quot;./&quot; extractedFilePath := filepath.Join( targetDir, file.Name, ) // Extract the item (or create directory) if file.FileInfo().IsDir() &#123; // Create directories to recreate directory // structure inside the zip archive. Also // preserves permissions log.Println(&quot;Creating directory:&quot;, extractedFilePath) os.MkdirAll(extractedFilePath, file.Mode()) &#125; else &#123; // Extract regular file since not a directory log.Println(&quot;Extracting file:&quot;, file.Name) // Open an output file for writing outputFile, err := os.OpenFile( extractedFilePath, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, file.Mode(), ) if err != nil &#123; log.Fatal(err) &#125; defer outputFile.Close() // &quot;Extract&quot; the file by copying zipped file // contents to the output file _, err = io.Copy(outputFile, zippedFile) if err != nil &#123; log.Fatal(err) &#125; &#125; &#125;&#125; 4. 压缩4.1 文件压缩123456789101112131415161718192021222324252627282930313233343536// This example uses gzip but standard library also// supports zlib, bz2, flate, and lzwpackage mainimport ( &quot;os&quot; &quot;compress/gzip&quot; &quot;log&quot;)func main() &#123; // Create .gz file to write to outputFile, err := os.Create(&quot;test.txt.gz&quot;) if err != nil &#123; log.Fatal(err) &#125; // Create a gzip writer on top of file writer gzipWriter := gzip.NewWriter(outputFile) defer gzipWriter.Close() // When we write to the gzip writer // it will in turn compress the contents // and then write it to the underlying // file writer as well // We don&#x27;t have to worry about how all // the compression works since we just // use it as a simple writer interface // that we send bytes to _, err = gzipWriter.Write([]byte(&quot;Gophers rule!\\n&quot;)) if err != nil &#123; log.Fatal(err) &#125; log.Println(&quot;Compressed data written to file.&quot;) &#125; 4.1 文件解压缩1234567891011121314151617181920212223242526272829303132333435363738394041424344// This example uses gzip but standard library also// supports zlib, bz2, flate, and lzwpackage mainimport ( &quot;compress/gzip&quot; &quot;log&quot; &quot;io&quot; &quot;os&quot;)func main() &#123; // Open gzip file that we want to uncompress // The file is a reader, but we could use any // data source. It is common for web servers // to return gzipped contents to save bandwidth // and in that case the data is not in a file // on the file system but is in a memory buffer gzipFile, err := os.Open(&quot;test.txt.gz&quot;) if err != nil &#123; log.Fatal(err) &#125; // Create a gzip reader on top of the file reader // Again, it could be any type reader though gzipReader, err := gzip.NewReader(gzipFile) if err != nil &#123; log.Fatal(err) &#125; defer gzipReader.Close() // Uncompress to a writer. We&#x27;ll use a file writer outfileWriter, err := os.Create(&quot;unzipped.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; defer outfileWriter.Close() // Copy contents of gzipped file to output file _, err = io.Copy(outfileWriter, gzipReader) if err != nil &#123; log.Fatal(err) &#125;&#125; 5. 其他操作5.1 临时文件和目录ioutil包提提供了两个函数TempDir()和TempFile()。使用完毕后，调用方需要负责主动删除临时文件。这两个函数唯一提供的一项便利是在传空参数时，它会在系统的/tmp目录下自动创建。 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( &quot;os&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;fmt&quot;)func main() &#123; // Create a temp dir in the system default temp folder tempDirPath, err := ioutil.TempDir(&quot;&quot;, &quot;myTempDir&quot;) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Temp dir created:&quot;, tempDirPath) // Create a file in new temp directory tempFile, err := ioutil.TempFile(tempDirPath, &quot;myTempFile.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Temp file created:&quot;, tempFile.Name()) // ... do something with temp file/dir ... // Close file err = tempFile.Close() if err != nil &#123; log.Fatal(err) &#125; // Delete the resources we created err = os.Remove(tempFile.Name()) if err != nil &#123; log.Fatal(err) &#125; err = os.Remove(tempDirPath) if err != nil &#123; log.Fatal(err) &#125;&#125; 5.2 从HTTP下载文件123456789101112131415161718192021222324252627282930313233package mainimport ( &quot;os&quot; &quot;io&quot; &quot;log&quot; &quot;net/http&quot;)func main() &#123; // Create output file newFile, err := os.Create(&quot;devdungeon.html&quot;) if err != nil &#123; log.Fatal(err) &#125; defer newFile.Close() // HTTP GET request devdungeon.com url := &quot;http://www.devdungeon.com/archive&quot; response, err := http.Get(url) defer response.Body.Close() // Write bytes from HTTP response to file. // response.Body satisfies the reader interface. // newFile satisfies the writer interface. // That allows us to use io.Copy which accepts // any type that implements reader and writer interface numBytesWritten, err := io.Copy(newFile, response.Body) if err != nil &#123; log.Fatal(err) &#125; log.Printf(&quot;Downloaded %d byte file.\\n&quot;, numBytesWritten)&#125; 5.3 哈希和摘要（Hashing和Checksums）示例1 12345678910111213141516171819202122232425package mainimport ( &quot;crypto/md5&quot; &quot;crypto/sha1&quot; &quot;crypto/sha256&quot; &quot;crypto/sha512&quot; &quot;log&quot; &quot;fmt&quot; &quot;io/ioutil&quot;)func main() &#123; // Get bytes from file data, err := ioutil.ReadFile(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; // Hash the file and output results fmt.Printf(&quot;Md5: %x\\n\\n&quot;, md5.Sum(data)) fmt.Printf(&quot;Sha1: %x\\n\\n&quot;, sha1.Sum(data)) fmt.Printf(&quot;Sha256: %x\\n\\n&quot;, sha256.Sum256(data)) fmt.Printf(&quot;Sha512: %x\\n\\n&quot;, sha512.Sum512(data))&#125; 示例2 12345678910111213141516171819202122232425262728293031package mainimport ( &quot;crypto/md5&quot; &quot;log&quot; &quot;fmt&quot; &quot;io&quot; &quot;os&quot;)func main() &#123; // Open file for reading file, err := os.Open(&quot;test.txt&quot;) if err != nil &#123; log.Fatal(err) &#125; defer file.Close() // Create new hasher, which is a writer interface hasher := md5.New() _, err = io.Copy(hasher, file) if err != nil &#123; log.Fatal(err) &#125; // Hash and print. Pass nil since // the data is not coming in as a slice argument // but is coming through the writer interface sum := hasher.Sum(nil) fmt.Printf(&quot;Md5 checksum: %x\\n&quot;, sum)&#125; 原文Working with Files in Go","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"},{"name":"笔记","slug":"博文/笔记","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://straysh.github.life/tags/Docker/"}]},{"title":"Docker容器网络","slug":"Docker容器网络","date":"2022-03-13T04:10:09.000Z","updated":"2022-03-13T06:27:02.606Z","comments":true,"path":"2022/03/13/Docker容器网络/","link":"","permalink":"https://straysh.github.life/2022/03/13/Docker%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"简介Docker通过网络驱动network drivers实现容器间的网络互联。默认情况Docker提供两种网络驱动，bridge网络驱动和overlay网络驱动。查看网络： 123456$ docker network lsNETWORK ID NAME DRIVER SCOPEfbae7442cff4 bridge bridge local10a61a961499 docker_gwbridge bridge local28ffffe21438 host host local4c0fc68dd142 none null local 名为bridge的网络是一个特殊的网络即docker0。除非显示声明，否则Docker创建容器时默认使用该网络。 123456789$ docker network inspect bridge -f=&quot;&#123;&#123;json .Options&#125;&#125;&quot;|jq&#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot;&#125; 启动容器，未指定网络默认使用docker0： 123$ docker run -d --rm --name demo_net_container busybox sh -c &quot;sleep 600&quot;$ docker ps -a|grep demoeed56010136e busybox &quot;sh -c &#x27;sleep 600&#x27;&quot; 4 seconds ago Up 3 seconds demo_net_container 查看容器网络： 12$ docker inspect -f=&quot;&#123;&#123;json .NetworkSettings.Networks&#125;&#125;&quot; demo_net_container&#123;&quot;bridge&quot;:&#123;&quot;IPAMConfig&quot;:null,&quot;Links&quot;:null,&quot;Aliases&quot;:null,&quot;NetworkID&quot;:&quot;fbae7442cff48ab9c8af7f5e120c8ce530db9e1416c0067c47094ba692a9d0a8&quot;,&quot;EndpointID&quot;:&quot;83fbc769afc547237687183904acad786808edd81212d96cabf69f01defded03&quot;,&quot;Gateway&quot;:&quot;172.17.0.1&quot;,&quot;IPAddress&quot;:&quot;172.17.0.2&quot;,&quot;IPPrefixLen&quot;:16,&quot;IPv6Gateway&quot;:&quot;&quot;,&quot;GlobalIPv6Address&quot;:&quot;&quot;,&quot;GlobalIPv6PrefixLen&quot;:0,&quot;MacAddress&quot;:&quot;02:42:ac:11:00:02&quot;,&quot;DriverOpts&quot;:null&#125;&#125; 获取容器IP地址： 12$ docker inspect -f=&quot;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&quot; demo_net_container172.17.0.2 Docker网络驱动类型 bridage：默认网络驱动。桥接网络是的独立的容器之间能够通信。 host：对于独立容器，移除容器和宿主机之间的网络隔离，直接使用宿主机的网络。 overlay：覆盖网络能够将多个Docker daemon连接起来，并使swarm services之间能够通信。也可以使用覆盖网络在swarm services和独立容器之间建立网络通信，或者不同的Docker daemon之间的独立容器建立通信。 macvlan：Macvlan网络允许向容器分配固定的MAC地址，使它在网络中好似一个物理设备。Docker daemon通过MAC地址将流量路由到指定容器。 none：禁用网络。这种模式下，通常会搭配自定义网络插件。 Overlay网络，覆盖网络overlay网络在Docker daemon之间创建了一个分布式的网络。它工作在宿主机网络之上，将容器连接起来。并支持网络加密特性。当初始化swarm集群或加入一个swarm集群时，在Docker宿主机上会创建两个新的网络： ingress网络：一个名为ingress的覆盖网络，控制着swarm services的流量。当创建service且不指定自定义覆盖网络时，它被默认使用。 docker_gwbridge：一个名为docker_gwbridge的桥接网络，在swarm集群中充当连接各个独立Docker daemon的角色。 对覆盖网络的操作创建覆盖网络 前置条件： Docker daemon使用覆盖网络的防火墙规则，保持协议和端口双向放开 TCP端口2377，用以集群管理的通信 TCP和UDP端口7946，用以节点之间通信 UDP端口4789，用以覆盖网络间流量 初始化swarm集群或者加入一个swarm集群。加入集群后会自动创建一个名为ingress的覆盖网络。然后你才可以创建自定义的覆盖网络。 覆盖网络流量加密使用AES算法的GCM模式 swarm集群模式和独立容器自定义ingress网络123456789101112131415# 删除ingress网络下的所有服务$ docker network inspect ingress# 删除ingress网络$ docker network rm ingress# 自定义ingress网络$ docker network create \\ --driver overlay \\ --ingress \\ --subnet=10.11.0.0/16 \\ --gateway=10.11.0.2 \\ --opt com.docker.network.driver.mtu=1200 \\ my-ingress 自定义docker_gwbridge网络接口docker_gwbridge是一个虚拟网桥，用以将覆盖网络和集群间的Docker daemon的物理网卡连接起来。当初始化swarm集群或加入swarm集群时，Docker会自动创建它，但它不是一个Docker设备。它存在与Docker宿主机的内核中。要对docker_gwbridge进行定制，必须在加入swarm集群之前或先退出swarm集群。 停止Docker daemon 删除docker_gwbridge网桥12$ sudo ip link set docker_gwbridge down$ sudo ip link del dev docker_gwbridge 启动Docker daemon但不要初始化swarm集群 创建docker_gwbridge网桥123456$ docker network create \\ --subnet 10.11.0.0/16 \\ --opt com.docker.network.bridge.name=docker_gwbridge \\ --opt com.docker.network.bridge.enable_icc=false \\ --opt com.docker.network.bridge.enable_ip_masquerade=true \\ docker_gwbridge 初始化或加入swarm集群。因为docker_gwbridge已存在，Docker不会使用配置创建它。 对swarm集群服务的操作在覆盖网络中发布端口绕过swarm集群的routing mesh1--endpoint-mode=dnsrr 参考资料： Network containers Use overlay networks Networking with overlay networks Manage swarm service networks Use macvlan networks","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://straysh.github.life/tags/Docker/"}]},{"title":"Iptables和容器","slug":"Iptables和容器","date":"2022-03-13T03:01:46.000Z","updated":"2022-03-13T07:11:05.880Z","comments":true,"path":"2022/03/13/Iptables和容器/","link":"","permalink":"https://straysh.github.life/2022/03/13/Iptables%E5%92%8C%E5%AE%B9%E5%99%A8/","excerpt":"","text":"复习网络七层模型 简介在Linux中防火墙全称为Netfilter/Iptables。Netfilter工作在内核空间，负责根据规则执行具体的动作如数据包过滤、网络地址转换、数据包内容修改等。Iptables位于用户空间，是一个命令行工具，用来设定各种规则从而操作Netfilter。 iptables有五条链： PREROUTING 路由前 INPUT 流入 FORWARD 转发 OUTPUT 流出 POSTROUTING 路由后 以及四张表： Raw Mangle 主要负责修改数据包标记 Nat 主要负责网络地址转换 Filter 主要负责过滤 表和链的组合设定多条规则，实现对数据包的控制。iptables中四张表按优先级执行 raw -&gt; mangle -&gt; nat -&gt; filter。 命令12345678910111213iptables -t 表名 &lt;-A/I/D/R/L/F&gt; 规则链名 [规则号] &lt;-i/o 网卡名&gt; -p 协议名 &lt;-s 源IP&gt; --sport 源端口 &lt;-d 目标IP&gt; --dport 目标端口 -j 动作 -t 指定表名，未指定表名时默认为 Filter 表-A和-I都是添加规则，-A增加的规则放在现有规则的最后，-I添加的规则放在规则号指定的位置，该位置原先的规则往后顺位。-D 删除规则号指定的规则-R 替换规则号指定的规则-L 查看相应的规则-F 清楚某条链或者表的规则-i/o 指定输入和输出的网卡-p 指定数据包协议，如 tcp、udp、icmp 等，这里支持简单的表达式，如 -p !tcp 去除 tcp 外的所有协议-s和-sport分别指定数据包源 IP 地址及端口-d和-dport分别指定数据包目标 IP 地址及端口-j 指定前述的参数匹配上数据包以后执行的动作。常用的处理动作包括 ACCEPT 放行、REJECT 拒绝、DROP 丢弃、REDIRECT 重定向、DNAT 修改目的 IP 及端口、SNAT 修改源 IP 及端口等等 iptables规则示例 禁用SSHD默认的22端口1iptables -t filter -A INPUT -p tcp --dport 22 -j DROP 只允许特定网段10.160.0.0/16访问本机的10.160.100.1的SSHD(22端口)服务1234567#设置默认的drop，再允许特定的网段进入和出去iptables -P INPUT DROPiptables -P OUTPUT DROPiptables -P FORWARD DROPiptables -t filter -A INPUT -s 10.160.0.0/16 -d 10.160.100.1 -p tcp --dport 22 -j ACCEPTiptables -t filter -A OUTPUT -s 10.160.100.1 -d 10.160.0.0/16 -p tcp --dport 22 -j ACCEPT 过滤掉状态有问题的http包。只允许http80端口且限定连接状态为Established和Related的数据包1iptables -A INPUT -p tcp --sport 80 -m state --state ESTABLISHED,RELATED -j ACCEPT 开启儿童上网模式，星期一到星期五的8:00-21:00禁止游戏相关网页”game“1iptables -I FORWARD -s 192.168.0.0/24 -m string --string &quot;game&quot; -m time --timestart 8:00 --timestop 21:00 --days Mon,Tue,Wed,Thu,Fri -j DROP 生产环境mysql数据库仅允许内网特定ip访问1iptables –A INPUT –s 10.160.41.1 –p tcp –dport 3306 –j ACCEPT 将目的IP为10.160.132.55且目的端口为9090的我们做DNAT修改目标地址处理，重定向到10.162.37.1:80801iptables -A INPUT -d 10.160.132.55 -p tcp --dport 9090 -j DNAT --to 10.162.37.1:8080 拦截所有入站tcp80端口和8080端口数据包重定向到某个代理服务的15001端口进行统一处理1iptables -A INPUT -p tcp --dport 80,8080 -j REDIRECT --to-ports 15001 Docker的iptables列出所有nat规则1$ iptables -L -nvt nat 列出所有filter规则1$ iptables -L -nvt filter 原文从 iptables 谈 ServiceMesh 流量拦截","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"},{"name":"笔记","slug":"博文/笔记","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://straysh.github.life/tags/Docker/"}]},{"title":"Linux网桥工作原理","slug":"Linux网桥工作原理","date":"2022-03-12T10:02:29.000Z","updated":"2022-03-13T03:46:32.589Z","comments":true,"path":"2022/03/12/Linux网桥工作原理/","link":"","permalink":"https://straysh.github.life/2022/03/12/Linux%E7%BD%91%E6%A1%A5%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","excerpt":"","text":"Linux网桥是一种使用软件实现的虚拟设备。它将多个网络接口连接起来，一个接口接收到的数据会复制到其他网络接口。 Docker在启动时，会创建一个名为docker0的虚拟网桥，默认使用172.17.0.1/16地址。并使用虚拟设备对veth-pair将容器与docker0网桥连接起来。Docker会创建一条iptables NAT规则将来自172.17.0.0/16网段的数据包通过物理网卡转发出去（如图中的enps03网卡）。 网桥工作在二层网络，能够根据MAC地址对数据包进行广播或者单播。当网桥在本地hash表中找到MAC地址对应的网桥端口，说明此数据包是单播数据包，否则就是广播数据包。 当网络接口接收到数据包时，会判断这个网络接口是否绑定在某个网桥上，如果绑定了就开始处理数据。首先从数据包中学习到的目标MAC地址插入到网桥的hash表中。如果数据包是广播包(MAC地址首位是1)，调用广播函数将数据包发送给连接在网桥上的所有网络接口。否则在hash表中查找目标MAC地址的网桥端口，若未找到，将数据包发送给连接在网桥上的所有网络接口。若找到，将数据包发送给此端口","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://straysh.github.life/tags/Docker/"}]},{"title":"并发和调度的关系","slug":"并发和调度的关系","date":"2021-03-16T05:54:13.000Z","updated":"2021-03-16T12:33:24.990Z","comments":true,"path":"2021/03/16/并发和调度的关系/","link":"","permalink":"https://straysh.github.life/2021/03/16/%E5%B9%B6%E5%8F%91%E5%92%8C%E8%B0%83%E5%BA%A6%E7%9A%84%E5%85%B3%E7%B3%BB/","excerpt":"","text":"注：该文章基于Go 1.13 将Go协程从一个系统线程切换到另一个系统线程有一定开销，若触发非常频繁，将降低应用的性能。但随着时间的推移，Go调调度器已经解决了这个问题。它负责协调并发工作时协程和系统线程间的关系。让我们回到几年前，看看这个问题是如何改进的。 原始问题在Go的早期，1.0和1.1版本，在多线程上(如提高GOMAXPROCS数值)执行并发的代码有性能问题。我们看看文档中素数筛的代码示例： 这是GOMAXPROCS非值1时，Go 1.0.3版本，计算前10万素数的压测数据。 12345name time/opSieve 19.2s ± 0%Sieve-2 19.3s ± 0%Sieve-4 20.4s ± 0%Sieve-8 20.4s ± 0% 为了理解上面的测试结果，需要先了解当时的调度器是如何设计的。在Go的第一个版本中，调度器只有一个全局队列，所有的线程都从这里存取协程。下图所示为运行了两个系统线程M的程序，通过设置GOMAXPROCS=2实现： 只有一个队列无法保证协程恢复时，仍在相同的线程上执行。第一个就绪的线程会选取一个等待中的协程并运行它。因此，必然导致协程在另一个线程上执行，同时付出相当大的性能代价。以阻塞的管道举例： 协程#7的管道阻塞并等待消息。当消息到达时，协程被推入全局队列。 然后，管道发送消息，协程#X将在空闲线程上执行，而协程#8因管道阻塞 协程#7此时将在空闲线程上执行 此时协程在不同的线程上执行了。只有一个全局队列也迫使调度器使用全局锁来控制对协程的调度操作。这里是通过pprof的CPU分析 1234567891011Total: 8679 samples3700 42.6% 42.6% 3700 42.6% runtime.procyield1055 12.2% 54.8% 1055 12.2% runtime.xchg753 8.7% 63.5% 1590 18.3% runtime.chanrecv677 7.8% 71.3% 677 7.8% dequeue438 5.0% 76.3% 438 5.0% runtime.futex367 4.2% 80.5% 5924 68.3% main.filter234 2.7% 83.2% 5005 57.7% runtime.lock230 2.7% 85.9% 3933 45.3% runtime.chansend214 2.5% 88.4% 214 2.5% runtime.osyield150 1.7% 90.1% 150 1.7% runtime.cas procyield，xchg，futex和lock都是调度器的全局锁相关。显而易见，程序的大部分消耗都在锁上。 这些问题使得Go无法利用多核，因此在Go 1.1版本中修复了。 并发亲和性(Affinity in concurrency)Go1.1带来了新的调度器实现及本地协程队列的创建。这个优化使用本地协程队列避免了对整个调度器加锁，并允许协程在相同的系统线程上运行。 由于线程会因系统调用而阻塞，且阻塞线程的数量是无限制的，Go引入了处理器(processor)的概念。一个处理器P表示一个运行的系统线程并管理者本地协程队列。新的图示： 这是用Go 1.1.2中新的调度器压测的结果： 12345name time/opSieve 18.7s ± 0%Sieve-2 8.26s ± 0%Sieve-4 3.30s ± 0%Sieve-8 2.64s ± 0% 此时，Go真正发挥了多核的优势。CPU分析也改变了： 1234567891011Total: 630 samples163 25.9% 25.9% 163 25.9% runtime.xchg113 17.9% 43.8% 610 96.8% main.filter93 14.8% 58.6% 265 42.1% runtime.chanrecv87 13.8% 72.4% 206 32.7% runtime.chansend72 11.4% 83.8% 72 11.4% dequeue19 3.0% 86.8% 19 3.0% runtime.memcopy6417 2.7% 89.5% 225 35.7% runtime.chansend116 2.5% 92.1% 280 44.4% runtime.chanrecv212 1.9% 94.0% 141 22.4% runtime.lock9 1.4% 95.4% 98 15.6% runqput 大部分锁相关的操作被移除了，chanXXXX相关的操作仅和管道相关。但是，若调度器优化了协程与线程间的亲和性，在某些场景下亲和性会被减弱。 亲和性受限(Affinity limitation)为理解亲和性受限，我们需要理解被放入本地和全局队列的东西是什么。所有操作，如管道/select阻塞，等待定时器，锁都会使用本地队列除了系统调用。因此，两个特性会限制协程和线程的亲和性： 协程窃取。当处理器P没有足够的本地队列任务时，它会从其他P中窃取协程，若全局队列和network poller中都是空的。发生协程窃取时，协程会在另一个线程上运行。 系统调用。发生系统调用时(如文件操作，http调用，数据库操作等)，Go将运行的系统线程变更为阻塞状态，让另一个新的线程接管当前P进而处理本地写成队列。 但，为了更好的管理本地队列的优先级，这两个限制可以避免。Go 1.5旨在给在管道中通信的协程更高的优先级，从而优化协程与线程的亲和性。 排序以强化亲和性(Ordering to enhance affinity)在管道上来回通信的协程会频繁的阻塞，亦即频繁的重入本地队列。但，本地队列遵守FIFO原则，若有协程独占线程，被解锁的协程无法保证被及时执行。下图为被阻塞协程恢复可运行的例子： 在因管道阻塞后协程#9恢复了(管道接收到消息)。但，它必须等待协程#2，#5和#4结束才能运行。在这个例子中，协程#5将独占线程，使得协程#9被延期执行，从而有被其他线程窃取的风险。从Go 1.5开始，从阻塞管道返回的协程会有更高的优先级(通过处理器P的一个特殊属性实现)： 协程#9此时被标记为下一个可运行的。这个优先级特性使得协程能尽快被处理避免被再次阻塞。然后，其他协程才能获得运行时间。这个改变对Go标准库有全面的积极影响。 原文 Go: Concurrency &amp; Scheduler Affinity","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"gsignal，信号大师","slug":"gsignal，信号大师","date":"2021-03-16T05:47:25.000Z","updated":"2021-03-16T05:52:17.145Z","comments":true,"path":"2021/03/16/gsignal，信号大师/","link":"","permalink":"https://straysh.github.life/2021/03/16/gsignal%EF%BC%8C%E4%BF%A1%E5%8F%B7%E5%A4%A7%E5%B8%88/","excerpt":"","text":"注：该文章基于Go 1.13signal包提供了一些允许Go程序与信号量交互的方法。在深入前，我们先从这段监听器代码开始。 信号订阅我们使用通道(channel)来订阅信号。以下代码监听中断信号或终端缩放： 123456789101112131415161718192021222324252627282930313233package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;os/signal&quot; &quot;syscall&quot;)func main() &#123; done := make(chan bool, 1) s1 := make(chan os.Signal, 1) signal.Notify(s1, syscall.SIGINT, syscall.SIGTERM) go func()&#123; &lt;-s1 fmt.Println(`/!\\ The program is going to exit...`) done &lt;- true &#125;() s2 := make(chan os.Signal, 1) signal.Notify(s2, syscall.SIGWINCH) go func()&#123; for &#123; &lt;- s2 fmt.Println(`/!\\ The terminal has been resized.`) &#125; &#125;() &lt;- done&#125; 每个通道都有自己的事件逻辑，如图： Go也提供了停止向通道发送通知的方法Stop(os.Signal)，或者忽略信号的方法Ignore(...os.Signal)。举例： 12345678910111213141516171819202122232425262728package mainimport ( &quot;fmt&quot; &quot;os&quot; &quot;os/signal&quot; &quot;syscall&quot;)func main() &#123; done := make(chan bool, 1) s2 := make(chan os.Signal, 1) signal.Notify(s2, syscall.SIGWINCH) signal.Ignore(syscall.SIGINT) go func()&#123; &lt;- s2 fmt.Println(`/!\\ The terminal has been resized.`) signal.Stop(s2) // will block forever since we stop listening &lt;- s2 done &lt;- true &#125;() &lt;- done&#125; 这段程序无法通过CTL+C终止，而且在首次接收到终端缩放信号时，停止了该通道，导致其此后再接收不到任何信号。那么接下来，我们看看信号是如何被监听和处理的。 gsignal在初始化阶段，signal派生了一个在循环中处理信号量的消费者协程。该协程处在休眠状态，直到接收到通知。过程如图： 当信号发到达程序时，信号句柄将该信号量代理给一个特殊的协程gsignal。这个协程初始化时使用了一个很大的栈空间(32k，为满足不同操作系统的要求)，该栈大小固定不会再增长。每一个线程(图中标以M)都有一个gsignal协程来处理信号量。如图： gsignal分析信号量并判断若能够被处理，则唤醒休眠的协程同时将信号量发送到队列。 同步信号量，如SIGBUS和SIGFPE，无法被处理并会导致panic。 然后，looping协程能够处理该信号。它找到订阅了该信号的第一个通道并将信号推送给它。 处理信号的looping协程可以通过go tool trace可视化的观察。 对gsignal加锁或阻塞会导致处理信号异常。同时因其有固定大小的栈空间，无法重分配内存。这就是为何在处理信号量的链路中需要两个独立的协程：一个协程将到达的信号尽快存储到队列，另一个协程循环的处理这个队列。 此时，我们可以对文中第一张图最终修改如下： 原文 Go: gsignal, Master of Signals","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"go协程是如何回收的","slug":"go协程是如何回收的","date":"2021-03-16T05:46:25.000Z","updated":"2021-03-16T05:51:15.957Z","comments":true,"path":"2021/03/16/go协程是如何回收的/","link":"","permalink":"https://straysh.github.life/2021/03/16/go%E5%8D%8F%E7%A8%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%9B%9E%E6%94%B6%E7%9A%84/","excerpt":"","text":"注：该文章基于Go 1.13 协程很容易创建，有很小的初始栈空间，以及快速切换的上下文。大家都爱协程，但创建大量短命的协程，要花费时间去创建和销毁它们。 协程的生命周期下面通过一个小例子说明协程是如何被复用的。这是Go文档中的一个示例。 程序中创建了大量的协程去过滤素数，Go负责这些协程的创建和销毁。实际上，Go维护了一个空闲协程的列表： 每一个P维护自己的本地队列，在管理空闲协程时无需加锁。一个协程退出时，会被放置到本地队列中。 但为了更高效的分发协程，调度器也有自己的列表。实际上是两个列表：一个基于栈实现的列表，一个非栈的列表。 因为任何线程都可以访问他们，使用了锁来保护这两个列表，。当P的协程数超过64时，调度器从P的本地队列中获取协程放入中央列表。一次转移会抽取半数协程，那么P中将只剩于半数： 从上面的描述中，这个工作流非常直白，但实际工作中根据协程的内存分配情况会有不同的操作对则。 条件回收协程能够节省再分配的开销。但，协程的栈空间是动态的，其工作时可能分配了很大的栈。回收协程时，Go不会保留其栈(超过默认2k时)。 上面计算素数的程序很简单，且不会增长协程的栈空间，Go可以直接重用它们。此处为压测结果： 12345678With recycling Without recyclingname time/op name time/opPrimeNumber 12.7s ± 3% PrimeNumber 12.1s ± 3%PrimeNumber-8 2.27s ± 4% PrimeNumber-8 2.13s ± 3%name alloc/op name alloc/opPrimeNumber 1.83MB ± 0% PrimeNumber 5.82MB ± 4%PrimeNumber-8 1.52MB ± 7% PrimeNumber-8 5.90MB ±21% 没有关闭协程回收的方法，作者通过直接修改go标准库来测试的。可以看见，协程回收减少了3M内存分配。 让我们再来看另一个使用大栈空间的例子： 12345678910111213141516func ping() &#123; for i := 0; i &lt; 10; i++ &#123; var wg sync.WaitGroup for g := 0; g &lt; 10; g++ &#123; wg.Add(1) go func() &#123; _, err := http.Get(`https://httpstat.us/200`) if err != nil &#123; panic(err) &#125; wg.Done() &#125;() &#125; wg.Wait() &#125;&#125; 压测结果如下： 12345678With recycling Without recyclingname time/op name time/opPingUrl 12.8s ± 2% PingUrl 12.8s ± 3%PingUrl-8 12.6s ± 0% PingUrl-8 12.7s ± 3%name alloc/op name alloc/opPingUrl 9.21MB ± 0% PingUrl 9.44MB ± 0%PingUrl-8 9.28MB ± 0% PingUrl-8 9.43MB ± 0% 由于栈空间很大，这里实际效果不明显。 原文 Go: How Does Go Recycle Goroutines?","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"g0一个特殊的协程.md","slug":"g0一个特殊的协程","date":"2021-03-16T05:45:25.000Z","updated":"2021-03-16T05:49:13.641Z","comments":true,"path":"2021/03/16/g0一个特殊的协程/","link":"","permalink":"https://straysh.github.life/2021/03/16/g0%E4%B8%80%E4%B8%AA%E7%89%B9%E6%AE%8A%E7%9A%84%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"注：该文章基于Go 1.13所有Go程序创建的协程都由内部调度器的管理。Go调度器尝试给running状态的协程分配运行时间。当协程被阻塞或中断时，使用running状态的协程保持CPU处于工作状态。Go调度器实际是一个特殊的协程。 协程调度GOMAXPROCS变量让Go可以限制系统线程数量。Go在每个运行的线程上调度和管理协程。这个角色，一个称为g0的特殊协程，是线程创建的地一个协程。 然后，调度器将ready状态的协程分配到线程上运行。（参考 P,M,G模型） 为了更好的理解g0是如何调度的，我们来复习一下管道的使用过程: 123ch := make(chan int)[...]ch &lt;- v 管道阻塞时，当前协程被暂停，如进入waiting状态，被不会被推入任何协程队列中。 然后，g0替换协程并进行第一轮调度。 调度期间，本地队列local queue优先级较高，因此将运行#2协程。 当管道读取到数据时，#7协程将解除阻塞。 1v ：= &lt;-ch 协程接收到管道消息后，会将运行时切换到g0，并解除#7的阻塞再将其放到本地队列列首。 虽然调度器负责所有协程的调度，但它还有很多其他工作。 调度器的职责与常规协程不同，g0有一个固定且更大的栈空间。有些操作可能需要较大的栈空间，有些系统要求其栈空间不能增长。那么，g0协程主要有以下职责： 负责协程的创建。当调用go func()&#123; ... &#125;()或 go myFunction()时，Go先将函数创建任何转给g0，再将其放入本地队列中。 新近创建的协程有更高的优先级，会被放置在本地队列的列首。 分配 defer 函数。 垃圾收集，例如STW(stopping the world)，扫描协程栈空间，标记/清楚操作等。 栈空间增长。当协程需要更大的栈空间时，This operation is done by g0 in the prolog functions 另外还有诸如超大内存分配，cgo等，这个有着较大栈空间的特殊协程保证了我们的Go程序高效的运行。 原文 Go: g0, Special Goroutine","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"协程切换到底做了啥","slug":"协程切换到底做了啥","date":"2021-03-16T05:45:25.000Z","updated":"2021-03-16T05:50:19.909Z","comments":true,"path":"2021/03/16/协程切换到底做了啥/","link":"","permalink":"https://straysh.github.life/2021/03/16/%E5%8D%8F%E7%A8%8B%E5%88%87%E6%8D%A2%E5%88%B0%E5%BA%95%E5%81%9A%E4%BA%86%E5%95%A5/","excerpt":"","text":"注：该文章基于Go 1.13Go协程很小，启动仅需2Kb栈空间。协程也很轻，从一个协程切换到另一个协程不需要很多操作。在深入协程切换前，我们先从高层复习下切换是如何工作的。 建议结合Go: Goroutine, OS Thread and CPU Management一起阅读本文章。 案例Go调度器将协程分配到线程上执行，依据以下两类： 当协程阻塞时：系统调用，mutex锁，或者协程。被阻塞的协程进入休眠状态/放入一个队列中，并允许Go调度并运行其他等待中的协程。 在函数调用时，若在初始阶段，协程需要扩展其栈空间。该中断允许调度器运行其他协程，避免当前写成独占CPU。 以上场景中，调度器g0负责将当前协程替换为其他协程并准备运行之。然后，被替换的协程取代g0在线程上运行。 将running协程替换为其他协程，涉及两个切换： running协程切换为g0: g0切换为下一个协程g： 在Go语言中，切换协程是轻量的。保存协程状态需要两样东西： 在被取消调度前，协程停止的行号。当前执行指令被记录在程序计数器中(PC)。协程稍后会在相同的位置恢复。 协程栈空间，用以再次执行时恢复变量。 来看个例子 程序计数器(Program counter)例子中，一个写成生产数据，另一个消费数据： 12345678910111213141516171819202122232425262728293031323334package mainimport &quot;sync&quot;func main() &#123; var wg sync.WaitGroup c := make(chan int, 10) wg.Add(1) go func()&#123; for i:=0;i&lt;100;i++&#123; c &lt;- i &#125; close(c) wg.Done() &#125;() for i:=0;i&lt;3;i++ &#123; wg.Add(1) go func()&#123; for v:=range c &#123; if v%2 == 0 &#123; println(v) &#125; &#125; wg.Done() &#125;() &#125; wg.Wait()&#125; 程序将打印出1~99的偶数。先看第一个协程-生产者-将数字添加到管道(带缓冲的管道)。当管道装满时，数据放送方将被阻塞。同事，Go将切换到go并调度另一个协程来工作。 根据上文的知识，此时，Go需要保存当前操作以便恢复时能继续v执行。程序计数器(PC)被保存为协程的一个内部结构体。此处为图示： 可以使用go tool objdump命令查看指令及其地址信息。此处以生产者为例 12go tool compile -N -l 1.gogo tool objdump 1.o 程序逐个指令执行，直到被函数runtime.channelsend1阻塞。Go将当前程序计数器保存到协程的内部属性中。本例中，程序计数器被保存到地址0x4268d0： 然后，g0协程唤醒休眠的协程，执行同样的质量，循环数字并推入管道。接下来，我们看看协程切换时的站空间管理。 栈(Stack)在被阻塞前，running协程有它自己的栈空间。其中包含临时内存，如变量i： 然后，当管道阻塞时，协程及其栈被切换到g0(g0有一个更大的栈空间)： 协程切换前，栈空间会被保存以便恢复协程时可以继续执行： 现在，我们对协程切换有了一个完整的认识。我们来看看它是如何影响性能的。 操作(Operations)我们继续使用上面的代码测量协程切换消耗的时间。因查找下一个待调度协程需要时间，因此这个例子中的测量结果不是太精准。从函数的prolog切换比从阻塞的管道切换协程，要花费更多的操作。 总结一下待测量的操作： 当前被管道阻塞的协程g，切换到g0协程： PC和栈指针一起被保存到内部结构体 g0被设置为running协程 g0的栈空间替换当前栈空间 g0查找下一个待调度协程。 g0与待调度协程切换： PC和栈指针从内部结构体中恢复。 程序跳至PC指针位置执行 协程切换，从g到g0或从g0到g是最快的阶段。仅包含有限的几个指令，而调度器查找下一个协程需要很多操作。查找待调度协程甚至需要更多时间，这取决于运行的程序。 图示的测试结果，给出了性能的数量级示意。通常也没有标准的工具来做这么测量。当然测量结果也与架构相关(作者使用的是 Mac 2.9GHz 双核i5)。 原文 Go: What Does a Goroutine Switch Actually Involve?","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"go协程是如何启动和退出的","slug":"go协程是如何启动和退出的","date":"2021-03-16T05:44:25.000Z","updated":"2021-03-16T05:48:18.565Z","comments":true,"path":"2021/03/16/go协程是如何启动和退出的/","link":"","permalink":"https://straysh.github.life/2021/03/16/go%E5%8D%8F%E7%A8%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%90%AF%E5%8A%A8%E5%92%8C%E9%80%80%E5%87%BA%E7%9A%84/","excerpt":"","text":"注：该文章基于Go 1.14在Go语言中，go协程是携带当前运行程序信息(诸如栈、程序计数器、或其当前OS线程)的结构体。Go调度器依据这些信息给其分配运行时间。调度器也需要关注这些协程的开始与退出，这两个阶段需要非常细致的管理。 关于栈和程序计数器，推荐阅读作者的另一篇文章Go: What Does a Goroutine Switch Actually Involve? 协程启动启动协程非常简单。用代码举例： 1234567891011121314151617package mainimport &quot;sync&quot;func main() &#123; var wg sync.WaitGroup wg.Add(1) go func()&#123; println(&quot;goroutine is running...&quot;) wg.Done() &#125;() println(&quot;main is running...&quot;) wg.Wait()&#125; main程序在打印之前启动了一个协程。由于Go协程有其自己的运行时间，Go通知运行时(runtime)创建一个新的协程即： 创建stack 收集当前程序的计数器或调用方的数据。 更新协程的内部数据，如ID或状态。 但协程并不会立即获得运行时间。新创建的协程会入队到本地队列列首，并在调度器的下一轮调度时执行。下图是该状态的图示： 将其放置在队首使得该协程在当前协程结束后会首先被执行。该协程要么在当前线程被执行，要么在其他线程执行(若发生了线程窃取)。 协程创建过程在汇编代码中也能找到： 一旦协程被创建且入队后，就继续执行main函数的后续代码。 协程退出一个协程终止时，为避免浪费CPU时间，Go必须调度其他协程继续工作。也会保留这个协程以便之后重用。 Go需要一种方式来感知到协程的退出。该控制方法在协程创建时已确定。当协程创建时，在将程序计数器指向真实被调函数前，Go在栈中设置了goexit函数。则巧妙的使得协程终止工作时一定会调用goexit函数。下面的代码可以观察这个过程： 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; &quot;runtime&quot; &quot;sync&quot;)func main() &#123; var wg sync.WaitGroup wg.Add(1) go func()&#123; var skip int for &#123; _, file, line, ok := runtime.Caller(skip) if !ok &#123; break &#125; fmt.Printf(&quot;%s:%d\\n&quot;, file, line) skip++ &#125; wg.Done() &#125;() println(&quot;main is running...&quot;) wg.Wait()&#125; 输出结果为其调用栈信息: 12/app/straysh/docs/translation/1.go:17/home/uos/.gvm/gos/go1.14/src/runtime/asm_amd64.s:1373 src/runtime/asm_amd64.s的代码如下 然后Go会切换到go去调度另一个协程。 也可以通过手动调用runtime.Goexit()来结束一个协程: 123456789101112131415161718192021package mainimport ( &quot;runtime&quot; &quot;sync&quot;)func main() &#123; var wg sync.WaitGroup wg.Add(1) go func()&#123; defer wg.Done() runtime.Goexit() println(&quot;never executed&quot;) &#125;() println(&quot;main is running...&quot;) wg.Wait()&#125; 该函数会首先执行defer函数，然后继续执行调用该协程前的函数。 原文 Go: How Does a Goroutine Start and Exit?","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"角度变了,很多事情就看懂了!","slug":"角度变了-很多事情就看懂了","date":"2020-03-23T15:21:45.000Z","updated":"2020-03-23T15:36:11.220Z","comments":true,"path":"2020/03/23/角度变了-很多事情就看懂了/","link":"","permalink":"https://straysh.github.life/2020/03/23/%E8%A7%92%E5%BA%A6%E5%8F%98%E4%BA%86-%E5%BE%88%E5%A4%9A%E4%BA%8B%E6%83%85%E5%B0%B1%E7%9C%8B%E6%87%82%E4%BA%86/","excerpt":"","text":"文章来源：微信文章角度变了，很多事情就看懂了！普通人和高手之间的差距在哪里？ 在于能不能拥有自己独立清晰的判断和观点。不盲目跟风，不人云亦云。 这是从 1 楼爬到 10 楼，从普通到优秀。 那高手和顶尖高手的差距在哪里？ 在于能不能同时拥有自己和他人的观点，把自己放进别人的鞋子里，不以自我为中心。 这是从10楼爬到100楼，从优秀到极致。 两幅地图最近，疫情继续在全球扩散。 我们常听新闻里说，欧美等“西方国家”情况危急，像中国这样的“远东国家”，疫情得到了有效控制。 但是你有没有想过，为什么？ 为什么欧美是“西方”，中国是“远东”？ 如果我们看地图，明明美国在地图最右侧，更偏“东方”啊！ 为什么美国不是“东方国家”，而是“西方国家”？ 答案，其实也在我们使用的世界地图里。 各国使用的世界地图基本是两种： 一种是以太平洋为中心的世界地图，亚洲的一些国家，大多使用的是这种世界地图。 另一种是以大西洋为中心的世界地图，西方和美洲国家，大多使用这种世界地图。 亚洲一些国家以太平洋为中心来绘制世界地图，这样这些国家就处于地图的中央，象征着世界的中心。 同样，欧美国家以大西洋为中心来绘制世界地图，这样他们就处于地图的中央，象征着他们是世界的中心。 回到开头的问题，为什么美国是“西方国家”？ 因为我们中国人，其实也是站在大西洋为中心的世界地图角度来看的： 在这张地图上，美国不在中国的东面，而在西面。 明白这个道理，就知道很多时候我们喜欢以自己为中心审视问题、价值取舍、处理矛盾。 所以，英国的“群体免疫”，意大利的“全国封城”，美国宣布进入“紧急状态”，站在我们的角度可能是不合理的，站在他们的角度，也许是有原因的。 再比如，特朗普发了推特，把新冠病毒称为“中国病毒”。为什么？ 他完全以自我为中心，这一招把内部矛盾，转移为外部矛盾，果然迎来了很多民族主义者的点赞。但实际上，这招太阴损，太恶毒了。 （图片来自网络） 一次辩论我目睹过一次辩论，一身冷汗。 张伟俊老师是中国第一位私人董事会教练。有一次，他受邀主持了一场企业家辩论会。 这些平常在自己公司永远伟大、光荣、正确的企业家们坐在一起，谁都不能说服谁。 他们越辩越激动，辩得脸红脖子粗，我看差点就快打起来了。 然后，然后，张伟俊突然叫停辩论，说： “下一个环节，交换观点，继续辩论。” 听完之后，人都傻了。 稍微停顿后，大家又唇枪舌剑起来，开始帮对方自圆其说，又差点要打起来。 这对在场的企业家们，是深刻的一课。 你是以自我为中心，还是能换个角度站在他人的立场去考虑问题？ 你捍卫的是自己的观点，还仅仅是自己的尊严？ 你捍卫的是自己的屁股，还是脑袋？ 换个角度看世界，你会看到完全不一样的世界。 三个场景所以，设计产品、亲密关系、教育孩子，都是如此。 我们总爱站在自己的视角来解决问题，而不是真正站在用户的视角来解决需求。 我们总爱站在自己的视角说多喝热水，而不是真正站在对方的视角来嘘寒问暖。 我们总爱站在自己的视角来决定爱好，而不是真正站在孩子的视角问是否喜欢。 这也是为什么，世界上总有“自嗨”的产品，教不好的“坏小孩”，和永远教不会的“直男”。 这是非常容易犯的错误，不能实现自己想要的结果。 即使实现了，也极大造成成本的剧烈攀升。 如果能换个角度看问题，是提升，更是跃升。视角不同，视野不同，结果也会天差地别。 最后的话人和人之间的差距在哪里？ 在于是站在 1 楼，还是 10 楼，还是 100 楼看事物。 有的人终其一生的努力，都只是站在 1 楼的视角看外面，看到的都是细节，自己附近那点东西，市井吵架，违章停车。 有的人努力向上攀登，站在 10 楼的视角看外面，开始能逐渐看到局部，轮廓关联浮现。 有的人比你更优秀，还比你更勤奋更努力，他爬到了 100 楼看世界。 他看到的是全局，开始体会自然资源的分布，城市设计的气概和俯瞰世界的万丈豪情。 站在 100 楼视角审视天下的人，可以决定一场战争的走向。 站在 10 楼视角观察天下的人，可以决定一场战役的胜负。 站在 1 楼视角面对天下的人，可以决定一场战斗的起止。 当你在 1 楼时，要更愿意主动跑到 5 楼、8 楼、10 楼，去看看都能望到些什么。再回到 1 楼执行时，会更理解不同角度看待问题的深度，并且不会遗忘细节。 希望你能拥有上帝视角，能瞬间把自己拉到空中俯视全局。希望你能拥有操盘手技能，能瞬间俯冲到地面死磕细节。希望你能知道什么时候该在空中，什么时候该在地面。 人与人之间的差距，在于看事物的角度不同。角度变了，视野变了，很多事情就看懂了。看懂了，也就知道该怎么做了。 文章来源：微信文章","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"},{"name":"Banbrick","slug":"Banbrick","permalink":"https://straysh.github.life/tags/Banbrick/"}]},{"title":"2019反思","slug":"2019反思","date":"2020-03-10T15:35:22.000Z","updated":"2020-03-19T10:41:17.714Z","comments":true,"path":"2020/03/10/2019反思/","link":"","permalink":"https://straysh.github.life/2020/03/10/2019%E5%8F%8D%E6%80%9D/","excerpt":"","text":"持续整理中，无法阅读 提现 - 总结和反思失误LBS - 研究网格 https://blog.hxsen.com/article/81.html https://www.jianshu.com/p/ec612d4982b9 https://www.jianshu.com/u/513quchu202559d5c https://www.jianshu.com/p/ab314e21fa0e https://www.cnblogs.com/jifeng/p/4356006.html https://blog.csdn.net/fdipzone/article/details/53896842 https://blog.intelligentbee.com/2017/09/14/get-nearby-locations-mysql-database/ https://stackoverflow.com/questions/2234204/find-nearest-latitude-longitude-with-an-sql-query https://dba.stackexchange.com/questions/4214/how-to-best-implement-nearest-neighbour-search-in-mysql https://weixin.sogou.com/weixin?type=2&amp;s_from=input&amp;query=mysql+%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96&amp;ie=utf8&amp;_sug_=y&amp;_sug_type_=&amp;w=01019900&amp;sut=1845&amp;sst0=1582784283384&amp;lkt=1%2C1582784283281%2C1582784283281 轨迹 - 数据存储RabbitMQ - 模拟异步/事件广播/逻辑解耦MQTT - 长链接推送Etcd - 配置中心工具 - 群通知/阿里云工具sdk","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"BlowFish_https_ssl","slug":"BlowFish-https-ssl","date":"2020-03-10T11:00:20.000Z","updated":"2020-03-11T10:30:44.214Z","comments":true,"path":"2020/03/10/BlowFish-https-ssl/","link":"","permalink":"https://straysh.github.life/2020/03/10/BlowFish-https-ssl/","excerpt":"","text":"SSL到底用的哪个SSL证书端口看看啥是“SSL证书端口”常有人问“SSL到底使用的是哪个端口？”，或者“常用的SSL证书端口是哪些？”。答案是没有。SSL/TLS本身不使用任何端口，而HTTPS使用443端口。 SSL/TLS是基础设施，允许其他协议如HTTPS或DNS等基于TLS工作。这些协议各自使用特定的端口，但SSL/TLS并不需要。 两个不同的术语：SSL证书端口 vs HTTPS端口通常我们把TLS证书叫作SSL证书。因此当我们提到”SSL证书443端口”一点也不奇怪。只需要记住，当有人提到SSL证书端口，通常是指HTTPS。 端口也指代软件用来与硬件通信的内存地址。在此处特指I/O地址。 在进入故宫前，会有栅栏把人群分成若干组，否则人群蜂拥向大门简直是灾难。这个比喻并100%准确，但是端口差不多就是这些栅栏的作用。端口来管控数据的收发。HTTP使用80端口通信。HTTPS使用443端口通信。这就是为啥老说SSL证书端口443了。 HTTPS/SSL证书端口有多少个？专家将网络链接划分为四层（若基于OSI则是七层）。端口在传输层使用。一共有65535个端口，从0000h到FFFFh。 PORT #功能80HTTP443SSL21FTP990FTPs22SFTP/SSH3306MySQL SSL(和后继者TLS)直接工作在TCP协议上。因此更高层的协议如HTTP可以无需修改的提供安全连接。在SSL层下的HTTP即HTTPS。 正确使用SSL/TLS后，攻击者只能从链路中看到IP地址和端口号，传输数据的估算大小，加密方式和压缩方式。黑客也可以终止连接，这样做的话客户端和服务端都将知道被第三方介入了。 通常，攻击者也能得到你请求的域名，但URL中其他部分并不能看到。HTTPS本身并不会暴露域名，但浏览器会首先使用域名去获取IP地址。 对协议的高层级描述在建立TCP连接后，由客户端发起SSL握手。客户端会发送一些规范： 使用的SSL/TLS版本 期望使用何种加密算法（提供一个列表供选择） 期望使用何种压缩算法（提供一个列表供选择） 服务器会挑选出自己支持的SSL/TLS版本中最高的，从客户端提供的加密套件集合中挑选一套，并从中随机挑选一种压缩算法。 做完这些基础设置后，服务器向客户端发送它的证书certiticate。该证书要么被客户端信任，要么被浏览器信任的第三方证书机构信任。例如，若客户端信任GeoTrust，那么浏览器可以信任google.com，因为GeoTrust签发了Google的证书。 客户端验证了证书且确认服务器是其自称的服务器后，会生成一个随机秘钥randomkey，并使用接收的服务器公钥(或其他PreMasterSecret，取决于使用的密码套件)加密，发送到服务器。服务器使用私钥解密出秘钥randomkey。此时，双方可以使用该秘钥来安全的传输数据了。 名词解释： SSL - 该协议会给客户端与服务器之间的通信进行加密，保障通信的安全 TLS - 是SSL的一个新版本，该协议与SSL 3.0之间的差异并不显著，但这些差异的存在，已使得TLS 1.0和SSL 3.0之间不能互操作 所以，目前当谈论SSL时实际指TLS。 参考资料：","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"BlowFish_Gin源码阅读","slug":"BlowFish-Gin源码阅读","date":"2020-03-08T10:34:40.000Z","updated":"2020-03-12T15:30:34.398Z","comments":true,"path":"2020/03/08/BlowFish-Gin源码阅读/","link":"","permalink":"https://straysh.github.life/2020/03/08/BlowFish-Gin%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"持续整理中，不适合阅读 GIN版本 commithash a71af9c144f9579f6dbe945341c1df37aaf09c0dGin框架的特点 快：路由使用radix trie实现，检索路径短。无反射。API性能可预测。 支持中间件：请求可以有多个中间件逐个处理，最后交给业务处理。例如：Logger,Authorization,GZIP,最后写入数据库。 若发生了panic，Gin可以捕获并恢复错误，因此服务并不会终止，且可有机会介入错误恢复的过程。 JSON校验：Gin可以解析并校验请求的json数据，例如检查字段值。 路由分组：更好的组织路由。通过分组将需要鉴权和不需鉴权的路由分开，分组可以无限嵌套且不影响性能。 错误管理：Gin可以和很方便的收集错误信息。最后使用中间件将错误写入文件或数据库或发送到网络上。 内置视图渲染：提供了易用的接口来渲染JSON，XML和HTML。 可扩展：自定义中间件非常容易。 源代码阅读服务启动Socket Server VS HTTP ServerHTTP是应用层协议；Socket是系统提供的抽象接口，它直接操作传输层协议(如TCP、UDP等)来工作。它们不是一个层级上的概念。所以，只要Socket两端不主动关闭连接，就可以通过TCP连接来双向通信。而HTTP服务器则按照HTTP协议来通信：建立TCP连接 🡺 客户端发送报文 🡺 服务器相应报文 🡺 客户端或服务器关闭连接。每一个请求都要重复这个过程。虽说TCP协议是长连接的，但上层的HTTP协议会主动关闭它。另外HTTP中有一个Connection: keep-alive头信息，来重用连接，减少创建连接的消耗。它受到重用次数和超时时间的限制(服务器设置)，触发限制时仍会主动断开连接。因此这个所谓的”长连接”和Socket长连接的本质是不同的。 Socket Server例子，内层的for循环读并不会主动关闭连接(不发生panic时) 123456789101112131415161718192021222324252627282930func main() &#123; srv, err := net.Listen(&quot;tcp&quot;, &quot;:8080&quot;) // 协议，端口 if err != nil &#123; panic(err) &#125; defer srv.Close() for &#123; conn, err := srv.Accept() // 监听连接 if err != nil &#123; fmt.Println(&quot;accept failed:&quot;, err.Error()) continue &#125; go func(c net.Conn)&#123; defer c.Close() buf := make([]byte, 1024) for &#123; n, err := c.Read(buf) // 尝试读数据 if err != nil &#123; fmt.Println(&quot;read failed:&quot;, err.Error()) continue &#125; receiveData := buf[:n] // 接收到的字节buf[0:n] fmt.Println(&quot;received data=&quot;, receiveData) &#125; &#125;(conn) &#125;&#125; HTTP Server 123456789101112131415type handler struct &#123;&#125;func (h handler) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; requestUrl := r.URL.String() msg := fmt.Sprintf(&quot;request uri=%s\\n&quot;, requestUrl) fmt.Println(msg) _, _ = w.Write([]byte(msg))&#125;func main() &#123; err := http.ListenAndServe(&quot;127.0.0.1:8080&quot;, handler&#123;&#125;) //地址、端口，处理句柄 if err != nil &#123; panic(err) &#125;&#125; HTTP Server的底层还是TCP连接，对比上面Socket Server的代码，我们期望在HTTP Server的实现里发现 创建连接net.Listen 网络监听srv.Accept() 读取数据c.Read(buf) 额外的，在服务端发送完数据后，应该要关闭连接 带着以上四个目标，我们来跟一下HTTP Server的启动过程。 启动HTTP Servererr := http.ListenAndServe(&quot;127.0.0.1:8080&quot;, handler&#123;&#125;) &nbsp; 构造server对象 &nbsp; 1234func ListenAndServe(addr string, handler Handler) error &#123; server := &amp;Server&#123;Addr: addr, Handler: handler&#125; return server.ListenAndServe()&#125; 调用server的ListenAndServe方法。在#Line9我们发现了net.Listen(&quot;tcp&quot;, addr)，目标1找到。 1234567891011121314func (srv *Server) ListenAndServe() error &#123; if srv.shuttingDown() &#123; return ErrServerClosed &#125; addr := srv.Addr if addr == &quot;&quot; &#123; addr = &quot;:http&quot; &#125; ln, err := net.Listen(&quot;tcp&quot;, addr) if err != nil &#123; return err &#125; return srv.Serve(ln)&#125; 跟入#Line13行代码srv.Serve(ln) &nbsp;。这里，#Line4:rw,err := l.Accept()，目标2找到。这里的rw即是net.Conn，在#Line14重新包装了rw &nbsp;，，并在#Line14启动协程go c.serve(connCtx)。到此，服务器已经正常启动，并且给每一个新进来的Request都分配了一个协程。#Line3的for循环配合golang轻协程的特性，一个高并发的web服务器启动了。 123456789101112131415161718func (srv *Server) Serve(l net.Listener) error &#123; ... for &#123; rw, err := l.Accept() ... connCtx := ctx if cc := srv.ConnContext; cc != nil &#123; connCtx = cc(connCtx, rw) if connCtx == nil &#123; panic(&quot;ConnContext returned nil&quot;) &#125; &#125; tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(connCtx) &#125;&#125; 继续挖go c.serve(connCtx)看看net/http是如何处理一个Request的。先快速扫一下这个函数里面做了哪些事情： #Line20w, err := c.readRequest(ctx)构建Response对象。向内追找到HTTP协议的解析过程newTextprotoReader。目标3找到。 #Line35serverHandler&#123;c.server&#125;.ServeHTTP(w, w.req) 处理业务逻辑(即用户定义的路由逻辑)。ServeHTTP的第一个参数w就是Response对象，负责向客户端响应数据，w.req即Request，负责解析请求参数、头信息等。 #Line40w.finishRequest()中有flush操作，到这里服务器已经完成了数据响应。 #Line50-64处理了keep-alive重用连接和idle_timeout空闲超时断开连接的逻辑。这里涉及到一些网络知识不具体展开。若设置了Connection: close或者服务器保持连接直到空闲超时，都会return从而执行#Line5中的defer代码,注意源代码中的#Line1775~1777 &nbsp;。目标4找到 需要额外关注一下#Line35行上面的注释 &nbsp;。这里明确指出了net/http没有实现pipeline，理由是在HTTP1.1中pipeline并没有被（客户端/浏览器）广泛的实现，因此扔到了和HTTP2.0一起实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// Serve a new connection.func (c *conn) serve(ctx context.Context) &#123; c.remoteAddr = c.rwc.RemoteAddr().String() ctx = context.WithValue(ctx, LocalAddrContextKey, c.rwc.LocalAddr()) defer func() &#123;...&#125;() ... // HTTP/1.x from here on. ctx, cancelCtx := context.WithCancel(ctx) c.cancelCtx = cancelCtx defer cancelCtx() c.r = &amp;connReader&#123;conn: c&#125; c.bufr = newBufioReader(c.r) c.bufw = newBufioWriterSize(checkConnErrorWriter&#123;c&#125;, 4&lt;&lt;10) for &#123; w, err := c.readRequest(ctx) if c.r.remain != c.server.initialReadLimitSize() &#123; // If we read any bytes off the wire, we&#x27;re active. c.setState(c.rwc, StateActive) &#125; if err != nil &#123;...&#125; // Expect 100 Continue support req := w.req if req.expectsContinue() &#123;...&#125; c.curReq.Store(w) if requestBodyRemains(req.Body) &#123;...&#125; serverHandler&#123;c.server&#125;.ServeHTTP(w, w.req) w.cancelCtx() if c.hijacked() &#123; return &#125; w.finishRequest() if !w.shouldReuseConnection() &#123; if w.requestBodyLimitHit || w.closedRequestBodyEarly() &#123; c.closeWriteAndWait() &#125; return &#125; c.setState(c.rwc, StateIdle) c.curReq.Store((*response)(nil)) if !w.conn.server.doKeepAlives() &#123; // We&#x27;re in shutdown mode. We might&#x27;ve replied // to the user without &quot;Connection: close&quot; and // they might think they can send another // request, but such is life with HTTP/1.1. return &#125; if d := c.server.idleTimeout(); d != 0 &#123; c.rwc.SetReadDeadline(time.Now().Add(d)) if _, err := c.bufr.Peek(4); err != nil &#123; return &#125; &#125; c.rwc.SetReadDeadline(time.Time&#123;&#125;) &#125;&#125; P.S. 这里再额外挖一下#Line35serverHandler&#123;c.server&#125;.ServeHTTP(w, w.req)的实现，将用户代码和net/http包打通。这里首先构造了一个serverHandler对象并调用了它的ServeHTTP方法。&nbsp;之后，调用了sh.srv.Handler.ServeHTTP(rw, req)，这里的srv就是本文步骤2中构造server对象的这个server对象。因此这里的.Handler.ServeHTTP最终调用的是我们的HTTP Serverdemo中#Line4-9的代码。 Gin的启动过程挖完了net/http包，对http网络请求的过程有了一个整体的认知，接下来正式开挖Gin。 启动服务非常简便engine := gin.New()然后engine.Run(&quot;:8080&quot;) &nbsp; 123456789func main() &#123; engine := gin.New() //engine.GET(&quot;/someGet&quot;, getting) ... //engine.Use(middlewares.Authenticate()) engine.Run(&quot;:8080&quot;)&#125; gin.New()的细节。其中Engine的结构 &nbsp;其中： RedirectTrailingSlash若请求地址是/foo/且未匹配，但/foo可以匹配，则将客户端重定向到/foo，若请求是GET则状态码是301，其他动词则是307 RedirectFixedPath未匹配时尝试去除多余的../或//以修正路径(且转化为小写)，例如/FOO或/..//FOO都能匹配/foo HandleMethodNotAllowed未匹配时尝试其他动词，若路由匹配则以状态码405响应，否则将请求代理到NotFound句柄。 1234567891011121314151617181920212223242526272829303132333435// New returns a new blank Engine instance without any middleware attached.// By default the configuration is:// - RedirectTrailingSlash: true// - RedirectFixedPath: false// - HandleMethodNotAllowed: false// - ForwardedByClientIP: true// - UseRawPath: false// - UnescapePathValues: truefunc New() *Engine &#123; debugPrintWARNINGNew() //debug模式下打印开发模式警告 engine := &amp;Engine&#123; RouterGroup: RouterGroup&#123; // 路由分组 Handlers: nil, basePath: &quot;/&quot;, root: true, &#125;, FuncMap: template.FuncMap&#123;&#125;, // 模板函数？ RedirectTrailingSlash: true, RedirectFixedPath: false, HandleMethodNotAllowed: false, ForwardedByClientIP: true, AppEngine: defaultAppEngine, UseRawPath: false, UnescapePathValues: true, MaxMultipartMemory: defaultMultipartMemory, trees: make(methodTrees, 0, 9), delims: render.Delims&#123;Left: &quot;&#123;&#123;&quot;, Right: &quot;&#125;&#125;&quot;&#125;, secureJsonPrefix: &quot;while(1);&quot;, &#125; engine.RouterGroup.engine = engine engine.pool.New = func() interface&#123;&#125; &#123; // 连接池 return engine.allocateContext() &#125; return engine&#125; engine.Run(&quot;:8080&quot;)中的细节。它仅仅是http.ListenAndServe(address, engine)的语法糖，啥也没做。因此可以看出来，Gin对网络底层没做任何处理，直接使用了net/http包。其核心代码全部在Engine这个结构体中。根据我们分析net/http包的经验，Engine中一定实现了ServeHTTP方法 1234567891011// Run attaches the router to a http.Server and starts listening and serving HTTP requests.// It is a shortcut for http.ListenAndServe(addr, router)// Note: this method will block the calling goroutine indefinitely unless an error happens.func (engine *Engine) Run(addr ...string) (err error) &#123; defer func() &#123; debugPrintError(err) &#125;() address := resolveAddress(addr) // addr 是动态参数，默认值取:8080 debugPrint(&quot;Listening and serving HTTP on %s\\n&quot;, address) err = http.ListenAndServe(address, engine) return&#125; engine.ServeHTTP到底干了啥？ Engine结构体的方法集： gin.Context &nbsp; 12345678910111213// ServeHTTP conforms to the http.Handler interface.func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) &#123; // 源码#Line145行定义，这里返回engine.allocateContext()的结果 // 是*gin.Context指针 c := engine.pool.Get().(*Context) // 从连接池中取出一个连接 c.writermem.reset(w) // 重置 http.responseWriter c.Request = req c.reset() // 重置Context engine.handleHTTPRequest(c) // 核心!!! 路由处理逻辑 engine.pool.Put(c) // 执行结束，将连接放入连接池&#125; engine.handleHTTPRequest(c)的细节。 可以看到源码#Line403行调用c.Next()后c.index从-1自增到0，然后调用c.handlers[0]句柄，执行第一个中间件RouteLogger，而在中间件中我们需要再次调用c.Next()。非常明显的一个递归调用，然后执行第二个中间件RecoverWithWriter，之后调用GET动词注册的路由api.Ping，最后调用链路依次返回。参考下图（点击可放大） 路由Gin的路由按HTTP动词，分9组（默认engine.trees = make(methodTrees, 0, 9)）分别对应GET组，POST组，PUT组等。methodTrees是[]methodTree的别名：type methodTrees []methodTree。node是一颗前缀树或Radix trie。 1234type methodTree struct &#123; method string // 即HTTP动词，如GET root *node // 路由链路&#125; Trietrie译为字典树或单词查找树或前缀树。这是一种搜索树——存储动态集合或关联数组的有序的树形数据结构，且通常使用字符串做键。与二叉搜索树不同，其节点上并不直接存键。其在树中的位置决定了与其关联的键。所有的子节点都有相同的前缀，而根节点对应的是空字符串。键只与叶子节点关联。 trie术语的发明者念/ˈtriː/(tree)，而有些作者念为/ˈtraɪ/以便和tree区别。 下图是一颗字典树，描述了键值为A、to、tea、ted、ten、i、in、inn的情况。（图中节点并不是完全有序的，虽然应该如此：如root节点与t节点） 不难想象，字典树典型的应用场景是单词计数。 trie通常用来取代hash table，因为有如下优势： 在最坏的情况下，trie的时间复杂度是O(m)，其中m是字符串的长度。但哈希表有key碰撞的情况，最坏的情况下其复杂度是O(N)，虽然通常是O(1)，且计算哈希的复杂度是O(m)。 trie中没有碰撞。 当trie中一个key对应多个值时，会使用buckets来存储多个值，与哈希表中发生碰撞时使用的桶相似。 不论有多少个key，都不需哈希函数或重哈希函数。 key的路径是有序的。 但同时，相对哈希表，trie有如下缺点： trie的搜索通常比哈希表慢，特别是需要从硬盘上加载数据时。 浮点数做key通常导致链路过长。 有些trie可能比哈希表需要更多的空间，因为每一个字符都要分配内存。而哈希表只需要申请一块内存。 Radix Treeradix tree也叫radix trie或compact prefix trie。在字典树中，每一个字符都要占一个节点，这样造成树过高。radix trie则将唯一的子节点压缩到自身来降低树的高度。 参考资料： 字典树 Radix树 Trie Data Structure Tutorial - Introduction to the Trie Data Structure Trie and Patricia Trie Overview 图解Redis中的Radix树 Linux 内核数据结构：Radix树 解析请求参数渲染JSONsession &amp; cookieURL重定向goroutin inside a middleware日志模块debug日志/debug.go#L55 1234567891011func debugPrint(format string, values ...interface&#123;&#125;) &#123; if IsDebugging() &#123; if !strings.HasSuffix(format, &quot;\\n&quot;) &#123; format += &quot;\\n&quot; &#125; // DefaultWriter是在项目bootstrap阶段配置的写句柄 // 可以通过DefaultWriter=io.MultiWriter(...)自定义 // 也可以使用默认值os.Stdout见/mode.go#L31-38 fmt.Fprintf(DefaultWriter, &quot;[GIN-debug] &quot;+format, values...) &#125;&#125; /debug.go#L97 12345678func debugPrintError(err error) &#123; if err != nil &#123; if IsDebugging() &#123; // DefaultErrorWriter is the default io.Writer used by Gin to debug errors fmt.Fprintf(DefaultErrorWriter, &quot;[GIN-debug] [ERROR] %v\\n&quot;, err) &#125; &#125;&#125; 路由日志/logger.go#L131 1234567891011121314151617181920212223242526// defaultLogFormatter is the default log format function Logger middleware uses.var defaultLogFormatter = func(param LogFormatterParams) string &#123; var statusColor, methodColor, resetColor string if param.IsOutpu123456 or() &#123; statusColor = param.StatusCodeColor() methodColor = param.MethodColor() resetColor = param.ResetColor() &#125; if param.Latency &gt; time.Minute &#123; // Truncate in a golang &lt; 1.8 safe way param.Latency = param.Latency - param.Latency%time.Second &#125; // 默认日志格式： // [GIN] 时间戳|HTTP_Code|响应时间|客户IP| http_verb url 错误信息 return fmt.Sprintf(&quot;[GIN] %v |%s %3d %s| %13v | %15s |%s %-7s %s %#v\\n%s&quot;, param.TimeStamp.Format(&quot;2006/01/02 - 15:04:05&quot;), statusColor, param.StatusCode, resetColor, param.Latency, param.ClientIP, methodColor, param.Method, resetColor, param.Path, param.ErrorMessage, )&#125; /logger.go#L203 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// LoggerWithConfig instance a Logger middleware with config.func LoggerWithConfig(conf LoggerConfig) HandlerFunc &#123; formatter := conf.Formatter if formatter == nil &#123; formatter = defaultLogFormatter &#125; out := conf.Output if out == nil &#123; out = DefaultWriter &#125; notlogged := conf.SkipPaths isTerm := true if w, ok := out.(*os.File); !ok || os.Getenv(&quot;TERM&quot;) == &quot;dumb&quot; || (!isatty.IsTerminal(w.Fd()) &amp;&amp; !isatty.IsCygwinTerminal(w.Fd())) &#123; isTerm = false &#125; var skip map[string]struct&#123;&#125; if length := len(notlogged); length &gt; 0 &#123; skip = make(map[string]struct&#123;&#125;, length) for _, path := range notlogged &#123; skip[path] = struct&#123;&#125;&#123;&#125; &#125; &#125; return func(c *Context) &#123; // Start timer start := time.Now() path := c.Request.URL.Path raw := c.Request.URL.RawQuery // Process request c.Next() // Log only when path is not being skipped if _, ok := skip[path]; !ok &#123; param := LogFormatterParams&#123; Request: c.Request, isTerm: isTerm, Keys: c.Keys, &#125; // Stop timer param.TimeStamp = time.Now() param.Latency = param.TimeStamp.Sub(start) param.ClientIP = c.ClientIP() param.Method = c.Request.Method param.StatusCode = c.Writer.Status() param.ErrorMessage = c.Errors.ByType(ErrorTypePrivate).String() param.BodySize = c.Writer.Size() if raw != &quot;&quot; &#123; path = path + &quot;?&quot; + raw &#125; param.Path = path fmt.Fprint(out, formatter(param)) &#125; &#125;&#125; Build a single binary with templatesSee a complete example in the https://github.com/gin-gonic/examples/tree/master/assets-in-binary directory. http2 server pushhttps on port 8080 go服务要不要配nginx前端 参考阅读: Gin的路由为什么这么快? 参考资料:","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"Golang_汇编基础","slug":"Golang-汇编基础","date":"2020-03-01T12:21:23.000Z","updated":"2020-03-01T12:25:29.414Z","comments":true,"path":"2020/03/01/Golang-汇编基础/","link":"","permalink":"https://straysh.github.life/2020/03/01/Golang-%E6%B1%87%E7%BC%96%E5%9F%BA%E7%A1%80/","excerpt":"","text":"待填坑 参考资料 深入Go的底层，带你走近一群有追求的人 深度解密Go语言之Slice #34 go plan9 汇编入门，带你打通应用和底层 - Go夜读","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"汇编","slug":"汇编","permalink":"https://straysh.github.life/tags/%E6%B1%87%E7%BC%96/"}]},{"title":"健康码是个被忽视的奇点事件","slug":"健康码是个被忽视的奇点事件","date":"2020-02-28T13:11:55.000Z","updated":"2020-02-28T13:19:28.517Z","comments":true,"path":"2020/02/28/健康码是个被忽视的奇点事件/","link":"","permalink":"https://straysh.github.life/2020/02/28/%E5%81%A5%E5%BA%B7%E7%A0%81%E6%98%AF%E4%B8%AA%E8%A2%AB%E5%BF%BD%E8%A7%86%E7%9A%84%E5%A5%87%E7%82%B9%E4%BA%8B%E4%BB%B6/","excerpt":"","text":"文章来源：健康码是个被忽视的奇点事件健康码得以迅速推广，本质上是人和信息的紧密结合 大家都在谈健康码，但这件事情的重要性，可能还是被低估了。我们可能正在平静地度过一个历史性的奇点时刻。 2020年2月11日，疫情最严重的时候，健康码在杭州率先推出，用“红黄绿”三色二维码作为数字化健康证明，居民和来杭人员只需上钉钉或支付宝领取，绿码可凭码通行，红码和黄码需按规定隔离并健康打卡。 健康码的出现，居民不再需要重复填报健康表格，高速路口和小区卡口工作人员也更加便利，实现了“无接触式”查验，降低了感染风险。按照杭州市政府发布的企业复工申报指南，企业复工需要在钉钉上申报，在审核通过之后，企业员工每天也需要在钉钉上进行健康打卡。 因为种种的优点，健康码在全国迅速得到广泛应用，截至2月24日，健康码已在全国近200个地市上线。 健康码上线之后，我和阿里钉钉CEO无招通了个不长的电话，主要是他在说健康码的思路和经过，但这个过程中，我脑子里突然有一个念头亮了——我突然意识到健康码这个产品的成功，其实顺应了一个在过去10年缓慢发生的巨大变化，那就是人和信息不再是原来分离的状态，正在融为一体。 “强信息人”的时代 这是一个不可小觑的变化。因为在过去的成千上万年时间里，我们都是和信息处于分离状态的，自身产生、携带和可分享的信息是非常少的。可以说，整个世界的架构，其实都是在人和信息分离的前提下搭建的。从语言、文字的出现，到印刷术、电报、电话、互联网的发明，都是为了让人能更好地找到信息。 克劳德·香农在1948年奠定信息理论基础的论文《通信的数据理论》里就写到： “通信的基本问题是，在一点精确地或者紧似地复现在另一点所选取的讯息。这些讯息往往都带有意义。”想想唐僧师徒一行为了取经要走十万八千里，就知道那时人和信息分离的情况有多严重了。 之前有很多书籍和文章都在谈论因为广泛的连接，而给社会和世界带来的改变——改变不仅是因为你可以在手机上观看视频，更是因为你为什么可以在手机上看视频。而移动互联网和智能手机的普及，使得智能手机几乎成为地球上几十亿人身上突然增加的一个信息器官。从对人类的改变来说，这相当于一个发生在最近十年内的巨大基因突变，人类自身一下可以自己生成、携带、共享（主动和被动都包括）相当于过去无数倍的海量信息，正在成为网络里面一个愈发强力的节点，完全可以把现在的自己称为“强信息人“。要知道，根据技术史学家乔治·戴森（ George Dyson）收集的数据，在 1953年 3月，世界上只有 53K字节的高速存储空间（ RAM）。今天，我们裤兜里随便一个手机的存储空间就是这个数字的10万倍。 在欧洲，GDPR将产生可识别个人数据的人视为“数据主体”（data subject），其实也是从法律上认定并保护了信息与人的高度融合这一现实。 杭州市委副书记在接受白岩松在《新闻1+1》 中的采访时，曾专门提及健康——健康码的评判来源于三个维度，一个维度是空间，根据疫情风险程度，杭州市的大数据公司按照有关数据已经可以精确到乡镇（街道）；第二个维度是时间，某人去过疫区的次数以及时间的长短，这个与传染路径有关联；第三个是人际关系维度，与密切接人员接触状态等个人有效信息，量化赋分后最终生成相应的三色码。这充分说明了现在很多信息只有互相连接起来时，才是完整的或是有用的。 最近这几年，我们的身上已经早已被贴上了无数层密密麻麻的数字标签，人和信息早已经紧密地融合为一体。领导提及的三个维度，都是我们随身携带、唾手可得的信息，而生成的健康码，其实又成为新的信息和标签。 10年前，我们可以9点打开电脑，5点关掉它；现在是手机持续开着，我们定时睡觉和醒来。当我们身上贴了足够多的标签时，标签已经开始和外界频繁互动，且可以反作用于我们的时候，我们就会感知到自己已经和信息融为一体了。很简单的一个例子，我们可以感受一下——10年前，我们可以9点打开电脑，5点关掉它；现在是手机持续开着，我们定时睡觉和醒来。 有一本书《第七感》，里面谈到古生物学家斯蒂芬·杰·古尔德（ Stephen Jay Gould）将这种物种进化过程中的突变称为“间断平衡”（ punctuated equilibrium），即世界从一种均衡态跳入另一种均衡态，永无回头之时。古尔德主要是指恐龙的灭绝，实际上这一理论同样适用于思考历史。 我们可能正处在这个时刻。其实中国移动支付的普及，也是人和信息不断融合的结果。只是这个过程太长了，我们几乎没有意识到这种潜移默化的渐变。而健康码的意义，不仅仅对于防控疫情有很大帮助，更是因为时间很短，帮助我们感知到了发生在身边的巨大变化。 爬出经验的河流 《第七感》有句话非常带感——20世纪 20年代华尔街的拓扑结构，很大程度上取决于某个人在特定的某一天正好去了交易大厅。 人类社会是一个复杂的系统，而任何一个系统都包括三种构成要件：要素、连接、功能或目标，因为连接的改变，人作为这个系统里最重要的一个要素，也发生了巨大的变化。在欧洲启蒙运动的时候，一位农民从农奴变成市民，他的政治地位、经济前景、教与学的能力也会随之改变。这一变革触发了欧洲社会长达几百年的分裂和动荡，世界也因此重新布局。当一个人结合了不同的信息，放在不同的场景之下，他可以释放的能量，起到的作用，也是完全不同的。 我们都要保持对变化高度警醒，因为变化的巨流改道，会淹没众多曾经信赖的路径。美军特种作战的实践和数字化师的建立就是一个典型的例子，当一个士兵结合了不间断的通讯，更清晰且通过卫星和侦察机持续更新的地图、随时随地的空中甚至太空支援，甚至透过墙壁和其他阻碍探测敌军的热感应器的时候，他的作战效率和战场规则，和过往成千上万需要层级调动指挥的只会服从的士兵，是完全不同的。 我们都要保持对变化高度警醒，因为变化的巨流改道，会淹没众多曾经信赖的路径。 这次疫情，湖北政府在初期的手足无措、漏洞百出，很大程度上也是因为强信息人的出现，传播已经改变，每个人都有能力向外发出足够大的声音，但防疫和政府治理体系却没有跟上。 对企业来说，这次疫情的停摆，真正摧毁了一些传统的入口和行为习惯，结合信息和人高度融合的现状，加速了很多变化的到来。人类的习惯往往比感知转变得更快，疫情期间所做的很多不得已的变化，有很多都会在未来留下深刻的痕迹，新力量正毫不留情地对旧架构施加巨大的压力。 作为企业家，要想处理好未来强信息人员工和客户的关系，大可借鉴美军特种部队的支撑体系，和云计算架构给出的思路——那就是围绕着强节点，建立去层级、分布式、网络化的连接体系。这一次疫情，也同样有一些企业，例如林清轩，借助远程协同、在线销售，发挥一线员工的能动性，反而取得意想不到效果，这就可以视作正确的释放了强信息人员工的能量。其实，他们的能量就在那里，只是疫情给了他们机会——就像米开朗基罗说的那样——我只是把大卫从石头里释放出来。 健康码这个产品的成功，不是偶然的幸运，更应该是一种顺理成章。 同样，健康码这个产品的成功，也不是偶然的幸运，更应该是一种顺利成章。回头看看钉钉的历次版本的升级，会发现其实钉钉一直在调试人和信息的关系。健康码的成功，只是找到了最适合的产品形态，在最需要的时间和场景下，把这种关系呈现了出来。 钉钉最开始的特色功能“Ding”，成功地让自己从一开始就形成了鲜明的差异化，但也因此招到一些“反人性”的质疑。无招的解释是，“Ding”这个功能最初的设置目的，就是希望信息在需要的时候，能够到达正确的人。 的确，定位于工作场景的钉钉，对抗的是人的惰性。但回头看看“钉“的想法，还是没有脱离开人和信息分离的惯性思维，还是有中心化、层级化思维的影子，所以动作还是有些僵硬，也因此才会有质疑，有反弹。 不过随后钉钉团队在频繁的产品迭代中，持续引入审批、签到、工作方式、以及最新5.0发布的在线办公室、圈子等新功能，在不停地探索人和信息结合的最顺畅的场景。更重要的是，钉钉天生是一个通信平台，所有的发力其实都是做好连接和信息汇集，无形之中形成了一个去层级的、分布式开放平台，给了用户很多创新的空间。 例如复星集团就曾提了一个非常好的问题：系统到底服务于谁，是员工还是领导？这个问题的背景是复星要应对多文化、多时区，多企业的管理挑战，这个提法，哪怕是在3年前，都是难以想象的。而目前复星在钉钉上形成的开放式的新型组织形式——员工实时在线，打破边界，信息不加工，不过滤完全透明——追求的是适应性和效率，是典型的“失控”，而不是控制带来的安全感觉。这样灵活柔软且实时在线的组织，在面对现在越来越频繁的市场巨变时，就会从容很多。 以前应用最多的企业级管理系统，还是最早MRPII、ERP以流程为中心的设计思路，追求的是控制和有序，人和数据围绕着流程转；随着互联网公司的崛起，以数据为中心的系统建设思路抬头，这也是为什么最近数据中台火爆一时的原因。未来，会不会有更多真正以人（包括员工、客户、上下游合作伙伴）为中心的系统会涌现出来，去进一步释放人的价值呢？ 这样一来，一个企业会越来越具有生态的样子，在失控无序和敏捷复杂之间找到平衡。但是这样做的代价也是明显的——系统是无法被精确地设计的，同时也很难被完全控制——前几天钉钉被小学生恶作剧打一星求饶的事件，就是一个鲜活的例子，00后生下来就是强信息人，是数字化的原住民。而钉钉对于孩子们的得体回复，也可以看作是和全新用户打交道的实验。 但这样以人为中心的分布式网络化架构，最大的好处是很容易达成泛组织、跨组织数字化协同联动，这次健康码达成的“政府-企业-员工”，以及招致小学生恶搞的在线教育“政府-学校-班级-学生”，其实就是非常好的协同案例。无招自己也表示，健康码给他带来很多新的启发：“以前钉钉数据在线讲得比较多，没有意识到社会、城市、区县、乡村、小区街道、企业等组织全面在线化带来的深层次社会变革，今天我们从钉钉的健康码，钉钉的码上复工中开始突然有了体感。在线意味着每一个组织，每一个人都数字化了，每一个人的公开行为和组织执行过程都成为一个数字片段，组织和人、人和行为、组织和政策、政策和组织执行（被管理者和管理者的行为）都通过数字化变成了一个在线的虚拟镜像，而且是实时的镜像。你想象一下如果决策者坐在这个镜像面前，还能实时操作政策的变更，立刻看到变更的结果，实时分析实时优化，这个社会的进化速度会不会几何倍数的增长？” “集中-分散”是永恒的矛盾和循环，也是推动网络进化的力量之源——连接的发达趋向集中，而节点的强大则带来分散，而网络就是在这样“集中-分散“的角力中不断前进演化的。在一个网络里，“集中-分散”是永恒的矛盾和循环，也是推动网络进化的力量之源——连接的发达趋向集中，而节点的强大则带来分散，而网络就是在这样“集中-分散“的角力中不断前进演化的。在这样的模式里，钉钉代表的就是日益发达的连接，而每个强信息人就是新的节点，这种新的连接和节点融合在一起带来的科学、透明、公开、高效，可以在很大程度上消除大组织、大社会信息传递的低效和歪曲，执行的不透明和不统一，阶层人群的不平等形成的网络张力，会对旧结构带来巨大冲击。 这些都是技术带来的进步和普惠，在这个基础上，新数字化体系正在萌芽，一个个未来的在线数字化经济体会孕育出来。从这个角度来看，钉钉对于国家和社会的重要性，在未来可能会超越淘宝带来的社会价值。 同时，因为信息和人的关系越来越紧密，信息安全和隐私问题，科技与治理的问题，也会成为未来极其重要且敏感的议题。这也是个宏大的话题，限于篇幅，就不展开了。（本文首发钛媒体，作者/刘湘明） 文章来源：健康码是个被忽视的奇点事件","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"},{"name":"Banbrick","slug":"Banbrick","permalink":"https://straysh.github.life/tags/Banbrick/"}]},{"title":"杂记_负责任地说，这次疫情已经成为压垮全球化的最后一根稻草","slug":"杂记-负责任地说，这次疫情已经成为压垮全球化的最后一根稻草","date":"2020-02-28T12:08:54.000Z","updated":"2020-02-28T12:47:38.450Z","comments":true,"path":"2020/02/28/杂记-负责任地说，这次疫情已经成为压垮全球化的最后一根稻草/","link":"","permalink":"https://straysh.github.life/2020/02/28/%E6%9D%82%E8%AE%B0-%E8%B4%9F%E8%B4%A3%E4%BB%BB%E5%9C%B0%E8%AF%B4%EF%BC%8C%E8%BF%99%E6%AC%A1%E7%96%AB%E6%83%85%E5%B7%B2%E7%BB%8F%E6%88%90%E4%B8%BA%E5%8E%8B%E5%9E%AE%E5%85%A8%E7%90%83%E5%8C%96%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E6%A0%B9%E7%A8%BB%E8%8D%89/","excerpt":"","text":"文章来源：反者道之动弱者道之用疫情还在持续，这次疫情能给世界带来什么，众说纷纭，但有一点是确定的，全球化时代的句点到来了。 全球化作为某种大同世界的雏形，吸引过无数的人为之奋斗，而且，客观地说，全球化也确实给许多国家带来了飞跃。 以中国为例，如果没有全球化，没有产业链的跨国分布，中国根本不可能有经济起飞的机会。 正是因为中国人接下了西欧北美转移的低端产业，从最苦最累的活做起，一点点建成了世界第二大经济体。 毫不夸张地说，中国是全球化的最大受益国，改革开放的四十年，就是中国拥抱全球化的四十年，全球化为中国的发展贡献了无数的资源，而中国也以庞大的市场和叹为观止的生产能力为全球化提供了无穷动力。 然而，这一和谐的图景在过去五年出现了大逆转。人们发现，全球化看上去美好，却会带来巨大的危险。 第一，全球化让很多国家的产业狭隘化了。 由于全球协作，各国都在发展有比较优势的产业，像中国的纺织、中东的石油，澳洲的矿石，美国的高科技，都是全球分工的结果。风平浪静时，一切协调而美好，然而，只要一环出现问题，就会牵一发动全身。 还以中国为例，前几年，若不是总理大人在一次谈话中露出了信息，大部分国人都不知道中国连生产圆珠笔尖的能力都没有。 很多人惊讶：我们能建世界上最好的高铁，为什么做不了小小的笔尖？其实，这没什么好奇怪，既然有人做，不见得非得自己来。 今天，笔尖问题已经解决了，但是，没有“解决”的问题仍然数不胜数。 美国一个芯片禁运，打得中兴几近倒闭，好多民众咬牙切齿地说，一定得发展自己的芯片，免得受制于人。 话是这么说，但你就算解决了芯片，就没有别的事吗？上亿吨的大豆总不能都自己种吧？每年进口的四亿吨石油，不能都从自己的地上挖吧？ 各司其职，协同配合，这本是全球化的基本逻辑。芯片受制于人，根子不在芯片，而在中美关系。换言之，只要是全球化，总会有东西受制于人，中国要是和乌克兰关系闹僵了，中兴倒是没事，可下一艘军舰可能就没有燃气轮机了。 中国的窘境也是各国的窘境。产业的狭窄化让所有的国家都如履薄冰，既怕别人出事断了自己的供应，更怕自己丢掉产业链上的位置，失去发展机会。 其实，纵使像美国这样产业大国，也不能置身事外。美国汽车工业的衰败，导致汽车城底特律的破产；制造业的外迁，催生了五大湖湖畔漫长的“铁锈”地带。 三十年前，美国打掉了日本的发展势头，却没有换来家电产业的复兴；二十年前，不少议员还在阻止中国的纺织品的倾销，可到了今天，它的纺织业已被彻底摧毁。 曾有一位美国的高官忧心地谈到，美国许多战斗机上的显示屏自己不能生产，这可怎么得了！如果某天要与那个显示屏生产国开战，战斗机岂不是要断供？ 这就是全球化。 面对这种局面，很多国家开始思考，产业的大转移，真的是好事吗？ 第二，全球化让全球贫富差距扩大。 全球化通过资源的自由流动，实现了优化配置，进而产生了更高的效率，更低的价格，以及更丰厚的利润，而所有这一切，对于那些因产业转移而承担消极后果的人们来说，则是另外一番图景。 在全世界范围内，确实一些国家因全球化而强大，但更多的国家，因为产业、人才、资金以及技术的抽离，每况愈下。 而在那些所谓“强大”的国家里，同样因为产业、人才、资金和技术的流动，发达地区更发达，落后地区更落后；少数阶层更富有，多数阶层更贫穷。 于是，最奇怪的情况出现了：在经历了70余年的狂奔后，所有国家，无论是受益国还是受损国，几乎都对全球化不满意。 第三，全球化令那些最不该全球化的东西全球化了。 在全球化之初，人们只是想到经济的效率将因资源的自由流动而提高，谁也没想到的是，许多糟糕的东西也全球化了。 首先，难民全球化了。众所周知，中东的难民几乎摧毁了繁荣富裕的西欧，然而，这并不全是难民的错，若没有欧洲各国对劳动力的渴求，谁也不会打开大门。结果，欧洲的一手好牌打丢了，成千上万的难民不仅没有带来有效的劳动力，反倒带来了无数的犯罪，以及对社会资源的天量消耗。 现在，数万墨西哥及中南美的难民又挤到了美墨边境，试图进入美国。 其次，极端宗教思想全球化了。西方世界常年与伊斯兰原教旨主义作战的结果是，恐怖主义作为实体被几近消灭，但恐怖主义的思想却依托着难民以及不断增加的伊斯兰教人口在欧美落地生根。今天，欧洲主要大城市都在伊斯兰化，荷兰、德国、英国从人口构成上看，50年内就将成为穆斯林国家。 再次，武器贩运与贩毒也全球化了，如中亚的塔吉克斯坦，几乎以从阿富汗向俄罗斯转运毒品为生，贩毒占了这个国家GDP的40%；疾病与瘟疫也全球化了，非洲的猪瘟直接导致了中国的猪肉价格上涨了120%，HIV成为了全球性的绝症；还有，像洗钱、走私、绑架、奴隶贩运……都全球化了。 越来越多的人们发现，全球化已经到了“利好出尽是利空”的转折点，再这么下去，祸害无穷而获利甚微，于是，美国选出了特朗普，英国开始脱欧，整个西方世界，保守派领袖纷纷上台，民族主义开始大回潮。 同时，各国也开始重建自己的产业系统；强化货币和金融主权；弱化联合国和其他国际组织的作用；或多或少地封闭边境，限制移民；重新回归宗教和传统价值观；部分国家甚至整军备战。 这个局面，有人称之为逆全球化。 不过，仍有一些国家对全球化抱有希望，或者说，他们对全球化就这样落幕并不甘心。 这些国家，或是产能大，需要为全球提供服务，或是市场大，需要消化大量境外来的商品，或是资源多，需要向境外巨幅出口，再或是技术依赖大，必须依靠全球的智力。总之，对这些国家而言，全球化的好处还是远远大于坏处。 中国就是这样的一个国家。 当然，不止中国，印度也是，还有东盟各国。 由于利益攸关，这些年来，中国仍在逆势为全球化奔走呼号，甚至试图建立一个全球化的次级版本——“一带一路”。 中国试图告诉世界：产业狭窄没那么可怕，只要国际社会总体上和平发展，国际分工还是利大于弊的。贫富差距扩大也不是大问题，毕竟先富者可以用资金和技术帮助后进者；全球化引发的社会问题也并非不可控，只要有针对性地处理好恐怖分子、跨国犯罪等棘手问题，全球化总体上还是一片光明。 可惜的是，突如其来的新冠肺炎打破了这一切。 这次肺炎疫情给本已降温的全球化又生生地泼了一盆冰水，让所有的国家都意识到了下面两个关键问题： 第一，像中国这样体量的大国忽然停摆将发生什么？ 很多人都预测，这次疫情将重创中国经济，但很少人意识到，肺炎疫情也会重创世界经济。因为大量的中国工厂因疫情无法开工，严重缩减了全世界的商品供应。 全球连锁的沃尔玛超市有70%的商品来自中国，中国停工三周，沃尔玛的供应链就会断裂。如果中国停工三个月，世界上就没有沃尔玛了。 韩国的现代汽车由于大量的零部件来自中国，现在中国的疫情直接影响到了现代汽车的产量，许多生产线已经关闭。 苹果手机的代工厂主要在中国，按最乐观的情况估计，今年的销量也要下降10%。 前些天大家都在为日本的援助而感动，但真实的情况是，日本只能救急而不能救穷，因为日本市面上的口罩主要是中国生产的，现在日本的病例已经数百，日本政府已经在认真的考虑，如果中国持续无法开工，疫情又在日本爆发，日本的口罩从哪里来？ 美国的疫情还算好，但美国市面上的几乎所有医用防护服都产自中国，如果美国同时爆发大规模疫情，后果将比中国还要惨烈。 第二，在各国治理模式不一的情况下片面推进经济全球化是否明智？ 全球化最深刻矛盾，莫过于社会治理的各自为战与资源全球化配置之间的冲突。 通俗点说，经济上虽然协同一致，但政治上却天差地别，早晚会出事。 以这次肺炎为例，一国爆发瘟疫，它的信息是否第一时间会向国际社会公布？真实情况是否会有隐瞒？瘟疫的原发国应对是否得力？它的社会资源是否足够？它是否会接受他国的建议和要求？这些问题都将对世界构成巨大挑战。 这就好比一个单元楼有多个住户，一家发生火灾，如果这家人呼救得不及时，处理得不得当，最倒霉的其实是邻居。 现在的全球化就是这样一幅画面，一个国家的行为虽然会影响全球，但他到底要怎么做，能怎么做，又做过什么，其他国家既无信息来源，也无权置喙。 这次疫情就充分体现了这种脱节。虽然民间营造的气氛是“山川异域，风月同天”，但各国在如何应对这场瘟疫的问题上几乎吵到人仰马翻。 中国外交部说美国没有对中国提供实质性援助，美国反驳说它们其实才是捐助最多的国家；世界卫生组织希望各国不要对中国进行旅游限制，但一百多个国家先后都与中国断了航班；中国说有些国家的撤侨产生了恐慌，可朝鲜作为传统意义上的中国盟友却第一个封闭了边境。 此外，就病毒的来源，传播的途径，真实病例的人数，疫情控制的现状，中国政府应对方式的合理性等问题，国际社会始终存在不同看法。 其实，这也不难理解，这么大的疫情发展下去，对世界各国都会有影响，大家当然会去找最优的处理方案，并且七嘴八舌的提些意见，可是，国际社会是天然无政府的，想统一声音，一同行动，又谈何容易？ 于是，几次碰壁后，不少国家有意无意地会走向另一个思路： 首先，既然与别国进行产业协作风险这么大，那把各类产业放在自己手中总不会错吧？实在自己不能掌控的，就尽量分散，鸡蛋反正不能放在一个篮子里。 其次，与其坐等像新冠肺炎这样的疫情爆发，自己既伸不上手又说不上话，还不如早早远离，当年非洲的埃博拉病毒为什么没有波及欧美？还不是因为非洲根本没有纳入全球化么。 你看，一场肺炎，不仅坐实了全球化退潮的事实，还让那些模棱两可，似是而非的观念，那些在保守主义者心头萦绕却又难以启齿的话语，纷纷表露出来了。 所以，负责任地说，这次疫情已经成为压垮全球化的最后一根稻草。 我估计全球化将从现在彻底刹车，并缓缓地开始倒退。 虽然，这并不意味着世界将回到彼此孤立的年代，但全球化显然不会再前进和深化了，这对中国而言，绝不是好消息。 在可见的未来，出于对新冠肺炎负面影响的恐惧，会有更多的外国公司关闭在中国的分支机构，缩减生产能力，撤出大量资本，甚至是将工厂整体迁出。 如果疫情持续下去，这些公司撤出资源的速度还会加快。而那些离开的人、财、物，只要撤走，短时间内就不会再回来。 前两天我和一位朋友说，如果疫情不能在两个月内结束，疫情之后的中国经济，将会像被核弹炸过一样。 你也许会问，那全球化还有没有未来呢？答案是：会有的，但彼时的全球化必须有一个条件： 全球社会治理模式的高度一致，甚至是统一。 简言之，经济的全球化必须以政治的一体化为前提。 因为，只有上层管理的统一，才能一方面实现资源全球配置的最大效益，另一方面又有效地遏制其负面影响。 当然，这种政治一体化的具体形态我们今天还无法预测，但可以肯定的是，它一定不同于现存的任何一种政治体制，因为全球化所需要的社会治理模式，至少要满足国际社会协同行动的基本需要，而当下各国的政治体制均无法达到这个目标。 其实，我一直觉得，全球化本来就应该是一项包含政治、经济、社会、文化等多方面的系统工程，但现在看，我们从一开始就走偏了，只顾着发展经济，忘记了其他的，结果，还得退回去重新走。 不过，最终实现目标那天，应该是在很遥远的未来了。","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"},{"name":"Banbrick","slug":"Banbrick","permalink":"https://straysh.github.life/tags/Banbrick/"}]},{"title":"MySQL_页分裂","slug":"MySQL-页分裂","date":"2020-02-28T06:49:54.000Z","updated":"2020-02-28T10:41:20.623Z","comments":true,"path":"2020/02/28/MySQL-页分裂/","link":"","permalink":"https://straysh.github.life/2020/02/28/MySQL-%E9%A1%B5%E5%88%86%E8%A3%82/","excerpt":"","text":"文章来源：InnoDB Page Merging and Page Splitting 表结构假设有张表名为windmills，其数据文件目录结构可能如下： 1234data/ windmills/ wmills.ibd wmills.frm 数据存储在一个叫wills.ibd的文件中。该文件由N个segments组成，每个segments都关联着索引。 在删除数据行是，该文件的尺寸不会改变，segment自身会增大或缩小取决于其名为extent的子元素。一个extent只能存在于segment中并且固定大小为1MB（若page是默认页大小时）。一个page是extent的子元素且有默认16KB的大小。 因此，extent最多有64pages。一个页可以包含2~N的数据行。具体的行数取决于行的大小（在表结构中定义了）。Innodb中有一个强制规定，即一个页至少要包含两个数据行，由此有了另一个规定：数据行不能超过8000字节。InnoDB uses B-trees to organize your data inside pages across extents, within segments. Roots,Branches,and Leaves每个页(leaf)包含2~N的数据行，由主键索引来组织其结构。该树形结构有一些特殊的页来管理树枝(branches)，即所谓的internal nodes(INodes)。This image is just an example, and is not indicative of the real-world output below.上图对应的具体细节如下： 123456ROOT NODE #3: 4 records, 68 bytes NODE POINTER RECORD ≥ (id=2) → #197 INTERNAL NODE #197: 464 records, 7888 bytes NODE POINTER RECORD ≥ (id=2) → #5 LEAF NODE #5: 57 records, 7524 bytes RECORD: (id=2) → (uuid=&quot;884e471c-0e82-11e7-8bf6-08002734ed50&quot;, millid=139, kwatts_s=1956, date=&quot;2017-05-01&quot;, location=&quot;For beauty&#x27;s pattern to succeeding men.Yet do thy&quot;, active=1, time=&quot;2017-03-21 22:05:45&quot;, strrecordtype=&quot;Wit&quot;) 表结构: 12345678910111213CREATE TABLE `wmills` ( `id` bigint(11) NOT NULL AUTO_INCREMENT, `uuid` char(36) COLLATE utf8_bin NOT NULL, `millid` smallint(6) NOT NULL, `kwatts_s` int(11) NOT NULL, `date` date NOT NULL, `location` varchar(50) COLLATE utf8_bin DEFAULT NULL, `active` tinyint(2) NOT NULL DEFAULT &#x27;1&#x27;, `time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `strrecordtype` char(3) COLLATE utf8_bin NOT NULL, PRIMARY KEY (`id`), KEY `IDX_millid` (`millid`)) ENGINE=InnoDB; B-Tree都有一个根节点，本例子中根节点是#3。root页（即根节点）包含有索引的ID值，其INodes数量等信息。INodes页包含自身的属性，取值范围等信息。最后，是叶子节点(leaf nodes)，这里我们能获取到数据行。在这个例子中，我们可以看出来，#5号叶子节点包含57行数据，总共7524字节。 我们使用表和行来维护数据，而Innodb使用branches，pages，和records来维护数据。必须牢记一点，Innodb并不直接操作数据行，而是操作pages。当从磁盘上加载一页后，他会扫描改页来确定数据行(row/record)。 Page Internalspage可以是空的也可以是装满的。数据行由主键组织，若你的表有AUTO_INCREMENT属性，那么主键就是顺序的，如ID=1,2,3,4等等。 page有另一个重要的属性：MERGE_THRESHOLD。其默认值是页大小的50%，它在Innodb的合并过程中很重要。 插入数据时，页顺序填充（若装得下）。当页满了，下一行数据会填充到下一页中。 我们不仅能自上而下的遍历branches，也能水平遍历叶子节点。因为叶子节点含有指向下一叶子节点的指针。 例如，page#5有指向下一节点页#6的指针。而页#6含有一个指回页#5的指针，同时有一个指向下一节点页#7的指针。 该结构能加速顺序扫描的速度（如范围检索）。上述是对AUTO_INCREMENT的插入场景的描述。但若我们执行删除操作呢？ Page Mergeing当删除一行时，它并不会物理上删除该数据。而是标记为已删除，其占用的空间是可申领的状态。(Instead, it flags the record as deleted and the space it used becomes reclaimable.) 当执行了足够多的删除操作，达到MERGE_THRESHOLD（默认是页大小的50%）时，Innodb开始检查附近的页（前一页和后一页），看看是否能通过合并两页来优化空间。 本例中，页#6使用的空间不足其50%，页#5因为多次删除操作也有&lt;50%的使用率，从Innodb的角度看，他们是可以合并的。 合并之后，页#5包含自身旧数据和页#6的数据，而页#6则为空留待使用。 Page Splits上面提到过，但页使用满了，下一行数据会被写入下一页中。但若我们遇到如下情景呢？ 页#10剩余空间不足以存放新的数据行(或更新操作)，按照上面的下一页逻辑，该行数据应插入到下一页，但实际上： 页#11也满了，此时数据无法按既定的书序插入，该怎么办？还记得上面提到的链表结构吗？此时，页#10含有前一页#9和后一页#11的指针。Innodb只需要简单的如下操作： 创建一个新的页 找到数据源页（页#10）中应该从哪开始分裂(按数据行级别)。 移动数据。 重新定义页之间的指向。 创建一个新页#12。 页#11保持不变，改变的是页之间的指向关系： 页#10将指向前一页#9和后一页#12。 页#12将指向前一页#10和后一页#11。 页#11将指向前一页#12和后一页#13。 此时，B+树的路径在逻辑上仍然是连续的，但物理上，页之间是无序的，通常都在不同的extent中。 Innodb使用INFORMATION_SCHEMA.INNODB_METRICS跟踪页分裂的次数。 一旦页分裂发生，只能通过删除新页数据达到&lt;50%使用率来触发也合并来恢复。 或者通过OPTIMIZE优化表结构，这通常很消耗资源且需要很长时间。但有时也是唯一的办法（比如使用了uuid做主键）。 另外需要记住一点，当发生页分类或也合并时，Innodb会在索引上获取x-latch，在一个繁忙的系统中，这会是一个性能障碍。在Innodb中称为”悲观”更新，此时加的是悲观锁。这会造成索引上的锁竞争。若合并或分裂只操作了一页，被称为”乐观”更新，锁是乐观锁。 不同主键的比较良好的主键不但对读数据重要，也能在写数据时正确的分布数据。 一例中我们使用简单的自增ID做主键。二例中主键使用了ID（取值范围1-200）且自增。三例中我们使用了同样的ID做主键，但该主键关联到UUID上。 插入数据时，页分裂的情况如下： 前两例的数据分布更紧凑。也意味着它们有更好的空间利用率。例三有着大量空闲的页，意味着有大量的页分裂操作。 而在也合并的情况： 在插入-更新-删除操作下，自增ID的例子中合并操作更少，更低的合并成功率9.45%。而使用UUID为主键的例子中，合并操作更多且合并成功率更高22.34%","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"},{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"}]},{"title":"MySQL_B+Tree索引","slug":"MySQL-B-Tree索引","date":"2020-02-27T12:50:08.000Z","updated":"2020-02-28T11:13:35.198Z","comments":true,"path":"2020/02/27/MySQL-B-Tree索引/","link":"","permalink":"https://straysh.github.life/2020/02/27/MySQL-B-Tree%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引是提高查询效率的，在MySQL中以B+树索引为主（绝大部分MySQL数据库引擎都是用的Innodb，而Innodb默认使用B+树）。B+树索引又分为聚簇索引和非聚簇索引，本文着重介绍聚簇索引。 聚簇索引聚簇索引并不是单独的索引类型（索引类型包含：主键索引、唯一索引、单列索引、聚合索引等），而是一种数据存储方式。其非叶子节点仅包含索引列，数据行存储在叶子节点上。Innodb只保证相同页上的数据是连续的，但不同的页可能相距甚远。 结构 类似平衡二叉查找树：节点有序(左节点&lt;根节点&lt;右节点)，树是平衡的不会单边增长。 节点上存有多个索引值，控制树的深度，稳定查询效率。 节点大小是一个页4KB。 B-Tree和B+Tree的对比B-Tree B+Tree 聚簇索引的优点 叶子节点包含数据行，因此聚簇索引比非聚簇索引更快。 叶子节点中的数据是连续的，实现范围查找效率更高（叶子节点之间有顺序指针）。 聚簇索引的缺点 更新聚簇索引代价很高，以为需要移动数据行到新的位置。 可能导致页分裂，从而某时刻导致大量的I/O操作。 参考资料： 8.3.1 How MySQL Uses Indexes 8.3.9 Comparison of B-Tree and Hash Indexes 8.3.6 Multiple-Column Indexes covering index Mysql索引简明教程 聚簇索引及 InnoDB 与 MyISAM 数据分布对比 不准犹豫！再有人问你为什么MySQL用B+树做索引，就把这篇文章发给她","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://straysh.github.life/tags/Mysql/"}]},{"title":"杂记_风物长宜放眼量，人间正道是沧桑 - 一位北美留学生的内心独唱","slug":"杂记-风物长宜放眼量，人间正道是沧桑-一位北美留学生的内心独唱","date":"2020-02-27T12:11:09.000Z","updated":"2020-03-23T15:35:04.383Z","comments":true,"path":"2020/02/27/杂记-风物长宜放眼量，人间正道是沧桑-一位北美留学生的内心独唱/","link":"","permalink":"https://straysh.github.life/2020/02/27/%E6%9D%82%E8%AE%B0-%E9%A3%8E%E7%89%A9%E9%95%BF%E5%AE%9C%E6%94%BE%E7%9C%BC%E9%87%8F%EF%BC%8C%E4%BA%BA%E9%97%B4%E6%AD%A3%E9%81%93%E6%98%AF%E6%B2%A7%E6%A1%91-%E4%B8%80%E4%BD%8D%E5%8C%97%E7%BE%8E%E7%95%99%E5%AD%A6%E7%94%9F%E7%9A%84%E5%86%85%E5%BF%83%E7%8B%AC%E5%94%B1/","excerpt":"","text":"文章来源：微信文章摘录部分，点击上方文章来源浏览原文。 引言我对于本科时光的印象，还停留在那所普通大学的建筑物之间，我坐在大学的时光长廊里，满眼望去，都是经历的过的故事。可毕业后回首，却很少有人能说，自己从来没有迷茫过。迷茫，仿佛就是一团乌云，笼罩在每一个心中怀有抱负的人的头上。每当夜深人静，思绪归于对自己人生未来的严肃思考，不知去往何处的苦闷，再加之不断迫近的升学/就业选择的压力，一些看似周密的计划，由于想把每一环都做的尽善尽美，往往不仅减少了反馈（一切的目标、报偿都在最后）、还因为人生的不确定性而失败：以保研为例，我常常见到一些平常学习认真、热心参加学术活动的人，但是由于不关心所在院系制度，到保研时因为一些硬性规定（如必须完成某些并不喜欢的所谓保研必修课）、或者是不公平的排名标准（比如说活动成绩、竞赛成绩计算很高，导致成绩上产生的差距在计算时权重过低）导致与原本准备了很长时间的保研机会失之交臂，而此时离考研也已不远，本来就没有提前准备，也很容易落榜。在这样的过程中，亲历者绝望、愤懑甚至因而导致抑郁的，我都见过，以至于都有些麻木了。太阳照常升起，一个人的悲哀苦痛，回到这个宏大的时代与社会中，连一粒尘埃也算不上。而那些看多了这样的故事的人，也难免兔死狐悲，故而更难聚集其一股欲与天公试比高的拼搏的意志。2019年7月29日，《中国青年报》刊发《大学生抑郁症发病率逐年攀升 大一和大三高发》引发读者广泛关注。有31.2万网民参与了中国青年报微博发起的&nbsp;大学生抑郁症发病率逐年攀升，你觉得自己有抑郁倾向吗的网络投票，其中认为自己有抑郁倾向且情况很严重的达到了8.6万，占比27.6%，若是统计为有过低沉的倾向，则有约18万人，占比为60.8%。一些市面上的流行语录，似乎也从侧面说出了这种感觉：什么我所得到的不过是侥幸，纵然得到了一时的世俗意义上的成功，由于从来没有对自己的人生命运有一个通盘的规划与目标，即使通过搜集到的一些信息、经验走了一些捷径，但是短暂的兴奋后，却又会重新回到迷茫与空虚中来。我忍不住还是要发问：上了好学校、找了好工作、赚了不少钱，那然后呢？倘若有一步，没能像这样环环相扣地被达成，那又该怎么办呢？纯粹的人往往能在一个方向做出不俗的成绩，可过于纯粹，就难以承受突如其来的打击，难以接受自己的规划被命运玩弄，付之东流。回想我的大学生活，也的确是这样：一起朝夕相处的同学朋友们，虽然可以一起感慨未来的未知与自身的无力，可自己的命运最终还是只能自己把握，总要面对那条只能一个人走完的路。而在我在大学期间认识的大约几百人里，真正能有坚定的三观信仰，又努力去做的（即所谓知行合一的），实在是凤毛麟角，而且这往往出自他们不断地痛苦地思考与试错，有时甚至还需要一点运气。人本应该是越长大越坚强越成熟的，可在大学期间因迷茫和各种诱惑堕落的大多数人，其心志能力，往往连高中时代都不如，既失去了当初的纯粹与坚定，又没有真正获得一些面对问题、解决问题的勇气与能力。这种普遍的迷茫，不只是存在于那些混吃等死的人中，我认识的无数的名列前茅、努力拼搏的同学，也还是深受其苦。笔者也是前两年，才逐渐开始想明白。像这样大环境的精神空虚与迷茫，究竟是谁的责任呢？毕业晚会时，一曲谁的青春不迷茫唱出了我的心声，歌词非常写实，也写出了很多人的青春，可是让我有些近乎条件反射般的讨厌：谁的青春又想迷茫呢?一个人、一小群人的迷茫，尚且可以认为是个人心理问题，抑或是环境，甚至是遗传；可成批的学生都怀疑人生、看不到未来的出路、不知道自己要干什么，这究竟是谁的责任呢？辗转反侧后我觉得需要用文字来阐述一下我的观点。那么究竟为什么会变成这样呢？就有没有什么合理的办法、科学的想法可以借鉴呢？作为个人，我们是不是也应该参考古今中外的真正的大人物、向那些慧人借鉴呢？我的青春并不想迷茫，我相信大家的青春也都不想迷茫。虽然因为运气我有了一点成绩，但是我觉得这不少都得归于时运的赐予，不把这样的经验分享给更多还在痛苦思考中继续前行的人，我无法获得良心的安宁。本文将从成因开始着手分析迷茫这个问题，从问题的产生到表现、再到教育制度、人性的缺点、再到我们可以锻炼的能力以及可以采取的想法（由于笔者也算是半个做技术的，相关的能力将主要以技术为基础）。 摘录部分，点击上方文章来源浏览原文。","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"},{"name":"Banbrick","slug":"Banbrick","permalink":"https://straysh.github.life/tags/Banbrick/"}]},{"title":"网络_tcp_udp","slug":"网络-tcp-udp","date":"2020-02-27T08:55:50.000Z","updated":"2020-03-10T11:17:41.487Z","comments":true,"path":"2020/02/27/网络-tcp-udp/","link":"","permalink":"https://straysh.github.life/2020/02/27/%E7%BD%91%E7%BB%9C-tcp-udp/","excerpt":"","text":"先复习下OSI七层模型 TCP/UDP工作在传输层 TPC/IP协议族 ARP: 是正向解析地址协议(Address Resolution Protocol),通过已知的IP,寻找对应主机的MAC地址. RARP: 是反向地址转换协议,通过MAC地址确定IP地址. IP: 是因特网互联协议Internet Protocol ICMP: 是Internet控制报文协议Internet Control Message Protocol,用于在IP主机、路由器之间传递控制消息. IGMP: 是Internet组管理协议Internet Group Management Protocol,是协议族中的组播协议,运行在主机和组播路由器之间. TCP: 传输控制协议Transmission Control Protocol,一种面向连接的、可靠的、基于字节流的传输层协议. UDP: 用户数据报协议User Datagram Protocol，是无连接的、不可靠的、基于报文的传输层协议. HTTP: 超文本传输协议Hyper Text Transfer Protocol,互联中应用最广泛的一种协议. FTP: 文件传输协议File Transfer Protocol 用户数据报协议 UDP（User Datagram Protocol）UDP面向数据报，无连接，不可靠，可以一对一，一对多，多对一，多对多互相通信（组播）。 即无需确认双方状态，数据准备完毕即刻发送，也不需确认对方是否接收成功。 传输控制协议 TCP（Transmission Control Protocol）面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信。每一个TCP连接只能是点对点（一对一）的。 三次握手 服务器等待连接中… SYN=1,Seq=x - 客户端已准备就绪，询问服务器是否就绪。 SYN=1,Seq=y,ACK=1,ACKnum=x+1 - 服务端收到询问并已准备就绪，询问客户端是否就绪。 ACK=1,ACKnum=y+1 - 服务器收到确认，双方确认完成，连接建立。 开始发送数据 其中，步骤1-2是第一次握手，表明客户端发送正常且服务端接收正常；2-3是第二次握手，表明服务端发送正常，客户端接收正常。此时，客户端能确认自己和服务端都能正常发送、接收，但服务端还不知道客户端的接收能力。所以有了3-4的第三次握手，服务器确定了客户端接收和发送正常。此时连接建立，后续开始发送数据。 四次挥手 FIN=1,seq=x - 客户端通知服务端我要断开了。 ACK=1,ACKnum=x+1 - 服务端告诉客户端，我收到了断开请求。 此时服务端不能立刻答复客户端关闭连接，因为可能还有数据在准备中，当这些数据发送完成后。 FIN=1,seq=y - 服务端告诉客户端，你可以断开了。 ACK=1,ACKnum=y+1 - 服务端收到客户端已经断开连接。 服务端连接断开。 其中，第二次握手之后，并不是立刻断开连接，需要等待服务端将缓冲区的数据发送完毕。而第四次握手之后，服务端关闭了，但客户端需要等待两个握手时间再关闭：因为若服务器未收到最后的ACK，会重新发起第三次握手请求，这个2次握手时间就是在等待这种可能的情况。若未收到第三次握手请求，客户端就可以正常断开了。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"Api开发_认证_授权","slug":"Api开发-认证-授权","date":"2020-02-27T07:21:53.000Z","updated":"2020-02-27T08:56:31.461Z","comments":true,"path":"2020/02/27/Api开发-认证-授权/","link":"","permalink":"https://straysh.github.life/2020/02/27/Api%E5%BC%80%E5%8F%91-%E8%AE%A4%E8%AF%81-%E6%8E%88%E6%9D%83/","excerpt":"","text":"在接口开发中，第一步要解决的问题就是认证和授权。 Authentication - 认证即使用某种凭证来证明身份的过程。如用户使用账号+密码，使用手机号+短信码，使用指纹识别等。 Authorization - 授权将某种凭证(一般非原始认证信息，且有过期时间、使用次数、使用频率等限制)给予第三方使用的过程。如允许微信使用相册的能力，允许小程序读取微信基本信息的能力 Credential - 凭证用于认证或授权的信息。如账号、密码、短信码、cookie、session、token等。 Cookie and Session本质上，cookie存储在客户端（一般是浏览器），session存储在服务端。但session同时依赖于session-id（存储在客户端）来和客户端关联（一个请求到达服务端，依赖session-id来定位到具体的session）。因此Cookie和Session Token用户认证成功后，由服务器签发的凭证（一般有过期时间、使用频率等限制），在今后调用api时使用。 JWT - Token的一种实现方式JWT规范Auth0社区资料 Oauth2.0 - Token实现的一种方式现流行的实现开放API的授权方案。Oauth2.0社区技术资料","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"MySQL_事务","slug":"MySQL-事务","date":"2020-02-26T12:38:48.000Z","updated":"2020-03-01T06:06:13.680Z","comments":true,"path":"2020/02/26/MySQL-事务/","link":"","permalink":"https://straysh.github.life/2020/02/26/MySQL-%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"还未填坑……","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"MySQL_分库分表","slug":"MySQL-分库分表","date":"2020-02-26T12:30:45.000Z","updated":"2020-03-01T06:06:13.688Z","comments":true,"path":"2020/02/26/MySQL-分库分表/","link":"","permalink":"https://straysh.github.life/2020/02/26/MySQL-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/","excerpt":"","text":"还未填坑…… MySQL 分库分表方案，总结的非常好！","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"杂记_求职_程序员现在跳槽容易么","slug":"杂记-求职-程序员现在跳槽容易么","date":"2020-02-26T12:20:26.000Z","updated":"2020-02-28T12:28:13.912Z","comments":true,"path":"2020/02/26/杂记-求职-程序员现在跳槽容易么/","link":"","permalink":"https://straysh.github.life/2020/02/26/%E6%9D%82%E8%AE%B0-%E6%B1%82%E8%81%8C-%E7%A8%8B%E5%BA%8F%E5%91%98%E7%8E%B0%E5%9C%A8%E8%B7%B3%E6%A7%BD%E5%AE%B9%E6%98%93%E4%B9%88/","excerpt":"","text":"文章来源：猎聘App 工作7年+的程序员，从大厂跳槽竟然没人要至今工作也8年了，回想起来，一时间难免感慨，时间真的过的太快了。 当初在北京的4年多，是我工作中最精彩的一段经历，这也是为何我的小说以我在北京打拼时的真实经历为背景，因为那是一段难忘而又精彩的时光。 16年偶得一个大厂的offer，因此我就毅然决然的来到了杭州，来到杭州以后，我的工作平淡了许多，或许和年龄有一定关系，也或许和杭州这个宜居的城市氛围有关。 大厂有各种小公司没有的福利和待遇优势，因此在这家公司的3年，我住进了自己的房子，也生了个可爱的儿子，也有了自己的豪车，总的来说，在老东家的这3年里，我收获了很多，不仅仅是工作上的，更是生活上的。 但是，天下没有不散的宴席，3年了，我也知道，不能再继续沉沦下去，因此我在19年，果断开启了人生第4次跳槽之路。 此时我已经过了而立之年，和当初在北京跳槽时，我明显感觉出了很大的不同。 接下来，我就和大家简单聊聊，我这次跳槽的感受，相信对于很多同学应该有些帮助和参考，特别是在杭州的同学。 1 整体感观 先谈一谈这次跳槽经历的整体感观，用一个字总结就是——难！ 实话说，这在我跳槽之前是完全想象不到的，在我想来，以前在北京的时候，我啥大厂背景也没有，学校也不咋地，都能每次轻松跳槽，拿到一个理想的offer。 如今我技术肯定比以前更强了，而且还有大厂的经历，跳槽应该是手到擒来。 谁曾想，事与愿违，事实是，这一次跳槽我碰了很多壁。 有某公司谈好的最后突然变卦了，说因为我学历不够，事实上应该是HC突然被锁死了。也有和某公司HR畅聊到深夜，就为了那几千块钱工资，最后还是没谈拢。 也有某公司面试，我驱车40+公里前往，足足在门口等了2个小时，才看见面试官晃晃悠悠的来到。更有某公司HR在公开场合恶意诋毁我，最终差点影响我离职。 这些具体是什么公司，这里就不说了，总之都是一些让人十分头疼的经历。 当然了，也有一些十分真诚的公司，比如某车、某虹、某喽，虽然最终没有谈拢，但也算是为这冰冷的市场，添加了一份温暖。 总的来说，我这次跳槽一共面试了将近20家公司，其中涵盖了杭州所有大公司，以及一些中小型公司，但真正拿到offer的，只有4家，而这4家中真正有竞争力的offer，只有2家！ 是的，你没有看错，只有2家！ 当然了，找工作就和找老婆一样，不管多少，最终也只能选择一个，因此一个也足以了。 但这结果却和我预想的有很大差别，本来我以为从大厂出来，拿offer应该是拿到手软，然后被众多HR围着大喊“来我们这里吧”，最后我勉为其难的选择一家公司，然后对其它公司纷纷致以诚挚的歉意，充分体现我的逼格和地位。 谁曾想，最后我只能在可怜的2个offer之间稍微徘徊一下，就可以选择了，因为没有别的选择给你了。 事后，我仔细回想了下，为何这次面试会出现这样的结果，是因为自己的技术不到家吗？ 我觉得应该不是，虽然我不敢说技术多强，但应该也不会太差，所以这肯定不是原因，至少不是主要的原因，因为很多公司最终没谈拢，并不是我技术不够的原因。 那到底是因为什么呢？接下来我和大家一一探讨下，如果有碰到同样问题的同学，也可以参考下。 2 市场 没错，我觉得，第一个原因就是——市场。 我在北京的那几年，刚好赶上一波P2P的浪潮，当时只要是做P2P的，工资都给的贼高，而且对程序员的需求简直是供不应求。 但是随着最近P2P被整治，一大批公司倒闭，再加上互联网的发展到了一定瓶颈期，可创业的项目越来越少，因此，投资人在互联网这块都变的冷静了，资金开始大量流入到其它行业。 不仅如此，一些原来的大厂，存活起来也越来越艰难，因为很多大厂存活都是靠的一条主业务，当这个主业务不行的时候，如果没有开辟出新的战场，那就只能饮鸠止渴了。 也因此，前段时间，很多公司爆出裁员的消息，也被证实有很多是真实的。 这样一来，大量大厂的程序员涌入市场，再次加剧这种情况。 当然，市场冷却这背后还有一个原因，那就是做程序员的越来越多了，新人也越来越强了，985研究生可以说到处都是，而培训机构大批量的量产程序员，更是加剧了这种情况。 这或许也是因为，程序员这个职业经过一段时间的沉淀，逐渐打出了“入行快、工资高”的标签，因此吸引了很多年轻人入行。 你想啊，有几个职业，可以培训个一年半载，未来就可以月薪轻松上万，甚至2、3万的。 不过这也正常，中国或者说市场一直都是这样，就拿开网吧来说，第一批开网吧的，都是挣了不少钱的。 但随着越来越多的人意识到，开网吧原来这么挣钱，然后都开始开网吧的时候，网吧的生意也就越来越难做了，再加上现在大部分家庭都有电脑，让网吧的生意更是雪上加霜。 这就是聚集效应了，就像我县城里的KTV一样，如果你发现某家KTV特别火，那么不用多久，就会有N家KTV开业，迫使原来独揽风骚的你必须进步和改进，而最终活下来的，往往只有那么1、2家。 总之一句话，只有自己不断的跟进时代，才能不被丢弃，市场规律我们是无法抗衡的，学会SSH，走遍天下都不怕的时代，已经一去不复返了。 同时，如今是跳槽黄金期，我也想奉劝下各位同学，要量力而行，别出来后才发现，原来你并不是那么吃香。 当然，跳槽的脚步也不要因为畏惧就退缩，我也只是让大家跳之前，要认真了解下自己的水平和市场，而且我始终认为，时刻想着跳槽是工作当中主要的学习和进步动力。 当然了，想着跳槽不是必须要跳槽，而是说你要为跳槽时刻准备着，这样你才有动力去学习。 想一想某务员，为啥这些人被贴上了不思进取，坐吃山空的标签，虽然这有些武断了，但也确实无风不起浪。而我并不认为这些某务员都是没有上进心的，而是一种思想左右了他，那就是——这个工作我要干一辈子，我不需要跳槽。 既然不需要跳槽，我还学习干嘛？这样人就很容易懒惰，这就是人的本性，这种人人都有的惰性，大部分人都不能幸免。 如果你幸免了，也就是没想着跳槽，但依然学习动力十足，我只能说：“你当前所在的公司绝对还不超过2年！你的热乎劲还没过，待你长发及腰，我们再来看。” 3 天花板 当初在北京跳槽的时候，我的工资还比较低，那个时候随便跳槽涨一涨，就是50%甚至更高的比例，哪怕在同一家公司不跳槽，有的时候涨薪都是20%甚至30%的在涨。 但是现在，我已经过了而立之年，工资和待遇上，已经达到了一个瓶颈，想要再保持这样的涨薪幅度，自然会难上加难。 这就像你5000涨到10000，可能很容易就翻倍了，但你想2万涨到4万翻个倍，那难度就不可同日而语了。 更何况，我现在在杭州，和北京相比，杭州的IT压根和北京就不是一个档次的，所以就更是艰难了，毕竟公司太少，大公司更少。 其实这种困境，也就是很多人常说的“中年危机”了，以前我不理解，现在多少也有点理解了这个词背后的含义。 其实就是，你的能力和待遇都到了瓶颈，当你想跳槽时，你会发现，你特么只能原地踏步，甚至倒退。更甚者，哪怕你不跳槽，也会一不小心被开，这就是“中年危机”的真实写照了。 但其实“中年危机”更可怕的，不是你找不到更好的工作，或者是随时会被开，而是心理上的那种压力，哪怕你一切都好好的，其实你也无形中承受了这种压力。 除非你可以爬过面前的这座山，看到山后面更好的风景，这种压力才会小很多。 说的通俗点，就是在你“中年危机”来临之际，甚至来临之前，你就已经爬到了更高的位置。 但毕竟，这是一小部分人，就像某大厂的高P，某大厂的高T等等，但事实上，我相信，哪怕你爬到了更高的位置，也有相同的压力等着你，只不过相对会小一点而已，因为你依旧会面临着被开和出去后找不到更好的工作的风险。 所以，很早之前，我就说过，应对“中年危机”最好的办法，就是尽快实现财务自由，这样你自然压力就小很多了。 当然了，所谓的财务自由，不是说让你挣个大几千万甚至上亿，一辈子都花不完那种才叫财务自由。 我所理解的财务自由，就是你基本上没什么债务，有固定的住所，简单的说就是你随便干点啥也能维持基本的生活，这就叫财务自由了。 至少你不会担心哪一天你拿不到高薪了，连饭都吃不起，最多是生活上节省一点而已。 聊了聊“中年危机”，我们再回到刚才的话题，当遇到天花板的时候，面试就会像我这次一样，很多公司并不是不要你，而是给不了你满意的薪水。 给的低了，你不满意，毕竟跳一次槽不容易，不涨点工资谁都会觉得不划算，而给的高了，人家又给不起，或者是不想给，最后只能是一拍两散。 这也是我这次面试这么不顺利的原因之一，有大约一半以上的公司都是这样的情况。 当然了，这或许也是大厂同学跳槽的劣势，因为大厂的工资（特别是总包算下来）高一些，所以如果不去同等级别的公司，想要保持当前的薪水甚至更进一步，难度就会大很多。 这也是为啥大厂同学跳槽相对会少一些的原因之一，不仅仅是因为公司本身不错，值得长期待下去，也有一个很重要的原因，是很多人根本就跳不出去。 因为比较高的待遇或者是股票期权，导致很多人被限制在了公司，除非你愿意舍弃一部分利益，否则就很难出去了，尤其是杭州这样的二线城市或者说准一线城市。 如果是北京的话，可能会相对好一些，毕竟北京的大厂多，能给得起工资的公司也多，所以出来以后找工作难度会低一些。 所以这再次提醒了我们，如果你觉得你目前的薪资和待遇已经到了天花板，或者接近天花板，那跳槽的时候，一定要放平心态，适当降低心里预期，否则你可能会十分的不适应这种变化。 在这里，给大家一个工资区间参考值，大家可以看下你是否处于这个区间当中，如果高过这个区间，说明你算是比较优秀的，如果低于这个区间，那说明你还仍需努力。 不过要注意，以下内容仅适用于杭州（如果是北上深的话，请在杭州的基础上加5000，如果是广州则和杭州一样），而且仅供参考，如有不符，概不负责，成年人要有自己的判断力。 在杭州，1年或以下的同学，大部分都是5K-1W的月薪，3年左右，一般都会提升到1.5W-2W左右，5年左右，一般会提升到2.5W-3W左右。 至于5年再往上，区别就比较大了，比如有的人升的快，可能7、8年的时候就年薪百万甚至更高了，但有的还停留在年薪30-40W之间，也就是和月薪3W时没啥大的变化。 说完区间以后，我们再来说说杭州的两个常见的天花板。 其实在杭州这个地方，3W（或者说年薪35W-50W）就是第一个天花板了，再提升一个档次，就是年薪大约50W-70W的这批人，这也是第二个天花板。 再提升一个档次，就是年薪大约70W以上甚至超过百万的人了，这些人就不讨论了，一般这种人跳槽都不太会考虑工资了。 因为如果考虑工资，就这些人目前的收入，往往是不需要跳槽的，或者说是不能跳槽的，因为他的收入可能有一大部分来自于股票期权，跳槽了就没了。 这两个常见的天花板是什么意思，其实就是当你年薪处于30多万，或者50多万的时候，你跳槽的时候就要格外的注意，因为这个时候你想更进一步，往往需要打破一定的天花板，也就是你要有质的提升，否则的话，你可能很难跳入下一个档次中去。 换句话说，你的工资从30W提升到50W（才60%），比你从10W提升到30W（涨了2倍）要难的多，而且甚至可能一直升不上去，这就是所谓的天花板了，它不比你从10W提升到30W，最多是快慢的问题。 从30W到50W，或者从50W到70W，可能你很长一段时间也上不去，甚至到你换行退役的时候还没达到都有可能。 所以一旦达到这个天花板的时候，跳槽时就要格外的谨慎了。 当然了，还是那句话，成年人要有自己的判断，切勿人云亦云，不要太纠结于我提到的数字，自己有没有到天花板，自己心里是要有点X数的，哪怕你当前月薪还不过万，也有可能这就是你的天花板，-_-。 4 行业限制 因为行业上的限制，也是导致我这次跳槽颇为不顺利的原因之一。我之前的行业，准确的来说是云计算PaaS领域。 我刚开始跳槽的时候想的很简单，想去一个小点的公司当个基层或者中层领导，拿个高薪，舒舒服服的开始下半辈子的奋斗。 现在来看，我这个想法在当时其实有点想当然了。 为啥？ 很简单，因为跨业务或者说跨行业的话，你对业务不够了解，那你就失去了一个很大的优势，尤其是对于从大厂出来的人来说，就更是如此，毕竟如果你是个小厂，你的那点业务经验人家一般不会看在眼里。 举个例子，某多招人的时候，最喜欢哪里的人？一般是某宝或者某猫的人。 因为业务类似，而你又有中国这个行业顶尖公司的经验，那么不管你的技术叼不叼，就你的业务经验，就最起码值年薪2、3十万，因为相同业务场景下所遇到的技术难点，你在老公司一般都会遇到过，或者有一些了解，这就是经验的价值所在了。 技术做的越久你就越会发现，任何一项技术，在没有经历过足够的验证之前，都是一大堆坑在等着你趟。而这些趟坑的过程，对于公司来说其实是一种损耗，如果你有过往的经验，就可以在这方面大大的减少弯路。 所以假设你进一家公司，单靠技术或其它能力本来可以拿30W年薪的，而如果刚好你有相同的行业经验，同时你之前的公司又是这个行业比较顶尖的，那或许最后你的年薪会溢出30%-50%，也就是达到40W-45W。 这点相信其实也不难理解，但我比较悲催的是，云计算这个领域的公司实在是太特么少了，特别是做PaaS的，又是在杭州的，那就更是少之又少，一个手的手指头都特么用不完，还特么能剩四个，也就是除了我的老东家以外，基本上只有一家。 这也就意味着，我跳槽除了这一家公司以外，基本上靠的就全是技术和其它综合能力，那点之前的业务经验或者行业背景，基本上没有用到，最多只能算是聊胜于无。 这也是为啥我连个小公司的领导都拿不下来，人家总不能招一个对业务啥也不知道的人来带团队吧。 什么？ 你说你可以学习？ 不好意思，人家招leader是来带领和引导团队的，不是让你来学习的。 或许在极度缺人的情况下，有的公司会退而求其次，但当前的市场氛围，显然很少有公司有这个闲心，而且我的云计算背景毕竟是极少数。 所以这就尴尬了，让你做开发，你不愿意，工资也提不上去，谁让你原来太高了呢，让你做领导，你对业务又不了解，没有相应的行业技术积累，哪怕你技术再厉害。 但作为一个leader，技术能力只是一方面，丰富的业务经验和该业务场景下的技术沉淀也很重要，所以这就难受了。 这也让我深刻的意识到，行业在职业生涯中后期对于程序员的重要性。事实上在很多程序员眼里，对于业务是没有什么概念的，总觉得做P2P也是做，做云计算也是做，做电商也是做，感觉都一样的。 不得不承认的是，在初期确实是无所谓的，但越到后期，就越重要。 所以我在此友情提示下，跳槽的时候，如果可能的话，还是尽可能保持自己的行业优势吧，同时记得选一个比较有前景，对口公司也比较多的行业，否则的话，要吗等你跳槽时这个行业已经垮了，要吗就像我现在这样，根本就找不到对口行业的职位。 啥行业有前景？ 我只能告诉你：“这东西谁能说的好，就跟房价是涨还是跌一样。” 5 生活限制 最后一个原因，这个相对占比低一些，但也确实是原因之一，就是生活上的限制。 比如最典型的距离，这也是买房为数不多的坏处之一了吧，因为我的房子比较偏，所以在工作的选择上，就比较受限制。 就拿刚才说的，那唯一一家和我对口行业的公司来说，特么离我家里有40多公里，来回一天将近90公里的距离，这特么怎么玩。 如果是租房就不存在这个问题了，就像以前在北京租房时换工作，基本上不太会考虑距离的问题，大不了就换个房子租。 除了距离以外，还有一个限制，就是工作时间，目前我也算是上有老下有下，不太可能像年轻时那样，经常加班到晚上11、12点，甚至凌晨1、2点，因此找工作就不能找太累的。 比如杭州某公司、杭州某公司还有上海某公司这种，哈哈，自己猜吧，这些都是相对比较辛苦一些的公司，当然了，报酬也是大大滴，就看你愿不愿意拿命换钱了。 其实就算不考虑家庭，就我自己的体力来说，也不允许再这么搞了，以前还好，现在熬夜加班的话，我已经远远不如以前那么能熬了。 所以，随着生活上逐渐稳定，你的负担会越来越重，对你的限制也会越来越多，难免会对你的职业生涯产生一定的影响，但相应的，这些负担往往也是幸福的源泉。 到底孰优孰劣，个中滋味还是大家自己去体会吧。 6 跳槽后的生活 虽然跳槽时各种纠结，也各种头大，但不得不说，跳槽后的生活还是十分Happy的。 薪资有了涨幅，工作有了变化，认识了一大堆新同事，学习了一大堆新知识，整个人都会处于一种积极亢奋的状态。 事实上在跳槽之后，很多人听到我跳槽后第一反应都是这样的，“啥？你跳槽了？卧槽！你为啥要跳槽？XXX那么好的公司，你走了岂不可惜？” 面对这种排比疑问句，我都不知道该咋回答，难道好公司一旦进去了就得从一而终吗？那每年从那些大厂出去的人，都是脑子抽了吗？ 非也，也可能是混不下去了。 开个玩笑，其实我早就说过，公司和老婆一样，不是要找最好的，而是要找最适合的。 我在老东家呆的时间也不短了，3年说长不长，说短不短，虽然收获了很多，无论是技术上还是经济上。 但不适合就是不适合，不能单纯只看钱多少和公司的大小，就决定这份工作的好坏，更何况这次薪资还涨了，而且新东家也是一家规模颇大的公司。 其实回想起来，当初我来杭州是十分仓促和盲目的，当时我就冲着两点就来了，一个是杭州，因为房价低好落户，一个是大厂。 至于公司团队文化是否匹配，工作内容是否喜欢，行业是否有前景，以及未来行业对自己职业的影响，等等吧，我压根就没考虑过。 所以，从生活上来说，我这是一次十分成功的跳槽和换城市，但是仅仅从工作上来说，其实16年那次从北京到杭州的跳槽并不是那么的理智。 当然了，大厂所带来的光环和优势，是很多公司无法相比的，这在程序员的圈子里，也算是进北大清华进修过了。最起码吃过猪肉，也看过猪跑了，可以说程序员的人生已无憾事。 在此要特别感谢一下当初招我进来的老板，还有我的老东家，可以说，当初跳槽到杭州，以及之后在老东家的这3年，改变了我的下半生。 来杭州以后，每次我和媳妇回忆起当初在北京，挤在一个10平米的小次卧里，七八个人一起抢厕所的日子，都觉得人生真的是有趣。 谁能想到当初月薪6000都兴奋的睡不着觉的我，现在能拿着年薪XX的收入，谁又能想到当初无依无靠在北京漂着的我，如今能住上自己的房子老婆孩子热炕头，谁又能想到当初每天人挤人的站在公交车里上班的我，如今能开着自己的豪车上班。 谁又能想到，当初意气风发的我，如今已经快秃了头。 回忆总是感慨，但终归是过去了，目前我在新东家干的风生水起，未来的日子也还要不断前行。 在此，我衷心的希望所有的程序员们，能够依靠自己的努力，攀上你以前想象不到的高峰。 虽然这个高峰，可能还在某些大佬的山脚下。","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"},{"name":"Banbrick","slug":"Banbrick","permalink":"https://straysh.github.life/tags/Banbrick/"}]},{"title":"杂记_杂","slug":"杂记-杂","date":"2020-02-26T04:24:05.000Z","updated":"2020-02-28T10:55:41.515Z","comments":true,"path":"2020/02/26/杂记-杂/","link":"","permalink":"https://straysh.github.life/2020/02/26/%E6%9D%82%E8%AE%B0-%E6%9D%82/","excerpt":"","text":"为人性僻耽佳句，语不惊人死不休 为人性僻耽佳句，语不惊人死不休！ 老去诗篇浑漫兴，春来花鸟莫生愁。 新添水槛供垂钓，故着浮槎替入舟。 焉得思如陶谢手，令渠述作与同游。 ①“值”，正逢。“水如海势”，江水如同海水的气势。“聊”，姑且。②“为人”，这里是平生的意思。“性僻”，性情乖僻，古怪。这里实指诗人的一种举趣、追求。“耽”，爱好，沉溺于。③“老去”，即年老了。“浑”，完全，简直。“漫”，随意。④“莫”，没有。⑤ “槛”，栏杆。⑥“故着”，又设置了,“着”，读zhuó，设置。“槎”，音chá，木筏。“替”，代替。⑦“焉得”，怎么找到。“思”，才思，诗才。“陶谢”，指陶渊明、谢灵运。⑧“令渠”，让他们，“渠”，代词。“述作”，作诗述怀。原诗中的“为人性僻”：杜甫自我解剖之词。漫与：相当于“随便对付”的意思，自谦之词。浮槎：木筏。陶谢：陶渊明、谢灵运。令渠：叫他们。此诗意思是：我为人情性孤僻，醉心于作诗，写出来的诗句一定要惊人，否则不肯罢休。到老来作诗还是很平庸，就不用再为春花秋鸟增添愁怀了。前不久门前修了个水槛，供凭栏垂钓之用，有时乘上木筏子也可以当做小船用。真希望能找到家陶潜和谢灵运这一类人做朋友，跟他们一起吟诗，同游山水才好呢！“语不惊人死不休”一语，后人引用为对自己写作的严格要求。 这首诗是诗人面对如大海汹涌的江水，抒发内心感受的叙怀之作。诗人站在江边，看到波涛滚滚的气势，引发了他无限的感慨。他审视了自己的创作：“为人性僻耽佳句，语不惊人死不休。”诗人自谓平生特别喜欢、刻意追求最能表情达意的诗句，然而这种追求，在别人看来简直是有些古怪，有些乖僻。但这确实就是我的态度，达不到语出惊人的地步，我是决不罢休的。这两句诗道出了杜甫诗作的特色，反映了他认真严谨的写作态度。 这两句似乎是诗人聊以自慰的，但此时此刻却像奔泻不已的江水，他又转想到随着岁月的消逝，自己也越来越老了，此时的心境，似乎失去了往日的激情，对着春天的花鸟，也没有了过去的苦恼与烦闷，所写的诗稿，不过是随随便便敷衍而成。 接下来两句，诗人的视线转向眼前景物：江边新装了一副木栏，可供我悠然垂钓，我又备了一只小木筏，可代替出入江河的小船了。这两句是对他老年心境的写照。其实杜甫是否真有如此而已的闲情雅致垂钓，并无可考，也许这只是诗人的一种无奈的自慰、自嘲。总之中间这两联，见出杜甫对自己年华老去的感慨，其中也暗含着对自己热情的减退的自责。 但杜甫毕竟是一位有时代感、有责任心的诗人。澎湃的江水似乎又激起了他高昂的创作欲望，他追思诗坛高手陶渊明、谢灵运，并想象与他们一起浮槎漫游。这既是江海游，也是诗海游，表明杜甫仍然壮心不已，追求不止，要继续写出惊人的诗作。 本诗作于上元二年（761）。杜甫时年五十岁，居于成都草堂。诗题中一个“如”字，突现了江水的海势 ，提高了江景的壮美层次，表现了江水的宽度、厚度和动态。江水如海势，已属奇观。然而诗题却偏偏曰 ：“聊短述 ”。诗题中就抑扬有致，这是诗人的一贯风格。 既然聊为短述，山语岂能平平？诗人自谓“为人性僻耽佳句，语不惊人死不休”，足见“聊短述”的良苦用心，炉火纯青的诗艺，严肃认真的写作态度和动人心弦的审美效果。 正由于杜甫艺术上的一丝不苟、勇于创新，因此老年臻于出神入化、妙手成春的极境。所谓“老去诗篇浑漫与，春来花鸟莫深愁”。仇兆鳌评杜甫“ 少年刻意求工，老则诗境渐熟，但随意付与，不须对花鸟而苦吟愁思矣 。”（《杜诗详注》卷之十）同时他还转引钱笺道：“春来花明鸟语，酌景成诗，莫须苦索，愁句不工也。若指花鸟莫须愁，岂知花鸟得佳咏，则光彩生色，正须深喜，何反深愁耶？”（《杜诗详注》卷之十）这里是说春光明媚，花香鸟语，快乐异常，因此不存在花鸟深愁的问题，“莫深愁”为杜甫自况。至于“ 浑漫与”中的“与”字，旧本曾作“兴”，清末郭曾忻解释说：“所谓漫兴，只是逐景随情，不更起炉作灶，正是真诗。”（《读杜札记》）此处强调任笔所之，自然而然。总之，首颔二联总体着眼，大处落墨，虽为短述，语实惊人，虽未直接描写江上海势，但胸中之海早已形成。它浑厚深涵，辽阔无垠，大气磅礴。心中之海，诗人采取了虚写的办法。正如金圣叹所说，此“不必于江上有涉，而实从江上悟出也。”（《杜诗解》卷二）所谓海势，其实是江，因此江上之景，亦应摄取，若完全避开江水，则海势亦无所依附，而不成其为江如海势。为此，诗人紧接首颔二联虚写海势以后，随即转入实写江水。故颈联道 ：“新添水槛供重钓，故著（着）浮槎替入舟。”此处虽写江水，但只是轻轻带过，如此触及江水、悟及海势的写法，令人玩味不尽。正如王嗣奭所说 ：“水势不易描写 ，故止咏水槛浮舟。此避实击虚之法 。”（《杜臆》卷之四）又如金圣叹所说 ：“不必于江上无涉，而实非着意江上也。”（《杜诗解》卷二）尾联诗人以一“焉”字，即巧作转折，融注新意。诗人之语，已经惊人 。若得陶渊明 、谢灵运那样的妙手，使其述作，并同游于江海之上，岂不快哉！尾联思路新奇，饶有兴味，且与首联相呼应，显示出诗人对艺术最高境界的执着追求 。“更为惊人之语也。”（《杜诗解》卷二）对诗与诗题之间的关系金圣叹先生写道：“每叹先生作诗，妙于制题。此题有此诗，则奇而尤奇者也。诗八句中，从不欲一字顾题，乃一口读去，若非此题必不能弁此诗者。题是‘江上值水如海势’七字而止，下又缀以‘聊短述’三字。读诗者，不看他所缀之三字，而谓全篇八句，乃是述江水也，值江水之势如海也。则八句现在曾有一字及江海乎？”（《杜诗解》卷二）从他评析中，可以得知：此诗诗题与诗中八句，构成了一个浑厚海涵、博大精深的整体。虽未写海，而如海势。此诗以虚带实，出奇制胜，意在言外，令人叹为观止。 注: 杜甫（712－770），字子美，自号少陵野老，汉族，祖籍襄州襄阳（今湖北襄阳），一般认为出生于巩县（今河南巩义）。盛唐时期伟大的现实主义诗人。代表作有“三吏”（《新安吏》、《石壕吏》、《潼关吏》）、“三别”（《新婚别》、《垂老别》、《无家别》）等。初唐诗人杜审言之孙。唐肃宗时，官左拾遗。后入蜀，友人严武推荐他做剑南节度府参谋，加检校工部员外郎。故后世又称他杜拾遗、杜工部。他忧国忧民，人格高尚，一生写诗1500多首，诗艺精湛，被后世尊称为“诗圣”。 智者乐其道，庸人乐其欲。以道制欲，则乐而不乱；以欲忘道，则惑而不乐。 《礼记·乐记》上说： 夫物之感人无穷，而人之好恶无节，则是物至而人化物也。人化物也者，灭天理而穷人欲者也。于是有悖逆诈伪之心，有淫泆作乱之事。 是故，强者胁弱，众者暴寡，知者诈愚，勇者苦怯，疾病不养，老幼孤独不得其所，此大乱之道也。 “未有知而不行者。知而不行，只是未知。有如知痛，必已自痛了，方知痛。知寒，必已自寒了。知饥，必已自饥了。知行如何分得开？” - 《传习录》 Novices need recipes Advanced beginners don’t want the big picture Competents can troubleshoot Proficient practitioners can self-correct Experts work from intuition 君子不器 君子矜而不争，群而不党。 RD 是研发（研究与开发） FE 前端研发 QA 是测试 UE 用户体验 OP 是运维。 UI 就是用户设计 DBA 数据库管理员 PM 产品经理 为什么整个互联网都缺前端工程师原文:http://www.tuicool.com/articles/qi2eMre原因一： 前端相对来说是一个新领域，当今的web需求要求更多的工程师供应现在还有一类人是独自处理一个Web应用程序的所有问题，即”全栈工程师”，但是优秀的全栈工程师同样如大熊猫一般数量稀少。 现在，一个创业公司想要成功，几乎要覆盖一个Web应用程序的诸多技术方面：前端、后台、DBA、运营等。虽然已经有一些服务可以让企业购买登陆页 面的前端模块和组件；但是如果你想要创造一些真正的Web应用程序，除了求助前端工程师，别无他法，就像你要创立一个品牌你也绕不开请一位设计师一样。 原因二： 对前端，普遍存在巨大的误解，其实前端一点也不简单 大多人都认为前端开发是一个“相对于其他模块来说更简单的领域”，在他们心中的前端工程师是这样工作的： 把Photoshop文件、图片或者线框放进一个网页； 偶尔设计Photoshop文件、图片或者线框； 用JS编程，为网页制作动画、过渡效果； 用HTML和CSS编程，确定网页的内容和形式。事实上，前端工程师在做的是： 在设计师和工程师之间创建可视化的语言； 用可视化的设计，定义一组代表内容、品牌和功能的组件； 为Web应用程序的公约、框架、需求、可视化的语言和规格设定底线； 定义Web应用程序的设备、浏览器、屏幕、动画的范围； 开发一个质量保证指南来确保品牌忠诚度、代码质量、产品标准； 为Web应用程序设定适当的行距、字体、标题、图标、余粮、填充等等； 为Web应用程序设定多种分辨率的图像，设备为主的实体模型，同时维护设计指南； 用account Semantic s、accessibility、SEO、schemas、microformats 标记Web应用程序； 用一种友好的，消耗小的，设备和客户端感知的方式连接API，获取内容； 开发客户端代码来显示流畅的动画、过渡、延迟加载、交互、应用工作流程，大多数时间用来考虑渐进增强和向后兼容的标准； 保证后台连接安全，采取跨地资源共享（CORS）的程序考虑，防止跨站点脚本（XSS）和跨站点请求伪造（CSRF ）； 最重要的是，尽管有严格的期限、利益相关者的要求，以及设备的限制，无论现在还是将来永远是“客户第一”。 为了实现上述目标，前端工程师采用了从可视化到编程的多种工具 ，甚至有时要照顾市场、 UX 到内容tweakes等等。 原因三： 大量糟糕前端工程师的存在，扰乱了市场 这或许是难以招到优秀前端工程师最明显的原因。由于前端工程师的入门门槛非常低，JS、CSS、HTML并不是很难入门掌握的语言，似乎只要花一点时间，谁都可以通过网上教程和书本学会它，前端工程师市场就是被这些浅尝辄止的家伙搞坏的。 糟糕的前端工程师是这样做事的： 滥用JS库，因为他们实际上并不了JS的内部（e.g. 一切都用jQuery）； 滥用JS插件，抄别人的代码哪怕自己根本读不懂（e.g.jQuery.doParallaxPls.js）； 给Web应用程序添加CSS框架，却只用到CSS/JS的5%，没有看到任何的需求、设计或者比较和评价； 认为只要添加了CSS框架，网站就可以“有求必应”； 一边在说着“响应式 Web设计 ”，却对服务器端技术一无所知； 用CSS编程时不管预处理器、命名规范等，却用不合适的selector/ids/ Magic numbers等； 忽视表现、内存泄露（并不理解内存泄露的真正含义），不会检测代码； 不会用指标衡量一个产品，或者这种指标旨在自己的电脑、浏览器、设备有效； 忽视软件技术。 要知道，入门容易精通难，计算机和软件的基础对你用JS或浏览器编程都非常重要。Web可能是最有影响力的平台和环境之一，在那里执行的程序必须被小心对待。一位优秀的前端工程师不仅要考虑Web技术和语言，并且还要了解所有不同的组件、系统和概念。 以下是优秀的前端工程师在即时面对普通的任务也会做的事情（这才是市场急需的前端）： DNS解析、使用CDN和关于multiple Hostnames as part of resources request； HTTP Headers （Expires, Cache-Control, If-Modified-Since）； Steve Souders的所有规则（High Performance Websites）； 如何解决PageSpeed, YSlow, Chrome Dev Tools Audit, Chrome Dev Tools Timeline显示的所有问题； 何时把任务传到服务器和客户端； 缓存，预取和负荷技术的使用； Native JS，知道何时从头开始做，何时查找别人的代码，同时可以评估这样做的优缺点； modern MVC Javascript libraries （e.g. AngularJS, EmberJS, ReactJS）, graphic libraries（e.g. D3 , SnapSVG）, DOM manipulation libraries （e.g. jQuery, Zepto）, lazy loading or package management libraries （e.g. RequireJS, CommonJS）, task managers （e.g. Grunt, Gulp）, package managers （e.g. Bower, Componentjs）and testing （e.g. Protractor, Selenium）的相关知识和用法； CSS标准、modern conventions、 strategies （e.g. BEM, SMACSS, OOCSS）的知识和用法； JS的电脑知识（内存管理，单线程的性质，垃圾收集算法，超时，范围，提升，模式）。 换句话说，如果说精通HTML+CSS+JS，了解后端知识，只是60分的合格前端；那么要想成为受追捧、拿高薪的80分优秀前端，要对业务需求和、架构设计有真正的运用；而100分的顶级前端，则必须要能够兼顾技术和设计，更接近“以前端开发为主的全栈工程师”了。 市场不是缺少前端，而是缺少优秀的前端工程师 现在，前端工程师终于前所未有的在Web中占有了一席之地。随着多设备、浏览器和Web标准的演变革命，前端正在成为兼顾逻辑、性能、交互、体验的综合性岗位。 虽然现在互联网行业普遍缺少前端工程师，但是我们相信越来越多的人将会加入前端的大军。不仅是因为大多数前端工作提供的优渥薪水和办公环境，也是因为Web中的前端编程变得越来越有挑战和意义。 最后，分享一组3~4月的100offer拍卖数据，给前端们鼓鼓劲： 前端工程师人均收获8.2个面试机会，已入职的前端工程师平均薪资涨幅达39%。其中，前端offer之王共收获47个offer，最高offer薪水38k*16 ，开自某土豪电商。 加油吧，前端！ oo_vs_oop前两天不是有一个问题是“什么时候用C++而不用C”，我一直觉得问错了，难道不是“能用C++就不用C”么？那么当然就要讨论什么时候用C而不用C++啦。 一直以来都严格遵循OO的原则来进行开发（用的工具是C#和Qt），直到最近，开始接手某同事的代码，整个项目20多个小工程（代码量并不多），除了界面部分用了MFC这种不伦不类的OO以外，所有的代码都是C写的。但是模块化做的非常好。后来跟他讨论为何不用C++，他说其实没有什么特别的，就是习惯和爱好而已，后又补充： 如果不用多态的话，其实不管怎么写，不管用那种语言写，都算不上真正的OO 忽然觉得很有道理…… 游戏服务端发展史 游戏服务端架构发展史（上） http://www.skywind.me/blog/archives/1265 游戏服务端架构发展史（中） http://www.skywind.me/blog/archives/1301 游戏服务端架构发展史（下） Tag系统Tagging研究综述http://portfolio.designbarder.com/2010/12/tagging%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0/ Tag的存在主要有两重作用，一个是keyword，一个是index。Keyword用来用极为简单的词语来说明内容的大意，index用来在搜索时进行匹配。这里主要谈index的作用。 Tag和category的不同在于，一种是逻辑严整的划分，一种是简单直接的划分。category可以通过增加维度来将目标精确化，Tag则往往只需要一个word就能找到精确度很高的目标。 和category相比，Tag最大的优势在于它提供了一个非常快捷的找到精确目标的可能性。这具有两个方面的含义，一个是Tag本身容易识别（Tag好找），一个是Tag对应的目标具有精确性（目标准确）。 在信息总量不变的前提下，Tag的总数量和精确度是一组负相关数值。 在具有大量信息的数据平台，在大量存在的Tag数量和每一个Tag涵盖的内容数量中，就需要进行平衡，以保证Tag的高速和高效。 一方面可以给Tag增加维度，来帮助用户使用Tag。比如把Tag也按照category进行分类（如豆瓣），还比如给某些Tag字体加粗，变色，变大，这也是从hot程度上进行辅助说明。 一方面可以给搜索结果增加维度来保证精确性。比如在哪个channel中搜索，在哪个时段的内容中搜索。 此外还可以使用相关Tag的方法，通过二次甚至多次的搜索，（以比category更快速的方法）来最终找到目标。 Tag的使用不宜过于复杂，否则就失去了其高速高效的意义。 tag对于UGC的产品是一个好的设计。对于视频，图片这些富媒体类型应用来说，tag最大的意义在于可以多维度地从非结构化的数据中提取出结构化的数据。而有了这些结构化的数据之后，原先对于非结构化数据难以进行的分类和挖掘有了进一步的可能，丰富了产品形态和体验，也能挖掘出更多的数据价值。 tag这种设计，是http://del.icio.us开发者Joshua Schachter最早采用的Joshua也曾经解释过，为什么他要用tag。我有一篇博文写过（ http://blog.donews.com/keso/archive/2005/01/30/262073.aspx ），摘录如下： 据Joshua介绍，1998年他在做一个网站时，手头有大量链接需要保存，最初这些链接是被保存在一个文件中。随着保存的内容越来越多，为了更快捷地找到某个链接，他开始在链接后面加上单词的备忘，这就是后来的标签（Tags）。 他说，“我希望借助电脑的帮助，把存储和取回分成两个独立的行为，因此当你给你存储的东西加上tags，你就可以更容易地取回它们。在这么做的时候，你取回其他人存储的东西也会变得更简单。tags促进并放大了这一点。” 图书馆学者们很快发现，随着在社会化网站中tag被大量采用，它成为传统的本体论分类方式之外的一种新的分类方式，群体行为在某些情况下具有惊人的一致性，这种一致性产生了对事物的有意义的社会定义。这种新的分类方式，被称为“大众分类”（folksonomy）。 大多数对tag理解的误区在于，我们仍然用传统分类法的精确性、唯一性，来要求大众分类，这就有点像传统数学家对解模糊数学的某种抵触。“10万人参加了街头抗议”，10万？这么精确？ 流的本质原文:http://www.infoq.com/cn/articles/essence-flow?utm_source=tuicool我们希望使流程更加合理，增加生产能力 下面是我们希望实现的目标: 简化我们的业务，同一时间专注于更少的事情（比如，减少在制品），以此增加生产能力，并缩短提前期 Kaizen ，持续改进流程，消除瓶颈，获得流 缩小交付增量，更快地、持续不断地为客户带来价值 一个良好的流的例子——接力棒 在世界锦标赛或奥林匹克运动会中，4x100接力赛通常是一项非常有趣且激动人心的赛事。 当前的世界记录为（男子）： 400米：43.18秒，由迈克尔·约翰逊于1999年8月26日在塞维利亚创造 4x100米接力赛：36.84秒，由牙买加于2012年8月11日在伦敦创造 虽然迈克尔·约翰逊是有史以来最好的运动员之一，但来自牙买加的团队创造的接力赛记录比他的400米记录快了15%。为什么会这样？有两个原因： 在来自牙买加的团队中，每个人都可以以最大能力跑（400米虽然不长，但要以跑100米的速度来跑是不可能的） 在接力棒交接时没有速度损失因此，在这种情况下，1+1+1+1不等于4，而是大于4！我们如何将这种情况转化为一种与我们的环境相匹配的描述？ 第一，人生那么长，不妨活得自我一点 生活是一场视觉盛宴，任何一套单一的理念都不可能垄断社会，社会是多元价值观并行的，每个人都有不止一个选择。我们没必要把一脚踏进职场另一只脚就迈进婚姻围城当成人生模板，也没必要把活成铿锵玫瑰当成最佳答案，我们可以根据自己的喜好和选择，以自己喜欢的方式生活。小城生活环境安逸，舆论压力巨大，选项单一到必须给周围看客一个交代。大城市不仅职场选择广就业机会多，生活方式也更具包容性。 第二，练习一个人生活的能力，增长阅历保持独立 我不否认在大城市里生活的艰辛，加班累成狗，下班无好友，不及在小城市里朝九晚五按部就班。但是，固定模式的生活里有的没有想象的明天，一成不变的生活不断积累的结果是逐渐让整个人凝固，上进心衰竭。大城市固然忙碌，却没有时间让你空虚寂寞，反而在浮华中学会不轻易辜负那些千载难逢的喜欢，坚守该坚守的，呵护想拥有的，带着也许不能触及的梦想，消磨你的戾气，督促你成长，让你学会独立保持清醒。就脱单这件事儿而言，或许你还不明白自己喜欢什么，但一定知道自己不喜欢什么。 至于孤独，相信你在任何一个地方，都难免会有孤独感，这样的人生必修课全凭个人修炼的火候，不分时间和地点。 第三，随时转换跑道，生活的每一面都很美 日本心理学家森田正马说：“每个人都藏着一个叛逆的小孩。人生重要有哪怕一次，放出自己内心那个叛逆的小孩，这样，到老的时候，我们才不会感叹，这一生，我都在为别人而活着。”身在大城市有无限任性的可能，你会计专业想转行做写手，你学设计出身想玩音乐，你做主持人腻味了想做心理咨询师……统统没有问题，培训课随时读，志同道合的人随时约，别说在小城市只要有网络一样能学到，But，很多事实践比理论更重要，你确定你找活人实践这事儿一样容易吗？至多，只能刷刷豆瓣小组看着坐标望而兴叹吧。 你为自己想要的生活，拼尽全力了吗对自己生活现状很满意的人只有10%，而一半以上的人都对自己的生活不大满意，并且不知道如何改变。 你是否信息闭塞，害怕改变？讲个小故事先。二哥有个研究生同学，毕业一年多，现在在一家做硬件的外企工作。但基本去年一年都在出差，从聊天过程中可以看出，他并不喜欢现在的生活。一方面因为累，另一方面长期出差就意味着和女朋友异地。后来聊的多了，突然发现他的薪资居然和我的差不多。而他所理解的高薪，正是二哥现在的薪资待遇。过程中还聊到另一个同学，他说那位同学辞职创业去了，他认为不应该这样做。现在创业哪里那么容易，失败了怎么办云云。我问他，为何不选择互联网公司？他说我不是科班出身，互联网公司没那么好进。但事实上，他们聊天时听了下，二哥现在工作所用的哪些语言和技术，他基本全会。 回来路上二哥感慨，这位同学研究生时是学生会主席，老师同学都很喜欢他，毕业后混的不太好，感觉略可惜。我说其实他是对自己太不自信。本可以稍微努力下就够到100分，而现在却在50分线上过日子。其实这位同学也很拼，他比二哥辛苦多了，加班，熬夜，长期出差，但因为给自己的定位太低，对除自己行业外的信息都不关注，所以导致暂时只能在50分线上转来转去。 而在这50分线上，我相信他一定是一个领导，同事都喜欢的好员工。他也可以把工作做的很出色。但作为旁观者，我觉得他可以跳出来，看到更大的世界，过更好的生活。至于他认为的另一个同学不该去创业的问题，我私以为也是他信息太闭塞，他理解的创业依然停留在自己出资，放手一搏的模式，而现在的互联网+的大环境下，更多的是你有想法有技术，就会有人给你投资，或者有经济实力的人拉你一起做创业合伙人。成功了，大家都发达，失败了，你至少多了一个创业的经历，并且这个过程中，你的薪资不会比做员工的时候差，也会认识很多平时接触不到的人。从任何一个层面来说，都是好事。 这个同学的故事就让我联想到很多后台发消息给我的小伙伴。很多人都很优秀，但就是意识不到自己的优秀。原本可以拿2w的薪资非要因为自己的信息闭塞和害怕改变而拿1w。 面对自己想要的，你拼尽全力了吗？讲个二哥的故事。二哥第一年考研，是他爸要求的，那时候他自己并不想考。结果是混日子，成绩下来当然是没考上。接着他去找工作，发现简历投出去之后没有任何回音，那时候他慌了，于是决定不找工作，再复习一年，继续考研。 第二次考研没有任何人逼他，是他自己感到了自己的不足。于是这一年，他无心顾及其他，每天都认真看书，做卷子，不断学习，考完研头发长到没法看的地步。成绩下来，很满意，考研成功。 自身的驱动力，比任何人的逼迫和劝说都好使。想要，就努力去够。拼尽全力，自然会得到自己想要的。 羊圈or狼窝我毕业后第一次感到迷茫是14年，每天坐地铁去上班都不知道为了什么。后来辞职转行，换了行业换了环境，一切都是新的，开始学新的知识和技术。一年后辞职跳槽，这次不是因为迷茫，而是因为方向太明确了。我辞职后不久，另一个同事也辞职换了工作，于是我们两个人的状态都差不多，那就是再原本工作的基础上，换了一个稍有不同的工作内容。 前不久交流过一次，我们都觉得现在的公司像狼窝，而之前的公司像羊圈。二者皆没有贬义的意思。羊圈对我们来说，是一个有着自己目标，人人都很和蔼，有亲和力，以自己最舒服的状态在工作的地方，因为舒服，所以事情的推进和发展并不会太快。狼窝则不同，员工都带着一股狼性，有着团队明确的目标，所有人各司其职，做不好是会被批的，所以事情的发展也会较快。现阶段的我们，都更喜欢狼窝的生活，因为虽然很累，收获却很多。原本一年学到的东西，这里根本不会给你那么长的时间，领导给你一个大方向，其他的都要靠自己搞定，不会的，不擅长的，都要在短时间内变得精通熟练。 我想大部分人都是在以羊圈的姿态过生活，并不能说这种姿态不好，我想等我30多岁，有了家庭和孩子，会更喜欢羊圈的生活，而目前，更需要的是在狼窝中找到自己的位置，成为狼中强者。 今天收到一条消息，有位关注者问我，“你是如何做到一直努力的？又是如何熬过最难过的时光的？为何我坚持不了多久就想要放弃？”一直努力是谬贊了，因为我偶尔也偷懒。但大部分时间是在朝着自己想要的方向努力的。因为我害怕过自己不喜欢的生活。举个例子，我刚来北京两年左右，曾经因为租房跟中介闹得很不愉快，我就想过几年以后，如果还要跟中介交涉这些事情，我会不开心的。那我就需要买房。于是通过努力，去年买了过渡房。上班挤地铁，偶尔遇到孕妇，看起来都已经七八个月了，我就会想，天呐，如果我怀孕的时候，还要挤地铁，那我会不开心的。于是为了买车或者怀孕了可以在家不工作也有钱赚，便一边努力工作，一边不放弃写作出版。我就是一个害怕未来不在我计划内或掌控中的人，所以不断努力去够到想要的生活。 伸手党你们身边有伸手党吗？遇到任何问题都喜欢问别人，丝毫不想自己动手动脑。其实这一定程度上是害了自己。无论是生活中，还是工作上，你去看，混的不错的人，一定都是自我驱动力很强的人。他们想要做什么事情，会通过自己达成。当然过程中也会找别人帮忙，但你要明白，资源整合和伸手党本质里是有区别的。你可以有高质量的朋友圈，遇事的时候他们可以帮你解决，但绝不可能遇事了自己吩咐下去别人帮你去执行。 经常收到一些消息，说“我想转行到互联网，你给我说说都有哪些职位，这些职位都具体负责什么吧？”我会说你去某几个互联网招聘网站，先去了解下，然后看下别人的岗位要求，工作内容，再跟自己做一下匹配，看是否喜欢，是否适合，用不了多久，又回来了，说“我看了，你说我从哪个做起呢？”，每当这个时候就会默默的火大，心想我又不了解你，我哪里知道你适合哪个。你自己都不清楚自己定位，又如何做到让一个陌生人帮你做决定呢？如果真的想转行，那百度，知乎，豆瓣，微信自媒体，那么多平台帮你了解呢！还有人会说“我现在的生活很无聊，但不知道自己的兴趣在哪里，你说我该学些什么呢？学英语怎么样。”我觉得这类人也是因为圈子太小，圈子里的人要么没有什么喜好，要么都人云亦云的去学自己原本不大感兴趣的英语了。还是那句话，多接触，多交朋友，你只有见识的多了，才能发现自己喜欢什么。就好像一个小孩子，你不带他去认识这个世界，去了解那些兴趣班，去接触其他小朋友，每天宅家里，他怎么知道自己会喜欢什么呢？ 接触的人中，有一类和这些人是反面。他们会在看完一篇文章后说：感谢你让我又多知道一个兼职渠道，我百度了下，已经找到三家有合作意向的公司了；你文中提到的某个点对我太有用了，我知道下一步该如何做了；除了你说的那个网站，我还有想要分享给你的类似网站。这类人，有很多是很早前就认识的，过段时间你会发现，他们的生活是越来越好的。并且这部分人，确实也有很多成为了不错的朋友，因为是一类人，他们有跟你类似的经历和谈资，这就是吸引力法则，于是圈子就越来越大了。 很多人不知道如何去了解一个新鲜事物。我是在遇到一个自己没接触过，又很好奇的事情后，会打开n个相关网页，看上一通。看完后还有问题，再去找相关的人问，这时候你就不再是一个伸手党了，因为能自己了解的东西，你都了解过了，此时提出的问题，一定是你经过思考后的，别人也乐得一答。 长期的坚持，会成为一种习惯。很多人说你是如何做到每天推送消息，每天读书写字的。刘同说过，“一件事只要你坚持得足够久，“坚持”就会慢慢变成“习惯”。原本需要费力去驱动的事情就成了家常便饭，原本下定决心才能开始的事情也变得理所当然。”我非常认同这句话。其实人都有犯懒的时候，推送文章这件事，我是因为想到有这么多关注者等着我，就会立马动力满满，所以无论回家多晚，我一定会坚持发。而写作，其实也有一个痛苦的坚持过程。去年接了一本书，8w字，根据交稿日期算了下，每天至少更新2000字，开始的前三天，说实话每天都有种“想死”的感觉，每天早上一睁眼，第一反应是今天又欠编辑2000字。那种感觉，真的是希望某天不要醒来，这样就可以不写字。但有意义么？接了就要完成。于是坚持了一天，两天，三天，到第四天，突然发现，写作这件事，成为习惯了。如果没写完，自己会很自觉的不看电视剧，写完了，会感觉全身筋骨都舒服。现在读书写文就变成习惯了，所以毫无压力了。 与其抱怨，不如趁早把事解决很多人遇到问题的时候喜欢抱怨，打电话和所有能扯上关系的人去煲电话粥，说自己过的如何糟糕，工作压力如何大，领导如何难搞。我觉得还是不够忙。 曾经自己也喜欢遇到点小事就抱怨，入职新公司后，压力也很大，突然发现自己变了，不再跟很多人闲聊，当然，还是不断会有人找我问问题，但我都说抽空回复，也不会跟任何人抱怨，因为根本没时间。这件事是你的，你就得在规定时间内完成。所以大脑快速运转的都是如何解决，而非抱怨。 如果你的生活目前很糟糕，一时半会改变不了，那不如把他当成一种人生经历，当成生活考验你的一种方式，去想办法解决；如果你的工作压力很大，问题很多，不妨把他当成魔鬼训练营，因为如果连这些问题都能解决，那你一定是强者，未来跳槽，你一定会有更多的谈资和加分项。 不必理会哪些不喜欢你的人写文章最初，微博大号转发我的文章，会特别在意哪些评论。负面评论会让我不开心好久。现在不了，有人评论你，只能说明你出现在他的视野内了。而出现在他的视野内，我就已经很成功了。所以不必想要所有人都喜欢你，毕竟这世界上还有很多喜欢动动嘴就发泄自己对生活不满的人。“如果你坚信你的做法显而易见是对的，那就不要去向误会的人解释了。因为他们针对的根本就不是你的做法，而是你的人。所以，你唯一要做的就是表面微笑，但心里别把他们当自己人。”无论是工作，生活，还是对自己的兴趣爱好，都是如此。与你共勉。 如果你不喜欢现在的生活，就立马做出改变。你身边一定有很多朋友，师长，去跟他们聊，聊你的困惑和迷茫，但一定要事先经过自己的思考，因为如果连你都无法描述你对生活的不满意，那别人也无法帮你走出泥潭。别人可以帮你指路，但走的人，只能是你自己。不要浑浑噩噩，不知所以的过生活，那样会让你几年后发现与同龄人的差距越来越大，你会变得不喜欢自己，对生活也失去信心。想要过上自己想要的生活，就首先要用十二分的真心和努力去对待现在的生活。 博客资源 C/C++ Skywind Inside | 写自己的代码，让别人猜去吧 PHP (重要) walu’s wiki|PHP扩展开发及内核应用 / 目录入口 App 后端 Mysql Redis http://objectrocket.com/blog/how-to Linux Mitchell Anicas Technical Writer @ DigitalOcean Javascript/Html/Css 互联网技术和架构 触摸屏网站开发系列（一）-ios web App应用程序（ios meta） Flexbox Advanced Cross-Browser Flexbox R Python","categories":[{"name":"杂记","slug":"杂记","permalink":"https://straysh.github.life/categories/%E6%9D%82%E8%AE%B0/"}],"tags":[{"name":"杂","slug":"杂","permalink":"https://straysh.github.life/tags/%E6%9D%82/"}]},{"title":"Mysql_索引","slug":"Mysql-索引","date":"2020-02-24T10:12:01.000Z","updated":"2020-02-28T08:00:54.149Z","comments":true,"path":"2020/02/24/Mysql-索引/","link":"","permalink":"https://straysh.github.life/2020/02/24/Mysql-%E7%B4%A2%E5%BC%95/","excerpt":"explain Indexes are used to find rows with specific column values quickly.","text":"explain Indexes are used to find rows with specific column values quickly. id (query id) - 查询执行的顺序 select_type (type of statement) - 查询的类型 SIMPLE，简单查询，不使用UNION或子查询等。 PRIMARY，子查询中的最外层查询。 UNION，UNION中的第二个或后面的select查询。 SUBQUERY，子查询中的第一个select，结果不依赖于外部查询。 DEPENDENT SUBQUERY，子查询中的第一个select，结果依赖于外部查询。 DERIVED，派生表的select,from子句的子查询。 table (table referenced) - 使用的表名，可能是表的别名。 type (join type) - 连接的类型 ALL - Full Table Scan，全表查询 index - Full Index Scan，只遍历索引树。 range - 只检索给定范围的行且使用索引。 ref - 连接匹配条件，即哪些列或常量被用于查找索引列上的值。 eq_ref - 类似ref，区别是使用的索引是唯一索引，对于每个索引值表中只有一条记录匹配。 const、system - 当mysql对查询某部分优化并转换为一个常量时，使用这些类型访问。例如将主键置于where条件中，MySQL就能将该查询转换为一个常量。system是const的特例，当查询表中只有一行记录时。 NULL - 优化后甚至不需要访问表或索引，如取最小值可通过单独索引完成查找。 possible_keys (which keys could have been used) - 可能使用到的索引 key (key that was used) - 实际使用到的索引，若没有任何索引则显示为null key_len (length of used key) - (实际使用的)索引的长度 表示索引中使用的字节数（通过定义得到的理论值） ref (columns compared to index) - 列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows (amount of rows searched) - 扫描的行数 Extra (additional information) - 额外的信息 Using index - 使用了索引覆盖。 Using where - 不用读取表中的数据，只通过索引就能完成查询。即通常所说的索引覆盖。 Using filesort - 当含有order by操作，且无法通过索引完成排序。 Using join buffer - 连接条件没有使用索引，且需要连接缓冲区来存储中间结果。通常是一个需要优化的信号。 Impossible where - where语句可能导致没有符合条件的行。 Select tables optimized away - 仅通过使用索引，优化器可能仅从聚合函数结果中返回一行。如MIN/MAX/MyISAM引擎中的Count(*) No tables used - Query语句中使用from dual（即空表）或不含任何from子句 高级用法explain FORMAT=JSON select xxx from yyy where zzz;查看用例 索引用以高效查询数据的数据结构。 Hash索引底层数据结构是哈希表，只能用于等值查询，在碰撞场景下效率低，无法利用索引完成排序，没有最左匹配特性。 B+树索引 - Innodb底层数据结构是多路平衡查询树，节点天然有序，额外的适用于范围查询。详细讨论参考另一篇文章 聚簇索引通常就是主键索引。索引所在的页储存了数据行。 非聚簇索引除聚簇索引之外其他的索引都称为非聚簇索引。索引所在的页只存储了主键值，若需要其他数据需要回表查询。 联合索引的索引覆盖创建测试数据库 1234567891011121314151617181920212223Create Table: CREATE TABLE `teacher` ( `id` int(11) NOT NULL, `name` varchar(32) NOT NULL DEFAULT &#x27;&#x27;, `age` int(11) NOT NULL DEFAULT 0, `subject` varchar(32) NOT NULL DEFAULT &#x27;&#x27;, `salary` int(11) NOT NULL DEFAULT 0, PRIMARY KEY (`id`), KEY `idx_name` (`name`), KEY `idx_name_subject_salary` (`name`,`subject`,`salary`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `teacher` VALUES (10101,&#x27;Srinivasan&#x27;,30,&#x27;Comp. Sci.&#x27;,65000),(12121,&#x27;Wu&#x27;,26,&#x27;Finance&#x27;,90000),(15151,&#x27;Mozart&#x27;,50,&#x27;Music&#x27;,40000),(22222,&#x27;Einstein&#x27;,76,&#x27;Physics&#x27;,95000),(32343,&#x27;El Said&#x27;,35,&#x27;History&#x27;,80000),(33456,&#x27;Gold&#x27;,49,&#x27;Physics&#x27;,87000),(45565,&#x27;Katz&#x27;,42,&#x27;Comp. Sci.&#x27;,75000),(58583,&#x27;Califieri&#x27;,38,&#x27;History&#x27;,60000),(76543,&#x27;Singh&#x27;,33,&#x27;Finance&#x27;,80000),(76766,&#x27;Crick&#x27;,46,&#x27;Biology&#x27;,72000),(83821,&#x27;Brandt&#x27;,29,&#x27;Comp. Sci.&#x27;,92000),(98345,&#x27;Kim&#x27;,31,&#x27;Elec. Eng.&#x27;,80000); explain FORMAT=JSON select subject,salary from teacher where name=&quot;Mozart&quot;;输出: 123456789101112131415161718&#123; &quot;query_block&quot;: &#123; &quot;select_id&quot;: 1, &quot;table&quot;: &#123; &quot;table_name&quot;: &quot;teacher&quot;, &quot;access_type&quot;: &quot;ref&quot;, &quot;possible_keys&quot;: [&quot;idx_name&quot;, &quot;idx_name_subject_salary&quot;], &quot;key&quot;: &quot;idx_name_subject_salary&quot;, &quot;key_length&quot;: &quot;130&quot;, &quot;used_key_parts&quot;: [&quot;name&quot;], &quot;ref&quot;: [&quot;const&quot;], &quot;rows&quot;: 1, &quot;filtered&quot;: 100, &quot;attached_condition&quot;: &quot;teacher.`name` = &#x27;Mozart&#x27;&quot;, &quot;using_index&quot;: true //索引覆盖 &#125; &#125;&#125; Using index (JSON property: using_index) The column information is retrieved from the table using only information in the index tree without having to do an additional seek to read the actual row. 索引如何加速排序 Mysql的ICP（Index Condition Pushdown Optimization） 索引的存储和缓存 索引区分度和索引长度 … 联合索引的最左匹配原则假设索引idx_a_b_c(`a`,`b`,`c`)索引是否使用where a=3where a=3 and b=4where a=3 and b=4 and c=5where c=5 and a=3 and b=4where b=4 / where b=4 and c=5 /where c=5where a=3 and c=5where a=3 and b&gt;4 and c=5where a is null and b is not nullwhere a &lt;&gt; 3 and b=4where a^3&gt;0where a=3 and b like 'k%' and c=5where a=3 and b like '%k%' and c=5where a=3 and b like 'k%k%' and c=5 索引下推 - Index Condition Pushdown Optimization索引失效场景不看后悔的腾讯面试题：SQL语句为什么执行的很慢？ 参考资料： 15.6.2.1 Clustered and Secondary Indexes 8.3.1 How MySQL Uses Indexes 8.3.9 Comparison of B-Tree and Hash Indexes 8.3.6 Multiple-Column Indexes covering index Mysql索引简明教程","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"算法_二叉树","slug":"算法-二叉树","date":"2020-02-19T03:30:46.000Z","updated":"2020-02-27T05:05:39.119Z","comments":true,"path":"2020/02/19/算法-二叉树/","link":"","permalink":"https://straysh.github.life/2020/02/19/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"（最大）二叉堆是一个维护着最大堆属性的完全二叉树。最大堆用来实现高效的优先级队列PQ(Priority Queue)。","text":"（最大）二叉堆是一个维护着最大堆属性的完全二叉树。最大堆用来实现高效的优先级队列PQ(Priority Queue)。 为集中精力讨论二叉堆本身，本教学中的最大二叉堆中不包含相同的数值。 完全二叉树的定义：除最下层，其它层组成满二叉树（每个节点上都有值），而最下层的节点都靠左排列不留空隙。 最大二叉堆属性：每个节点的父节点(除了根节点)，都比该节点大。 优先级队列： Enqueue(x)，将新元素x放入队列(按照某种次序) y = Dequeue()，将序列中优先级最大的值返回；若存在相同优先级的值，则返回最先插入的值，即退化到FIFO 最大二叉堆的高度 &lt;= log N(二分) 完全二叉树可以高效的存储为数组，因为展开为数组之后，各节点之间没有空隙。为简化遍历操作，我们假设数组下标从1开始。 parent(i) = i&gt;&gt;1，例如8号节点的父节点为4号则parent(8) = 8&gt;&gt;1 = 4 left(i) = i&lt;&lt;1 right(i) = (i&lt;&lt;1) + 1 本教学中会介绍一下（最大）二叉堆的操作： Insert(v) O(log N) ExtractMax() O(log N) Create(A) O(N log N)版本 Create(A) O(N)版本 HeapSort() O(N log N) Inset(v)插入操作只能在最末尾的位置进行，这是为来保证二叉树的完全二叉树属性。但此时也可能会破坏最大堆属性，因此插入点可能需要向上移动。这个操作称之为ShitUp或BubbleUp或IncreaseKey。 ExtractMax删除堆顶数据，然后将最末尾数据移动到堆顶，此时会破坏最大堆结构，因此需要向下调整。称之为ShiftDown或BubbleDown或Heapify操作。在向下调整时，总是和较大的子节点交换。 PQ - 优先级队列到此，有了Insert(v)和ExtractMax()操作，我们实现了优先级队列。 前中后序遍历1234567891011121314151617181920212223242526272829package mainimport ( &quot;fmt&quot; &quot;github.com/straysh/basis_go/datastructure&quot;)func main() &#123; n1 := &amp;datastructure.BTNode&#123;Data: 81&#125; n2 := &amp;datastructure.BTNode&#123;Data: 80&#125; n3 := &amp;datastructure.BTNode&#123;Data: 64&#125; n4 := &amp;datastructure.BTNode&#123;Data: 59&#125; n5 := &amp;datastructure.BTNode&#123;Data: 67&#125; n6 := &amp;datastructure.BTNode&#123;Data: 48&#125; n7 := &amp;datastructure.BTNode&#123;Data: 57&#125; n8 := &amp;datastructure.BTNode&#123;Data: 35&#125; n9 := &amp;datastructure.BTNode&#123;Data: 29&#125; n10 := &amp;datastructure.BTNode&#123;Data: 30&#125; n11 := &amp;datastructure.BTNode&#123;Data: 46&#125; n1.Left = n2; n1.Right = n3 n2.Left = n4; n2.Right = n5 n3.Left = n6; n3.Right = n7 n4.Left = n8; n4.Right = n9 n5.Left = n10;n5.Right = n11 //前序遍历 preOrder(n1)&#125; 递归法 - 前序遍历123456789func preOrder(node *datastructure.BTNode) &#123; if node==nil &#123; return &#125; fmt.Printf(&quot;%d &quot;, node.Data) preOrder(node.Left) preOrder(node.Right)&#125;//输出: 81 80 59 35 29 67 30 46 64 48 57 递归法 - 中序遍历123456789func inOrder(node *datastructure.BTNode) &#123; if node==nil &#123; return &#125; inOrder(node.Left) fmt.Printf(&quot;%d &quot;, node.Data) inOrder(node.Right)&#125;//输出: 35 59 29 80 30 67 46 81 48 64 57 递归法 - 后序遍历123456789func postOrder(node *datastructure.BTNode) &#123; if node==nil &#123; return &#125; postOrder(node.Left) postOrder(node.Right) fmt.Printf(&quot;%d &quot;, node.Data)&#125;//输出: 35 29 59 30 46 67 80 48 57 64 81 深度优先(DSF) - 前序遍历12345678910111213141516func dsf(node *datastructure.BTNode) &#123; stack := datastructure.NewStackBT(11) stack.Push(node) for ;stack.Size() != 0; &#123; item := stack.Pop() fmt.Printf(&quot;%d &quot;, item.Data) if item.Right!=nil &#123; stack.Push(item.Right) &#125; if item.Left!=nil&#123; stack.Push(item.Left) &#125; &#125;&#125;//输出: 81 80 59 35 29 67 30 46 64 48 57 深度优先(DSF) - 中序遍历1234567891011121314151617181920func dsfInOrder(node *datastructure.BTNode) &#123; stack := datastructure.NewStackBT(11) p := node for ;p!=nil; &#123; if p.Left != nil &#123; stack.Push(p) p = p.Left &#125; else &#123; fmt.Printf(&quot;%d &quot;, p.Data) p = p.Right for ;p==nil &amp;&amp; stack.Size()&gt;0; &#123; p = stack.Pop() fmt.Printf(&quot;%d &quot;, p.Data) p = p.Right &#125; &#125; &#125;&#125;输出: 35 59 29 80 30 67 46 81 48 64 57 深度优先(DSF) - 后序遍历//todo 广度优先(BSF) - 按层遍历12345678910111213141516func bsf(node *datastructure.BTNode) &#123; queue := datastructure.NewQueueBT(11) queue.Enqueue(node) for ;queue.Size()&gt;0; &#123; p := queue.Dequeue() fmt.Printf(&quot;%d &quot;, p.Data) if p.Left!=nil &#123; queue.Enqueue(p.Left) &#125; if p.Right!=nil &#123; queue.Enqueue(p.Right) &#125; &#125;&#125; 求树的高度1234567func btHeight(node *datastructure.BTNode) int &#123; if node == nil &#123; return 0 &#125; return max(btHeight(node.Left), btHeight(node.Right)) + 1&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"算法_位掩码","slug":"算法-位掩码","date":"2020-02-18T10:46:57.000Z","updated":"2020-02-27T05:05:42.327Z","comments":true,"path":"2020/02/18/算法-位掩码/","link":"","permalink":"https://straysh.github.life/2020/02/18/%E7%AE%97%E6%B3%95-%E4%BD%8D%E6%8E%A9%E7%A0%81/","excerpt":"","text":"bitmask提供了一种高效的操作小规模布尔值的方式。通过位操作，布尔位标志可以很容易且快速的检测、取正、取反。例如[旅行售货员问题]。 约定： 数值S &lt;= 32767(215-1) 置位, 按索引将i位值为1.索引从0开始，但是从左右边向左数。 取位，按索引检查i位是否是1。 反转位，将i位置值反转。 清除位，将位置i置0。 最低有效位(LSB)，快速检测最低位是是1的位置。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"草稿_go语法","slug":"草稿-go语法","date":"2020-02-14T07:27:42.000Z","updated":"2020-03-01T13:59:42.608Z","comments":true,"path":"2020/02/14/草稿-go语法/","link":"","permalink":"https://straysh.github.life/2020/02/14/%E8%8D%89%E7%A8%BF-go%E8%AF%AD%E6%B3%95/","excerpt":"","text":"数据类型 布尔型:只能取false或true．例var b bool = true. 数字类型: int - 整型 float32/float64 - 浮点型 原生支持复数 字符串类型 派生类型 指针类型 数组类型 切片类型 结构类型 struct channel类型 函数类型 接口类型interface map类型 go语法中一些特殊的数字类型类型长度byte类似 `uint8`rune类似 `int32`uint32或 64位int取决于操作系统uintptr无符号整型,用于存放一个指针 字符串类型go中字符串类型是只读的.采用UTF-8编码.每个字符对应一个rune类型. 1234string转int: int, err := strconv.Atoi(string) // anscii to intstring转int64: int64, err := strconv.ParseInt(string, 10, 64)int转string: string := strconv.Itoa(int)int64转string: string := strconv.FormatInt(int64, 10) 使用range关键字迭代字符串时,每次得到一个字符,rune类型,循环的索引值是字节为单位的偏移量.该rune类型的字符被称为代码点. 12345678910str := &quot;go语言&quot;for idx, char := range str &#123; fmt.Printf(&quot;%d:%#U\\n&quot;, idx, char)&#125;UTF-8中,一个英文字符是1字节,一个中文字符是3字节,输出:0:U+0067 &#x27;g&#x27;1:U+006F &#x27;o&#x27;2:U+8BED &#x27;语&#x27;5:U+8A00 &#x27;言&#x27; 变量及声明当一个变量被var声明之后,系统自动赋予它零值: bool为false 整型为 0 浮点型为0.0 字符串为空&quot;&quot; 派生类型均为nil 如果你想交换两个变量的值,则可以简单的使用:a,b = b,a nil值语法错误:未指定类型 var x = nil // 错误 给一个nil指的slice添加元素是合法的,但map不行. 12var m map[string]intm[&quot;one&quot;] = 1 // 错误 字符串拼接 运算符+ 1str := &quot;go&quot; + &quot;语&quot; + &quot;言&quot; // 输出 &quot;go语言&quot; 由于字符串是只读的,因此会产生很对碎片化的无用字符串,等待垃圾回收,性能较差. fmt.Sprintf()内部使用[]byte实现,不像运算符会产生很多临时字符串,但内部逻辑复杂,很多判断,且为了兼容数据类型使用了interface,性能一般 strings.Join()会计算最终字符串的大小,先申请合适大小的内存,在逐个字节的填入,为了构造这个数组,性能一般. bytes.Buffer 123456var buf bytes.Bufferbuf.WriteString(&quot;go&quot;)buf.WriteString(&quot;语&quot;)buf.WriteString(&quot;言&quot;)fmt.Print(buffe.String()) 比较理想.内部实现对内存增长有优化,若能预估最终长度,可以使用buffer.Grow()来设置capacity. string.Builder 123456var bs strings.Builderbs.WriteString(&quot;go&quot;)bs.WriteString(&quot;语&quot;)bs.WriteString(&quot;言&quot;)fmt.Print(bs.String()) 比较理想.内部使用slice来保存和管理内容,slice指向底层的数组.同样提供了Grow()来设置容量.但strings.Builder是非线程安全的,性能上和bytes.Buffer相差无几. 数组类型 + 长度 定义一个数组类型.所以[5]int和[10]int是两个不同的类型. 注:特殊的,我们可以使用[10]interface&#123;&#125;来定一个存放任意类型数据的数组,但使用值时需要先做类型判断. 声明数组 var关键字var arr1 [5]int arr1的类型是[5]int new关键字var arr1 = new([5]int) arr1的类型是 *[5]int 其他 123var arrAge = [5]int&#123;18, 20, 30, 35&#125;var arrLazy = [...]int&#123;5, 6, 7, 8, 9&#125;var arKeyValue = [5]string&#123;3:&quot;Chris&quot;, 4:&quot;Ron&quot;&#125; 切片切片的底层是一个数组,自身包含:①对数组的引用 ②起始偏移量 ③中止偏移量 ④切片容量可以理解为切片是对数组的一个动态窗口. 有点(多个)切片可以引用同一个数组,因此不需要额外的内存空间,比直接使用数组高效,因此go中更多的使用切片. 注:切片本身就是一个引用类型, 指向切片的指针没有意义.类似指向interface的指针也没有意义. 声明切片var slice1 []int只需要指定类型,而不能指定长度.(指定了长度就是数组了!!!)一个切片在未初始化前,值是nil,长度是0. 初始化切片的操作:var slice1 []int = arr1[start:end] 包含start索引,不包含end索引.即slice1包含arr1[start..end-1]的元素. var slice2 = []int&#123;1,2,3&#125;也可以初始化切片.切片将指向底层创建的一个匿名数组.注:这里的var slice2是var slice2 []int的简写. 作为数据源的数组还未定义时,可以使用make关键字创建切片:var slice2 = make([]int, 5)第二参数表示切片的长度,同样这里将切片类型var slice2 []int省略了,简写作var slice2.更常用的,简写为slice2 := make([]int, 5). make关键字创建切片的完整语法make([]int, 10, 50),第三个参数50指定了底层数组的大小,第二个参数10指定了切片的初始长度. 从数组或切片中生成一个新的切片:a[low : high : max] 例如: 12a := [...]int&#123;1,2,3,4,5,6&#125;b := a[1:3:5] 这里的切片b的长度是3-1=2,容量是5-1=4. 1, 3, 5都是索引. 切片重组(reslice)slice1 := make([]type, start_length, capacity) start_length是切片的初始长度,capacity是底层数组的大小也是切片的容量. 改变切片长度的操作称之为切片重组,但切片重组时不能超过底层数组的容量. 12345var slice1 []int = make([]int, 2, 5)slice2 := slice1[2:6]fmt.Println(slice2)// panic: runtime error: slice bounds out of range [:6] with capacity 5 切片扩容func append(s[]T, x ...T) []T当使用append函数时,可能会引发切片扩容,扩容后的切片将指向一个新的底层数组,跟之前的底层数组不再有关联. 字典map声明语法:var map1 map[string]int 初始化语法: var map2 = make(map[string]int, 10) 对值为nil的切片append是合法的(触发了切片扩容),但对值为nil的map赋值是非法的. 12345var s []ints = append(s, 2) //合法var m map[string]intm[&quot;one&quot;] = 1 // 非法 访问map的元素下标访问: val = aMap[key]判断下标是否存在: 12345if val,ok := aMap[&quot;dummy&quot;]; !ok &#123; fmt.Println(&quot;key dummy not exists!&quot;)&#125;fmt.Printf(&quot;value:%v\\n&quot;, value) 123456// 测试 map1 == nilvar map1 map[string]int // truemap1 := map[string]int&#123;&#125; //falsemap1 := map[string]int&#123;&quot;one&quot;: 1, &quot;two&quot;:2&#125; //falsemap1 := make(map[string]int) //falsemap1 := make(map[string]int, 10) // false 删除keydelete(map1, &quot;key&quot;).若key不存在也不会报错. go语法陷阱range循环中生成的值是真实集合的拷贝,它不是原有元素的引用.更新这些值并不会修改原来得值.使用这些值的地址并不会得到原有数据的指针. 12345678a := []int&#123;1, 2, 3, 4&#125;for _, v := range a &#123; v *= 2&#125;fmt.Println(a)输出: [1 2 3 4] defer 规则一 当defer被声明时,其参数就会被实时解析 规则二 defer执行顺序为先进后出,即栈LIFO 规则三 defer可以读取有名返回值,即可以改变有返回参数的值(不建议如此增加代码复杂度) 函数 普通带函数名的函数 匿名函数或者lambda函数 方法(methods) 函数可以作为类型使用type addFunc func(int, int) int 这里不需要函数体.函数是一等值(first-class value):它们可以赋值给变量:add := addFunc go默认使用值传递来传递形参.函数调用时,若希望在函数内改变原来的数据,则可以传递数据的指针到函数. 切片/字典/接口/通道(slice/map/interface/channel)模式使用引用传递(不需显示指出指针) 名称说明close用于关闭`channel`len、cap返回某个类型的长度/数量(字符串、数组、切片、map和channel)；返回某个类型的容量(只能用于切片、map)new、make用于分配内存：1. `new`用于值类型和用户定义类型，如自定义结构体.`new(T)`分配类型为T的零值并返回其地址，即指向类型T的指针.其也可以被用于基本类型:`v := new(int)`.2. `make`用于内置引用类型，如切片、map、channel.`make(T)`返回类型T的初始化后的值，因此它比`new`进行更多的工作.二者都是内存的分配(堆上)，但`make`只用于slice、map、channel的初始化(非零值)；而`new`用于类型的内存分配，并且内存置为零.copy、append`copy`复制切片，`append`添加元素到切片panic、recover用于错误处理机制 递归与回调使用递归函数常常遇到栈溢出的错误,一般使用惰性求值的技术解决.在go中可以使用channel和gorutine. 闭包123456789101112131415161718192021222324252627282930package mainimport &quot;fmt&quot;func main() &#123; add := addNumber(5) add(1) add(1) add(1)&#125;func addNumber(x int) func(int) &#123; fmt.Printf(&quot;%p, %d\\n&quot;, &amp;x, x) return func(y int) &#123; k := x + y x = k y = k fmt.Printf(&quot;%p, %d\\n&quot;, &amp;x, x) fmt.Printf(&quot;%p, %d\\n&quot;, &amp;y, y) &#125;&#125;输出:0xc0000aa010, 50xc0000aa010, 60xc0000aa030, 60xc0000aa010, 70xc0000aa048, 70xc0000aa010, 80xc0000aa060, 8 type关键字复习:go语言中的基本数据类型:布尔值、数值（整数、浮点数、复数）、字符串.另有一些基于上述基本类型衍生的：byte、uintptr、rune、error等 自定义类型type IZ int然后可以使用var a IZ = 5变量a拥有底层类型int，类型IZ和int之间也可以显示的转换.例如：将IZ类型转换成int类型：b := int(a). 12345678910111213141516package mainimport &quot;fmt&quot;func main() &#123; type IZ int var a IZ = 5 b := int(a) fmt.Printf(&quot;a&#x27;s type:%T\\n&quot;, a) fmt.Printf(&quot;b&#x27;s type:%T\\n&quot;, b)&#125;输出：a&#x27;s type:main.IZb&#x27;s type:int 可以看出来，类型IZ和类型int是两个不同的类型. 类型别名type IZ int语法中，新类型IZ无法访问原类型int的方法.此时使用别名语法:type IZ = int. 常用的类型： 类型可以是基本类型, 如:bool、int、float、string; 结构化的(复合类型), 如:struct、array、slice、map、channel; 只描述类型的行为的, 如:interface; 结构化的类型没有真正的值,使用nil作为零值.注意在go语言中不存在类型继承. 结构体struct结构体所包含的数据在内存中是连续块存在的. 接口interfce接口中包含若干只有定义而没有实现的函数.结构体不需要显示的声明自己实现了某接口.相反,只要结构体实现了某接口的所有函数,即可以认为结构体实现了该接口. 1234567891011121314151617181920// 接口B定义在包b中package btype B interface&#123; print()&#125;// 结构体A定义在包a中package atype A struct&#123; name string&#125;func (a *A) print() &#123; fmt.Println(&quot;aaa&quot;)&#125;// 在语法上不需显示的指出A实现了接B,即没有`implement`语法// A的方法`print`与接口`B`的`print`有相同的函数签名,即认为结构体A实现了接口B// 即时接口`B`和结构体`A`分别在两个不同的包里定义import &quot;path/to/b&quot;var x B = A&#123;&quot;straysh&quot;&#125; // 合法x.print() 接口可以嵌套接口,但不能嵌套结构体.嵌套接口时,多个不同的接口可以有包含相同签名的函数go 1.14实现. 接口类型变量的类型转换v := a.(T) 没有断言的强制类型转换,可能触发panic:invalid type assertion正确的做法是: 1234if val,ok := a.(T);!ok &#123; // handle with error&#125;// handle with val 当a可能含有多种可能的类型时: 123456789101112var a interface&#123;&#125;switch a.(type) &#123;case *Square: //case *Circle: //case nil: //default: //&#125; 测试一个值是否实现了某接口12345678type B interface&#123; print()&#125;if val,ok := a.(B); !ok &#123; // handle with error&#125;// handle with val 接收器123456789type A struct &#123; name string&#125;func (a *A) dummy()&#123; fmt.Println(&quot;aaaa&quot;)&#125;func (a A) foo()&#123; fmt.Println(&quot;foo...&quot;)&#125; 对于一个接结构体T, 不论变量是T类型还是*T类型,都可以调动值方法或指针方法. 12345678910111213a1 := &amp;A&#123;&#125;a1.dummy()a1.foo()a2 := A&#123;&#125;a2.dummy()a2.foo()输出:dummy...foo...dummy...foo... 但若变量是一个接口类型 12345678910111213type Intf interface &#123; M1() M2()&#125;type T struct &#123; Name string&#125;func (t T) M1()&#123; t.Name = &quot;name1&quot;&#125;func (t *T) M2()&#123; t.Name = &quot;name2&quot;&#125; 接口类型的变量无法调用指针接收器的函数: 12345678910var t1 T = T&#123;&quot;T1&quot;&#125; // 正确t1.M1()t1.M2()var t2 Intf = T&#123;&quot;T2&quot;&#125; //语法错误t2.M1()t2.M2()输出:cannot use composite literal (type T) as type Intf in assignment: T does not implement Intf (M2 method has pointer receiver) 何时使用值类型接收器 接收器本身就是引用类型(map、func、channel). 接收器是切片，且该方法不会触发切片重组或扩容. 接收器是一个小数组或原生的结构体类型且没有可修改的字段或指针，又或接收器是基本数据类型(bool、int、float、string) 何时使用指针类型 若方法需要修改接收器,则必须是指针类型. 若接收器包含sync.Mutex或包含了锁的结构体,则必须是指针类型,以避免拷贝. 若接收器是一个大结构体或大数组,为性能考量,需要只用指针类型. 若接收器是结构体、数组、切片,blablabla…,使用指针类型. 嵌套结构体中方法提升S(T)S(*T)S包含T的方法集YesYesS包含*T的方法集NoYes*S包含T的方法集YesYes*S包含*T的方法集YesYes 当变量类型本身就是引用类型是,上述方法集约束是强约束.否则,go有语法糖会自动转换. 挖坑待读 图解Go语言内存分配 码农桃花源 为什么遍历 Go map 是无序的？for range map 启动时会随机选择map的起始位置fastrand() Golang垃圾回收 图解Golang的GC算法 为什么golang的gc不用stw？ Golang 垃圾回收剖析 Go 语言的实现为何使用Plan 9的汇编器? Golang调度器 Go 语言调度（一）: 系统调度 Go调度器系列（2）宏观看调度器","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"算法_非基于比较的排序算法","slug":"算法-非基于比较的排序算法","date":"2020-02-14T03:20:46.000Z","updated":"2020-02-27T05:05:56.903Z","comments":true,"path":"2020/02/14/算法-非基于比较的排序算法/","link":"","permalink":"https://straysh.github.life/2020/02/14/%E7%AE%97%E6%B3%95-%E9%9D%9E%E5%9F%BA%E4%BA%8E%E6%AF%94%E8%BE%83%E7%9A%84%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"非基于比较的排序算法 - O(N) 计数排序 基数排序 由于不比较元素对，它们比基于比较的排序最优解O(N log N)还要快。","text":"非基于比较的排序算法 - O(N) 计数排序 基数排序 由于不比较元素对，它们比基于比较的排序最优解O(N log N)还要快。 众所周知（本教学中未证明），基于比较的排序算法，最快也是O(N log N)。因此，任何最快情况下复杂度是O(N log N)的算法，例如归并排序，被认为是最佳的。 计数排序假设：待排序序列是少量整型数值，我们统计各个数值出现的次数，然后迭代数值范围来输出正序的数组。 例如，下例中所有数均在[1..9]中，我们先计算1出现的次数，2出现的次数…9出现的次数，然后迭代1-9并按出现次数x打印数值。 统计数值频率是O(N)，打印正序数列是O(N+K)，其中k是数值取值范围。当k较小时，即O(N)。 若k极大，则受内存限制统计频率将不可行。 基数排序假设：待排序的元素是大范围但小数值的整数，我们可以在基数排序中利用计数排序的思想来达到线性的时间复杂度。 在基数排序中，我们将每个元素看到是包含w位的字符串（不足w位的元素，左侧补零）。 从最低位（最右侧）到最高位（最左侧），逐位按计数排序方法将数据源排序，知道最高位排序完成，则正序。 我们只需要O(w * (N+K))，本例中w=4,K=10。 讨论排序算法的特点不仅仅是比较、非比较、递归、迭代的区别，下面讨论一下原地排序/非原地排序，稳定排序/非稳定排序。 原地排序原地排序指的是在排序时，只需要有限额外内存的排序算法。即可以有少量，有数的额外变量，但不能有不确定的变量数特别是这个数的大小依赖与数据源的大小N。 归并排序在子步骤合并阶段需要额外的空间，不是原地排序算法。 冒泡、选择、插入是原地排序 快速排序，是原地排序 归并排序，不是原地排序 计数排序，不是原地排序 基数排序，不是原地排序 稳定排序当排序完成之后，值相同的元素的相对为止保持不变的算法，称之为稳定排序算法。 讨论：有哪些稳定排序算法？","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"算法_稳定排序算法","slug":"算法-稳定排序算法","date":"2020-02-13T10:18:50.000Z","updated":"2020-02-27T05:05:53.159Z","comments":true,"path":"2020/02/13/算法-稳定排序算法/","link":"","permalink":"https://straysh.github.life/2020/02/13/%E7%AE%97%E6%B3%95-%E7%A8%B3%E5%AE%9A%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"稳定排序算法当排序完成之后，值相同的元素的相对为止保持不变的算法，称之为稳定排序算法。","text":"稳定排序算法当排序完成之后，值相同的元素的相对为止保持不变的算法，称之为稳定排序算法。 讨论：有哪些稳定排序算法？ 冒泡 选择 插入 归并","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"算法_排序_基于比较的排序","slug":"算法-排序-基于比较的排序","date":"2020-02-12T09:53:18.000Z","updated":"2020-02-27T05:05:46.055Z","comments":true,"path":"2020/02/12/算法-排序-基于比较的排序/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F-%E5%9F%BA%E4%BA%8E%E6%AF%94%E8%BE%83%E7%9A%84%E6%8E%92%E5%BA%8F/","excerpt":"基于比较的算法 复杂度O(N²)我们讨论三种基于比较的算法： 冒泡 选择 插入 因为这些算法需要比较两个元素以决定是否交换位置，故称基于比较的算法。 这三种算法最容易实现，但不是最高效的。其时间复杂度是O(N²)。","text":"基于比较的算法 复杂度O(N²)我们讨论三种基于比较的算法： 冒泡 选择 插入 因为这些算法需要比较两个元素以决定是否交换位置，故称基于比较的算法。 这三种算法最容易实现，但不是最高效的。其时间复杂度是O(N²)。 冒泡算法给定数组N个元素，冒泡算法会： 比较一对邻近的元素(a,b)。 当这对元素无序时，交换他们。（此是当a&gt;b时）。 重复步骤1，步骤2直到数组尾。 此时，最大元素会是最后一个元素。然后令N=N-1，并重复步骤1直到N=1。 [29, 10, 14, 37, 14]，冒泡的动画示意如下： 冒泡排序的分析比较和交换需要常量时间，我们取c。 标准冒泡算法中有两层循环。外层循环需要迭代N次。但内层循环迭代的次数会越来越少： 当i=0, N-1次迭代 - 比较和(可能的)交换 当i=1，N-2次迭代 当i=2，N-3次迭代… 当N=N-2，1次迭代 当N=N-1，0次迭代 因此总共需要的迭代次数= (N-1) + (N-2) + ... + 1 + 0 = N*(N-1)/2。 总共花费的时间 = c * N*(N-1)/2 = O(N²)。 12345678910111213141516func BUB(a []int) &#123; N := len(a) for i := 0; i &lt; N; i++ &#123; allSorted := true for j := 0; j &lt; N-i-1; j++ &#123; if a[j] &gt; a[j+1] &#123; a[j],a[j+1] = a[j+1],a[j] allSorted = false &#125; &#125; if allSorted &#123; return &#125; &#125;&#125; 冒泡改进 - 提前终止冒泡排序并不高效，因为它是O(N²)的时间复杂度。 但若能提前终止(多余的)冒泡进程，时间复杂度可以提高到O(1)。 例：[3, 6, 11, 25, 39]。改进方法很简单：若内层循环没有发生交换，那么整个序列已经是正序的，此时可以终止冒泡进程。 讨论：这个改进让冒泡排序在一般情况下更快了，但无法改变其时间复杂度O(N²)的时间，为啥？ 选择排序令数组大小为N，L=0，选择排序步骤如下： 找到元素[L .. N-1]中最小元素X的位置。 将X元素和位置L的元素交换。 L++，并重复步骤1。 123456789101112131415161718func SEL(a []int) &#123; N := len(a) for L := 0; L &lt; N-1; L++ &#123; X := minElements(a, L) a[X],a[L] = a[L],a[X] &#125;&#125;func minElements(a []int, offset int) int &#123; min := offset for i:=offset;i&lt;len(a);i++&#123; if a[i] &lt; a[min] &#123; min = i &#125; &#125; return min&#125; 当然，也可以更改算法为找最大数并交换。 时间复杂度和冒泡是一样的O(N²)。 插入排序插入排序有点像给手上的扑克牌排序的过程。 手上拿第一张牌。 拿到下一张牌，在目前有序的牌中找到正确的位置并插入新牌。 重复上述步骤直到所有牌结束。 1234567891011func INS(a []int) &#123; N := len(a) for i := 1; i &lt; N; i++ &#123; for j := i; j &gt; 0; j-- &#123; if a[j] &lt; a[j-1] &#123; a[j],a[j-1] = a[j-1],a[j] &#125; &#125; &#125;&#125; 123456789101112func INS(a []int) &#123; N := len(a) for i := 1; i &lt; N; i++ &#123; // O(N) x := a[i] // 带插入的牌 j := i - 1 // 正序部分的最高索引 for ; j &gt;= 0 &amp;&amp; a[j] &gt; x; j-- &#123; //最好O(1)，最坏O(N) a[j+1] = a[j] // 给x腾出空间 &#125; a[j+1] = x // j+1是要插入的位置 &#125;&#125; 分析插入排序外层循坏需要N-1次迭代。 但内层循环的次数： 最好的情况下，只需要一次比较a[j] &gt; x就能确定是正序的，因此是O(1)。 最坏的情况下，数组是逆序的，这样每次迭代a[j] &gt; x都是true，每次都需要迭代整个内层循环，时间复杂度是O(N)。 因此，最好情况是O(1)，最坏情况是O(N²)。 基于比较的 O(N log N) 的排序 归并排序 快速排序 和 随机快速排序 这些算法通常使用递归实现，采用分治思想， 归并排序和随机快速排序都是O(N log N)。 快速排序的非随机版本是O(N²)的。 归并排序令数组大小为N，归并排序步骤如下： 将两个元素合并到一个正序的（仅含这两个元素）数组中。 将两个上述数组合并到一个正序的（现在含有四个元素）数组中。 最终：将两个各含有N/2的数组合并到一个数组中，则数组正序。 这仅是一个概要，我们还需要讨论更多细节才能应用归并排序 归并排序中复杂度O(N)的子步骤我们将归并排序拆开讨论，先讨论复杂度为O(N)的子步骤。 令数组A大小为N1，数组B大小为N2，我们很容易将之合并为一个正序的大小N=N1+N2的数组 只需要比较数组的第一个元素，将较小的数组元素全部放到另一个数组之前。这个O(N)的操作需要额外的数组来存储数据。 1234567891011void merge(int a[], int low, int mid, int high) &#123; // subarray1 = a[low..mid], subarray2 = a[mid+1..high], both sorted int N = high-low+1; int b[N]; // discuss: why do we need a temporary array b? int left = low, right = mid+1, bIdx = 0; while (left &lt;= mid &amp;&amp; right &lt;= high) // the merging b[bIdx++] = (a[left] &lt;= a[right]) ? a[left++] : a[right++]; while (left &lt;= mid) b[bIdx++] = a[left++]; // leftover, if any while (right &lt;= high) b[bIdx++] = a[right++]; // leftover, if any for (int k = 0; k &lt; N; k++) a[low+k] = b[k]; // copy back&#125; Golang实现： 12345678910111213141516171819202122232425262728293031func merge(a []int, low, mid, high int) &#123; N := high - low + 1 left := low right := mid + 1 idx := 0 b := make([]int, N, N) for ; left &lt;= mid &amp;&amp; right &lt;= high; &#123; if a[left] &lt;= a[right] &#123; b[idx] = a[left] left = left + 1 &#125; else &#123; b[idx] = a[right] right = right + 1 &#125; idx = idx + 1 &#125; if left &lt;= mid &#123; b[idx] = a[left] left = left + 1 idx = idx + 1 &#125; if right &lt;= high &#123; b[idx] = a[right] right = right + 1 idx = idx + 1 &#125; for i := 0; i &lt; N; i++ &#123; a[low+i] = b[i] &#125;&#125; 分治思想分治法 - Divide and Conquer简写作D&amp;C。 Divide step：将一个问题分解成小的问题，然后递归的解决小的问题。 Conquer step：将解决了的小问题合并起来，还原为原始问题的解。 归并算法的分治思想归并算法是分治的一个应用。 divide step：将数组分为两半，然后递归的对两个数组分派归并排序。 conquer step：将结果集合并成解，这一步骤的工作量最大。 123456789void mergeSort(int a[], int low, int high) &#123; // the array to be sorted is a[low..high] if (low &lt; high) &#123; // base case: low &gt;= high (0 or 1 item) int mid = (low+high) / 2; mergeSort(a, low , mid ); // divide into two halves mergeSort(a, mid+1, high); // then recursively sort them merge(a, low, mid, high); // conquer: the merge subroutine &#125;&#125; Golang实现： 12345678func MER(a []int, low, high int) &#123; if low &lt; high &#123; mid := (low + high) / 2 MER(a, low, mid) MER(a, mid+1, high) merge(a, low, mid, high) &#125;&#125; 归并排序时间复杂度分析归并排序中merge的复杂度是O(N)的，那么merge执行的次数决定了最终的时间复杂度。 可以看到k=lg N，忽略常数，我们记归并排序的时间复杂度为O(N log N)。 归并排序的优缺点首先，最重要的一点是，归并排序的时间复杂度稳定的是O(N log N)，不受数据源的影响。没有任何的测试用例，不论数据源的大小，都不能是归并排序的时间超过O(N log N)。 目前为止，我们讨论过的算法中，归并排序是最适合处理大规模数据的算法。 归并排序是稳定排序算法，为啥？ 当然，归并排序也有其劣势。 从头开始实现归并排序并不是很容易。 在合并阶段，需要额外的存储空间。因此内存利用率不高，也不是原地排序算法。 对归并排序的优化你可以参考这里 快速排序算法快速排序也是分治法的应用。 Divide step：选择一个元素p，即pivot。将数组a[i..j]分为三个部分：a[i..m-1], a[m], a[m=1..j]。a[i..m-1]包含比p小的元素。a[m]即pivot，m是p在数据源a中的索引。a[m+1..j]包含了大于等于p的元素。然后递归的重复上述步骤。 Conquer step：额，啥也不用做… 与归并排序对比，你会发现它的D&amp;C步骤刚好相反。 分析快速排序还是查分开讨论，先讨论复杂度为O(N)的子步骤。 为了分割a[i..j]，我们先找到一个pivotp = a[i]。剩下的a[i+1..j]将被分割为三个部分： S1=a[i+1..m]，元素都 &lt; P。 S2=a[m+1..k-1]，元素都 ≥ p。 未知部分 = a[k..j]，其他元素待分配到S1 或 S2。 讨论：为什么选择p=a[i]，有没有其他选择？硬核讨论：若每次都将==p的元素放在S2里合适吗？ 起初，S1和S2都是空的，所有的元素除了p=pivot都在未知区域中。对于每一个在位置区域的元素a[k]，将a[k]与p比较： a[k] ≥ p，将a[k]放入S2。 a[k] &lt; p，将a[k]放入S1。最后交换a[i]和a[m]，即将p放在S1和S2的中间。代码如下：12345678910111213141516171819202122int partition(int a[], int i, int j) &#123; int p = a[i]; // p is the pivot int m = i; // S1 and S2 are initially empty for (int k = i+1; k &lt;= j; k++) &#123; // explore the unknown region if (a[k] &lt; p) &#123; // case 2 m++; swap(a[k], a[m]); // C++ STL algorithm std::swap &#125; // notice that we do nothing in case 1: a[k] &gt;= p &#125; swap(a[i], a[m]); // final step, swap pivot with a[m] return m; // return the index of pivot&#125;void quickSort(int a[], int low, int high) &#123; if (low &lt; high) &#123; int m = partition(a, low, high); // O(N) // a[low..high] ~&gt; a[low..m–1], pivot, a[m+1..high] quickSort(a, low, m-1); // recursively sort left subarray // a[m] = pivot is already sorted after partition quickSort(a, m+1, high); // then sort right subarray &#125;&#125; Golang实现： 1234567891011121314151617181920func QUI(a []int, low, high int) &#123; if low &lt; high &#123; pivot := partition(a, low, high) QUI(a, low, pivot-1) QUI(a, pivot+1, high) &#125;&#125;func partition(a []int, i, j int) int &#123; p := a[i] m := i for k := i + 1; k &lt;= j; k++ &#123; if a[k] &lt; p &#123; m++ a[k],a[m] = a[m],a[k] &#125; &#125; a[i],a[m] = a[m],a[i] return m&#125; 快速排序的时间复杂度分析partition操作只有一个循环，复杂度是O(N)。 那么快速排序的时间复杂度取决于partition执行的次数。 最坏情况下，当数据源是升序的，经过一轮后，S1是空的，S2包含除了pivot外其他元素。一共需要N轮循环，所以时间复杂度是O(N²)。 而最好情况下，每一次pivot都将数据源分割为两半，一共需要log N次。因此复杂度是O(N log N)。 随机快速排序 - O(N log N)It will take about 1 hour lecture to properly explain why this randomized version of Quick Sort has expected time complexity of O(N log N) on any input array of N elements. In this e-Lecture, we will assume that it is true. If you need non formal explanation: Just imagine that on randomized version of Quick Sort that randomizes the pivot selection, we will not always get extremely bad split of 0 (empty), 1 (pivot), and N-1 other items. This combination of lucky (half-pivot-half), somewhat lucky, somewhat unlucky, and extremely unlucky (empty, pivot, the rest) yields an average time complexity of O(N log N). Discussion: Actually the phrase “any input array” above is not fully true. There is actually a way to make the randomized version of Quick Sort as currently presented in this VisuAlgo page still runs in O(N2). How?","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"算法_排序_开篇","slug":"算法-排序-开篇","date":"2020-02-12T08:27:36.000Z","updated":"2020-02-27T05:05:49.759Z","comments":true,"path":"2020/02/12/算法-排序-开篇/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F-%E5%BC%80%E7%AF%87/","excerpt":"排序问题和排序算法排序是一个经典的重排元素顺序的问题（其元素可以是整型、浮点型、字符串等可比较的类型）。可以重排为递增序列、非递减序列1，递减序列，非递增序列2。 排序算法非常多，各有优势与其限制。 排序通常是大学计算机学科中，介绍算法思想的入门问题。 不失一般性的，假定在本教学中我们只以非递减顺序排序整型，元素无需排重。","text":"排序问题和排序算法排序是一个经典的重排元素顺序的问题（其元素可以是整型、浮点型、字符串等可比较的类型）。可以重排为递增序列、非递减序列1，递减序列，非递增序列2。 排序算法非常多，各有优势与其限制。 排序通常是大学计算机学科中，介绍算法思想的入门问题。 不失一般性的，假定在本教学中我们只以非递减顺序排序整型，元素无需排重。 冒泡排序的演示 思想排序问题有大量的算法方案，都提现了计算机思想： 比较和非比较的底层策略。 迭代和递归的实现。 分治思想 最好/最差/平均时间复杂度分析。 随机算法 应用当数组A（整型数组）被排序后，许多问题都变得很容易： 在A中查找值v。 在（静态）数组A中查找最大/最小，第k大/小值。 测试唯一性并删除A中重复的值。 查找A中值v的个数。 计算A与另一个已排序数组B的交集/并集。 在A中找到一对数(x,y)，x∈A且y∈A，满足 x*y=z，等等。 算法的缩写 基于比较的算法 BUB - Bubble Sort 冒泡排序 SEL - Selection Sort 选择排序 INS - Insertion Sort 插入排序 MER - Merge Sort(递归实现) 归并排序 QUI - Quick Sort(递归实现) 快速排序 R-Q - Random Quick Sort(递归实现) 随机快速排序 非比较排序 COU - Counting Sort 计数排序 RAD - Radix Sort 基数排序 —OU名词解释 非递减序列 非递增序列12341,2,3,4,5 递增序列 - 无重复元素1,2,2,3,4,4,5,5 非递减序列 - 有重复元素5,4,3,2,1 递减序列 - 无重复元素5,4,4,3,2,2,1 非递增序列 - 有重复元素","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_队列_总结","slug":"数据结构-队列-总结","date":"2020-02-12T06:57:40.000Z","updated":"2020-02-27T05:05:21.378Z","comments":true,"path":"2020/02/12/数据结构-队列-总结/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97-%E6%80%BB%E7%BB%93/","excerpt":"①链表 ②栈 ③队列 ④双向链表 ⑤双端队列 这物种结构的插入操作都是相同的，而查找/插入/移除有些微差别。 栈，只能从头/栈顶执行peek/push/pop操作，都是严格受限的。 队列，只能从队首peek，从队尾push，从队首pop。 双端队列，只能从两端peek/enqueue/dequeue，而不能从中间操作。 顺序链表和双向链表没有上述限制。","text":"①链表 ②栈 ③队列 ④双向链表 ⑤双端队列 这物种结构的插入操作都是相同的，而查找/插入/移除有些微差别。 栈，只能从头/栈顶执行peek/push/pop操作，都是严格受限的。 队列，只能从队首peek，从队尾push，从队首pop。 双端队列，只能从两端peek/enqueue/dequeue，而不能从中间操作。 顺序链表和双向链表没有上述限制。 讨论下面对链表做一些深入的讨论： 若不存储尾指针 若使用dummy head 若tail元素指向head元素 标准实现todo… 在线练习在线练习UVa 11988 - Broken Keyboard (a.k.a. Beiju Text)Kattis - backspaceKattis - integerlists","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_双端队列","slug":"数据结构-双端队列","date":"2020-02-12T06:47:10.000Z","updated":"2020-02-27T05:05:11.038Z","comments":true,"path":"2020/02/12/数据结构-双端队列/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8F%8C%E7%AB%AF%E9%98%9F%E5%88%97/","excerpt":"双端队列——Double-ended queue缩写deque，读作deck。其元素只能两端添加或删除，即只能从头部/尾部添加或删除。","text":"双端队列——Double-ended queue缩写deque，读作deck。其元素只能两端添加或删除，即只能从头部/尾部添加或删除。 在本教学中，Deque基本上算是一个受限的双向链表，只能做如下操作： 查找头/尾元素(peek front/back)。 插入头部/尾部 从头部/尾部移除元素 所有操作都是O(1)。 应用Deque有一些高级应用，如finding the shortest paths 0/1-weighted graph using modified BFS, on some sliding window techniques, etc","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_双向链表","slug":"数据结构-双向链表","date":"2020-02-12T05:02:58.000Z","updated":"2020-02-27T05:05:07.210Z","comments":true,"path":"2020/02/12/数据结构-双向链表/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/","excerpt":"双向链表和顺序链表99%都是相同的。主要的区别是：每个节点包含两个指针，next指针指向后一个元素（若存在）即aj指向aj+1，另一个prev指针指向前一个元素，即aj指向aj-1（若存在）。 prev指针是的双向链表能够反向遍历，但同时也增加了内存的消耗。它解决了顺序链表中移除尾元素的低效问题。","text":"双向链表和顺序链表99%都是相同的。主要的区别是：每个节点包含两个指针，next指针指向后一个元素（若存在）即aj指向aj+1，另一个prev指针指向前一个元素，即aj指向aj-1（若存在）。 prev指针是的双向链表能够反向遍历，但同时也增加了内存的消耗。它解决了顺序链表中移除尾元素的低效问题。 注意：在本教学中，双向链表（以及之后的双端队列）的边界是无方向的。 Remove(i) - At Tail(i = N-1) 复习顺序链表最大的问题是尾元素的移除，即时我们有指向尾元素的指针，因为我们需要将tail指针指向尾元素的前一个。 有了双向链表的反向能力，我可以直接找到前一个元素： 1234Vertex* temp = tail; // remember tail itemtail = tail-&gt;prev; // the key step to achieve O(1) performance :Otail-&gt;next = null; // remove this dangling referencedelete temp; // remove the old tail example DLL [22 (head)&lt;-&gt;2&lt;-&gt;77&lt;-&gt;6&lt;-&gt;43&lt;-&gt;76&lt;-&gt;89 (tail)] 练习 insertHead(50) - insertTail(10) insert(3,4) removeHead() remove(5)","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_队列","slug":"数据结构-队列","date":"2020-02-12T04:00:47.000Z","updated":"2020-02-27T05:05:17.894Z","comments":true,"path":"2020/02/12/数据结构-队列/","link":"","permalink":"https://straysh.github.life/2020/02/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97/","excerpt":"队列这种ADT中，元素是有序的，主要操作有enqueue入队和dequeue出队。 即Fist-In-First-Out(FIFO)。最下进入队列的元素，会最后出队列。","text":"队列这种ADT中，元素是有序的，主要操作有enqueue入队和dequeue出队。 即Fist-In-First-Out(FIFO)。最下进入队列的元素，会最后出队列。 数组实现队列问题 （一）若使用紧凑数组来实现队列，a0表示第一个元素,aN-1表示最后一个元素，那么在执行dequeue出队操作是会有严重的性能问题。 因为对数组而言，在尾部添加元素是O(1)，但在头部移除元素是O(N)（需要向逐个左移动剩余所有元素）。 数组实现队列问题 （二）另种数组实现队列的方式，记录两个索引：front记录最左元素索引，当dequeue出队时递增，和back记录最右元素索引，当enqueue入队时递增。 假设数组容量是8，当前如下：[2,4,1,7,-,-,-,-]，其front=0，back=3。 若执行dequeue()，则变为[-,4,1,7,-,-,-,-]，其中front=1，back=3。 若再执行enqueue(5)，则变为[-,4,1,7,5,-,-,-]，其中front=1，back=4。 数组实现队列问题 （三）多次执行出入队之后，我们得到[-,-,-,-,-,6,2,3]，其front=5,back=7。尽管左端有许多空穴，但我们已经不能再执行入队操作了。 若我们允许front和back移动到M-1时绕回到索引0，这种高效循环数组又可以利用剩余的空间了。 例： enqueue → [8 , -,-,-,-,6,2,3] front=5,back=0 数组实现队列问题 （四）Yet, this does not solve the main problem of array implementation: The items of the array are stored in contiguous manner in computer memory. 假设，经过若干操作我们得到[8,10,11,12,13,6,2,3]，front=5，back=4。此时无法再执行入队操作。 那么，我们会生成一个更大的数组（扩容），使M=2*8=16，但由于要拷贝旧数组到新数组，其时间复杂度是O(N)。[6,2,3,8,10,11,12,13,-,-,-,-,-,-,-,-,]，front = 0, and back = 7 链表的实现在队列中，我们只需要两个参数，一个用作插入（入队），一个用作移除（出队） 插入尾部之后和从头部移除在顺序链表中非常快，是O(1)。我们使用链表中的head/tail来替换front/back，由于链表中的元素不需要连续存储它的大小可以随时增大缩小。 队列的应用队列能解决很多实际问题。 在宽度优先搜索中有非常重要的应用。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_栈","slug":"数据结构-栈","date":"2020-02-11T12:00:36.000Z","updated":"2020-02-27T05:05:14.430Z","comments":true,"path":"2020/02/11/数据结构-栈/","link":"","permalink":"https://straysh.github.life/2020/02/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/","excerpt":"Stack栈是一种数据结构，它要求只能从顶部增加元素即push，只能从顶部移除元素即pop。亦即Last-In-First-out(LIFO)。","text":"Stack栈是一种数据结构，它要求只能从顶部增加元素即push，只能从顶部移除元素即pop。亦即Last-In-First-out(LIFO)。 The Design Choice 栈的设计宗旨在多数实现中，包括本教学中，栈是使用受保护的链表（连续的）来实现的，我们只能在顶部访问元素，push元素到顶部（头部插入），从头部pop元素出来（头部移除）。所有操作都是O(1)的。 讨论：能否使用变长数组来实现高效的栈结构？ 栈的应用教科书上，栈有一些典型的应用： Bracket Matching 括号匹配。 Postfix Calculator 后缀计算器？ A few other interesting applications that are not shown for pedagogical purposes. Bracket Matching 括号匹配问题数学表达式可以很复杂，例如：{[x+2]^(2+5)-2}*(y+5)。 括号匹配问题是检查表达式中的括号是不是成对的，(和)，[和]，&#123;和&#125;。 Golang实现： 123456789101112131415161718192021222324func checkBracketMatching(input string) bool &#123; stack := NewStack() for i := 0; i &lt; len(input); i++ &#123; c := fmt.Sprintf(&quot;%c&quot;, input[i]) switch c &#123; case &quot;(&quot;, &quot;[&quot;, &quot;&#123;&quot;: stack.Push(c) case &quot;)&quot;: if stack.Pop() != &quot;(&quot; &#123; return false &#125; case &quot;]&quot;: if stack.Pop() != &quot;[&quot; &#123; return false &#125; case &quot;&#125;&quot;: if stack.Pop() != &quot;&#123;&quot; &#123; return false &#125; &#125; &#125; return stack.size == 0&#125; Calculating Postfix Expression 后缀表达式的计算后缀表达式用数学语言描述格式如下：data1 data2 op，与之对应的是更易理解中缀表达式：data1 op data2。 例：表达式2 3 + 4 * = (2+3) * 4。 在后缀表达式中，我们不需要括号辅助。 Golang实现： 123456789101112131415161718192021222324252627282930313233func postfixExpression() &#123; input := &quot;2 3 + 4 *&quot; stack := NewStack() for i := len(input) - 1; i &gt;= 0; i-- &#123; c := fmt.Sprintf(&quot;%c&quot;, input[i]) if c != &quot; &quot; &#123; stack.Push(c) &#125; &#125; fmt.Println(stack) for ; stack.size &gt; 1; &#123; data1 := stack.Pop() data2 := stack.Pop() d1, _ := strconv.Atoi(data1) d2, _ := strconv.Atoi(data2) op := stack.Pop() temp := 0 if op == &quot;+&quot; &#123; temp = d1 + d2 &#125; else if op == &quot;-&quot; &#123; temp = d1 - d2 &#125; else if op == &quot;*&quot; &#123; temp = d1 * d2 &#125; else if op == &quot;/&quot; &#123; temp = d1 / d2 &#125; stack.Push(strconv.Itoa(temp)) &#125; fmt.Printf(&quot;result:%s&quot;, stack.Pop())&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"数据结构_链表","slug":"数据结构-链表","date":"2020-02-10T14:13:00.000Z","updated":"2020-02-27T05:04:44.642Z","comments":true,"path":"2020/02/10/数据结构-链表/","link":"","permalink":"https://straysh.github.life/2020/02/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%93%BE%E8%A1%A8/","excerpt":"LL,Stack,Queue,DLL,Deque链表（Linked List）是一种数据结构，它由一组节点组成，它们作为一个整体表示一个序列。最简单的情形下，每一个节点由一个值data和指向下序列中一节点的引用link组成。 在链表中查找数77 链表及其变体是用来实现列表（List）、栈（Stack）、Queue（队列）、Deque ADTs1（双端队列）的底层数据结构。Linked List and its variations are used as underlying data structure to implement List, Stack, Queue, and Deque ADTs 下面我们会讨论链表（只包含一个next指针）和它的两个变体：Stack和Queue，还会讨论Doubly Linked List（DLL）（双向链表）（包含next指针和previous指针），及其变体：Deque。 动机链表这个数据结构通常会在大学本科阶段学习，有以下几个原因： 它是一个简单的线性数据结构。 作为一个抽象数据类型的列表，它有一系列的应用，例如学生列表，事件列表，任职列表等等（尽管有其他更多高级数据结构能做到同样(甚至更好)）或者作为栈/队列/双端队列的抽象数据类型。 它有一些有趣的边界用例，指出了数据结构的良好实现的必要。 它包含多种自定义的选项，因此特别适合使用OOP Programming面向对象编程来实现。","text":"LL,Stack,Queue,DLL,Deque链表（Linked List）是一种数据结构，它由一组节点组成，它们作为一个整体表示一个序列。最简单的情形下，每一个节点由一个值data和指向下序列中一节点的引用link组成。 在链表中查找数77 链表及其变体是用来实现列表（List）、栈（Stack）、Queue（队列）、Deque ADTs1（双端队列）的底层数据结构。Linked List and its variations are used as underlying data structure to implement List, Stack, Queue, and Deque ADTs 下面我们会讨论链表（只包含一个next指针）和它的两个变体：Stack和Queue，还会讨论Doubly Linked List（DLL）（双向链表）（包含next指针和previous指针），及其变体：Deque。 动机链表这个数据结构通常会在大学本科阶段学习，有以下几个原因： 它是一个简单的线性数据结构。 作为一个抽象数据类型的列表，它有一系列的应用，例如学生列表，事件列表，任职列表等等（尽管有其他更多高级数据结构能做到同样(甚至更好)）或者作为栈/队列/双端队列的抽象数据类型。 它有一些有趣的边界用例，指出了数据结构的良好实现的必要。 它包含多种自定义的选项，因此特别适合使用OOP Programming面向对象编程来实现。 List ADTList（列表）是一个有序数据序列 {a0, a1, …, aN-2, aN-1}通常的List ADT（列表抽象数据类型）的操作有：0. get(i) - 或许是个简单的操作，返回 ai（基于0的索引）。 search(v) - 判断值v在列表中存在(并返回其位置/索引) 或不存在（通常返回索引-1） insert(i,v) - 在列表的位置/索引i处插入值v，即将从位置[i+1 .. N-1]的元素均向右移动一个位置来生成一个空穴存放值v。 remove(i) - 将列表中位置i的值移除。即将位置[i+1 .. N-1]的元素均向左移动一个位置来消除空穴。 讨论：若我们需要在列表中移除值v呢？ 数组的实现 - Part 1（紧凑的）Array（数组）是实现List ADT的一个良好候选方式，因为它是处理数据集合的简单结构。 当我们说紧凑数组，我们指的是没有空穴的数组，举例：假设数组中有N个元素（数组的容量是M， M ≥ N），那么只有位置[0,N-1]被占用，其他位置[N, M-1]都是空的。 数组的实现 - Part 2紧凑数组命名为A，下标范围[0 .. N-1]填充了数据。 get(i)返回元素A[i]。当数组是非紧凑的，这个简单的操作也会无必要的复杂。(This simple operation will be unnecessarily complicated if the array is not compact.) search(v)我们逐项的遍历索引i（i∈[0 .. N-1]），检查A[i]==v。因为若值v存在，那么它的索引必将落在[0 .. N-1]中。 insert(i,v)我们将元素∈[i .. N-1]集合，顺序移动到∈[i+1 .. N]（向右移动）并设置A[i] = v。既要将v插入正确的索引i，还要确保数组是紧凑的。 remove(i)我们将元素∈[i+1 .. N-1]集合，顺序移动到∈[i .. N-2]（向左移动）将旧的A[i]覆盖掉。即能保持紧凑。 Time Complexity Summary 时间复杂性概要 get(i)非常快： 只需要一次查询，是O(1)。 search(v) 最好的情况，值v是第一个元素，即O(1)。 最坏的情况，值v不存在，我们需要O(N)复杂度的扫描才能确定这个实事。 insert(i,v) 最好的情况，i == N，不需要移动元素，即O(1)。 最坏的情况，i == 0，需要移动N个元素，即O(N)。 remove(i) 最好的情况，i == N-1，不需要移动元素，即O(1)。 最坏的情况，i == 0，需要移动N个元素，即O(N)。 Fixed Space Issue 固定空间问题紧凑数组的容量M不是无限的，它是一个有限的数值。那么这就有一个问题，在许多的应用中无法提前预知最大的容量需要多大。 若M过大，则未使用的空间浪费了。 若M过小，则没有足够的空间来存储（数组会越界）。 Variable Space 变长空间解决方案：让M动态可变。那么当数组满了，我们生成一个更大的新数组（通常是2倍的原容量）并将旧数据移动到新数组中。那么，容量将不再成为限制，或者说只受限于计算机的内存容量（通常都很大）。However, the classic array-based issues of space wastage and copying/shifting items overhead are still problematic.然而，经典数组问题诸如空间浪费，复制、移动元素仍然是课题。（？没译好） Observations 思考对于已知可能最大元素数量的固定大小M的集合，数组已经是基于List ADT实现的合理的最佳数据结构。 对于容量为M的动态集合，由于频繁的插入、删除操作，简单的数组是一个糟糕的数据结构。where dynamic operations such as insert/remove are common, a simple array is actually a poor choice of data structure. Linked List 链表现在我们介绍链表这个数据结构。它使用指针来允许元素在内存中非连续存储（这是与数组的本质区别）。元素的索引从0到N-1，使用元素i的指针来访问下一个元素i+1。 链表的C++实现其基本形式，链表的一个单一节点包含如下结构： 1234struct Vertex &#123; // we can use either C struct or C++/Java class int item; // the data is stored here, an integer in this example Vertex* next; // this pointer tells us where is the next vertex&#125;; Golang的实现 1234type Node struct &#123; Item int next *Node&#125; Linked List,Additional Data链表这个数据结构中，还有几个额外的数据。我们使用默认的链表[22 (head)-&gt;2-&gt;77-&gt;6-&gt;43-&gt;76-&gt;89 (tail)]来说明。 头节点指针指向a0 - 它的值是22，没有谁指向头结点的。 尾节点指针指向aN-1 - 它是a6 = 89，尾节点之后没有其他元素。 就这些，我们仅仅增加了两个额外的变量。 Variations 变化注意在大学课本中对如何实现链表有各种细微的区别。如是否使用尾节点，是否使用循环结构，是否使用dummy head元素。discuss here 这里，我们实现的教学版本（有尾节点非循环结构，没有dummy head）可能与你所学的不是100%相同，但其核心思想是一样的。 在本教学中，每一个节点包含整型值item，但很容易更换为其他类型。 Get(i) - Much Slower than Array 比数组慢很多由于我们保存了头尾节点的指针，除了头（索引0）尾（索引N-1）节点，其他节点需要列表顺序遍历来查找。 我们看一个简单的C++实现： 123456Vertex* Get(int i) &#123; // returns the vertex Vertex* ptr = head; // we have to start from head for (int k = 0; k &lt; i; k++) // advance forward i time(s) ptr = ptr-&gt;next; // the pointers are pointing to the higher index return ptr;&#125; 当i≤N-2时，其时间复杂度是O(N)。与数组的O(1)做一个比较。 Golang实现 1234567func (ll *LL) Get(i int) *Node &#123; node := ll.head for k:=0;k&lt;i;k++ &#123; node = node.next &#125; return node&#125; Search(v) - Not Better than Array 不比数组优由于我们只有头尾节点的引用，另指针总是指向右侧（高位），我们只能从头结点开始，通过next指针顺序遍历。 例： [22 (head)-&gt;2-&gt;77-&gt;6-&gt;43-&gt;76-&gt;89 (tail) 在索引2处找到元素77 在列表中未找到，然而这只有在遍历完N个节点后才能确定，因此它是最坏的时间复杂度O(N)。 Insertion - Four Cases 插入的四种情况链表的插入比数组有更多的边界条件。 大多数同学只有在它们写的链表代码失败时才能意识到所有的边界条件。 在这里，我们直接给出所有的边界。 对于插入insert(i,v)，有四种边界情况： 插入链表头，i == 0。 空链表（和上面的1相同）。 插入尾节点之后。i == N。 其他位置 i∈[1 .. N-1]。 Insert(i,v) - 头部插入(i==0)头部插入简单高效，O(1)C++实现如下： 1234Vertex* vtx = new Vertex(); // create new vertex vtx from item vvtx-&gt;item = v;vtx-&gt;next = head; // link this new vertex to the (old) head vertexhead = vtx; // the new vertex becomes the new head insert(0, 50)的动画： Golang实现： 1234node := &amp;Node&#123;&#125;node.Item = vnode.next = ll.headll.head = node 讨论：在数组的头部插入会发生啥？ Insert(i,v) - 插入一个空列表空结构是一个常见的边界条件，若无何时的测试它常常导致不可预期的崩溃。向空结构中插入新元素是合法的。幸运的是，实现的伪代码和向i==0插入是一样的，因此直接使用上面的代码即可。 Insert(i,v) - i∈[1 .. N-1]结合Get(i)的代码，我们可以实现向列表中间插入元素： 123456Vertex* pre = Get(i-1); // traverse to (i-1)-th vertex, O(N)aft = pre-&gt;next; // aft cannot be null, think about itVertex* vtx = new Vertex(); // create new vertexvtx-&gt;item = v;vtx-&gt;next = aft; // link thispre-&gt;next = vtx; // and this Golang实现： 123456pre := ll.Get(i-1)aft := pre.nextnode := &amp;Node&#123;&#125;node.Item = vnode.next = aftpre.next = node 常规插入： 边界条件，向尾节点插入 因需要遍历列表，该操作的时间复杂度是O(N)。 Insert(i,v) - Beyond the Tail,i == N 插入尾节点之后由于尾节点的存在，插入尾节点之后也是O(1)。 1234Vertex* vtx = new Vertex(); // this is also a C++ codevtx-&gt;item = v; // create new vertex vtx from item vtail-&gt;next = vtx; // just link this, as tail is the i = (N-1)-th itemtail = vtx; // now update the tail pointer Golang实现: 1234node := &amp;Node&#123;&#125;node.Item = vtail.next = nodetail = node 讨论：在数组的尾部之后插入会发生啥？ Removal - Three Cases 移除的三种条件移除操作remove(i)，根据i的值有三种可能性： i == 0，移除头元素，只影响head指针。 i == N-1，移除尾元素，只影响tail指针。 i ∈ [1 .. N-2]，其他位置。 讨论：对比上面的插入场景。从空结构中移除操作是否合法？ Remove(i) - At Head(i == 0) 移除头元素非常简单，直接上代码： 1234if (head == NULL) return; // avoid crashing when SLL is emptyVertex* temp = head; // so we can delete it laterhead = head-&gt;next; // book keeping, update the head pointerdelete temp; // which is the old head Golang实现： 12345678if ll.head == nil &#123; return nil&#125;head := ll.head.nextll.head = headreturn head 讨论：对于数组，移除头元素会发生啥？ Remove(i) - i ∈ [1 .. N-2]借用Get(i)的代码，很容易实现： 1234Vertex* pre = Get(i-1); // traverse to (i-1)-th vertex, O(N)Vertex* del = pre-&gt;next, aft = del-&gt;next;pre-&gt;next = aft; // bypass deldelete del; Golang实现： 12345prev := ll.Get(i-1)elem := prev.nextprev.next = elem.nextreturn elem 由于需要遍历列表，其时间复杂度O(N)。 Remove(i) - At Tail(i == N-1) - Part 1 移除尾元素(1)假设列表非空，我们可以如下实现： 1234567Vertex* pre = head;temp = head-&gt;next;while (temp-&gt;next != null) // while my neighbor is not the tail pre = pre-&gt;next, temp = temp-&gt;next;pre-&gt;next = null; // alternatively: pre = Get(N-2), temp = Get(N-1)delete temp; // temp = (old) tailtail = pre; // update tail pointer Golang实现： 123456789101112pre := ll.headtemp := pre.nextfor ;temp.next != nil; &#123; pre = temp temp = temp.next&#125;ll.head = prepre.next = nilreturn temp 循环调用上述步骤，我们将从尾部逐项移除元素，直到只剩下一个元素（head == tail），此时我们再执行移除头元素的方法。注：从空结构中移除是非法的，所以当链表欧为空时，停止。 Remove(i) - At Tail(i == N-1) - Part 2 移除尾元素(2)实际上，若我们维护了一个变量N来记录链表的大小，我们可以通过顺序遍历Get(i)来实现移除尾元素： 1234Vertex* pre = Get(N-2); // go to one index just before tail, O(N)pre-&gt;next = null;delete tail;tail = pre; // we have access to old tail Golang实现： 1234567prev := ll.Get(N-2)temp := prev.nextll.tail = prevprev.next = nilreturn temp 注意这个操作的时间复杂度是O(N)，为了插入到尾元素之后能正常进行，需要将tail指针从N-1移动到N-2。这种不高效的做法在后面Doubly Linked List（双向链表）中解决。 讨论：对数组移除尾元素会发生啥？ 时间复杂度概览 get(i)很慢，是O(N)。在链表中，我们需要从头元素开始顺序遍历 search(v) 最好情况下，命中头元素，O(1)。 最坏情况下，未找到，需要顺利遍历整个列表，是O(N)。 insert(i,v) 最好情况下，插入头部，O(1)。或插入尾元素之后（有tail指针）也是O(1)。 最坏情况下，插入N-2位置，我们需要顺序遍历找到元素N-2（tail的前一个元素），是O(N)。 remove(i) 最好情况下，移除头元素，O(1)。 最坏情况下，移除尾元素，为了更新tail指针需要顺序遍历整个链表，是O(N)。 链表的应用场景单纯的线性表（顺序表）很少有应用场景，因为紧凑数组（可变长的）能做得更好。 但，链表的一个基础特性是允许元素（在内存中）非连续存储。因此是Stack栈和Queue队列的完美基础。 名词解释： ADT","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"log15_Readme[译]","slug":"log15-Readme-译","date":"2020-02-10T07:41:45.000Z","updated":"2020-02-10T10:01:21.410Z","comments":true,"path":"2020/02/10/log15-Readme-译/","link":"","permalink":"https://straysh.github.life/2020/02/10/log15-Readme-%E8%AF%91/","excerpt":"log15包提供了一种固执且简洁的最佳实践工具包(both human and machine readable)。它是仿造标准库io和net/http构建的。 这个包强制仅记录key/value对。键必须是字符串。值可以是任何类型。默认的输出格式是logfmt，也可以选择使用JSON格式。例如:log.Info(&quot;page accessed&quot;， &quot;path&quot;， r.URL。Path， &quot;user_id&quot;， user.id)输出一行记录如下:lvl=info t=2014-05-02T16:07:23-0700 msg=&quot;page accessed&quot; path=/org/71/profile user_id=9","text":"log15包提供了一种固执且简洁的最佳实践工具包(both human and machine readable)。它是仿造标准库io和net/http构建的。 这个包强制仅记录key/value对。键必须是字符串。值可以是任何类型。默认的输出格式是logfmt，也可以选择使用JSON格式。例如:log.Info(&quot;page accessed&quot;， &quot;path&quot;， r.URL。Path， &quot;user_id&quot;， user.id)输出一行记录如下:lvl=info t=2014-05-02T16:07:23-0700 msg=&quot;page accessed&quot; path=/org/71/profile user_id=9 快速入门首先需要引入库文件：import log &quot;github.com/inconshreveable/log15&quot;然后可以开始使用了： 123func main() &#123; log.Info(&quot;Program starting&quot;, &quot;args&quot;, os.Args)&#125; 惯例记录对人有意义的信息是常见且良好的实践，传递给日志函数的第一个参数隐式的对应键msg。 另外，消息的level值会自动使用键lvl，而当前时间戳会使用键t。 可以使用任何额外的键值对作为上下文提供给日志函数。log15 allows you to favor terseness， ordering， and speed over safety。这种折中对日志函数是合理的。不需要显示的指明键值对，log15理解它们在变长参数列表中的含义:log.Warn(&quot;size out of bounds&quot;， &quot;low&quot;， lowBound， &quot;high&quot;， highBound， &quot;val&quot;， val)若你非常关注类型的安全性，你可以传入一个log.Ctx:log.Warn(&quot;size out of bounds&quot;， log.Ctx&#123;&quot;low&quot;: lowBound， &quot;high&quot;: highBound， &quot;val&quot;: val&#125;) 上下文logger常常需要添加一个上下文到日志中以便跟踪其操作。例如网络请求。能够很容易的创建一个携带上下文的日志，输出的每行日志都将自动包含该上下文信息： 1234requestlogger := log.New(&quot;path&quot;, r.URL.Path)// laterrequestlogger.Debug(&quot;db txn commit&quot;, &quot;duration&quot;, txnTimer.Finish()) 该日志输出如下：lvl=dbug t=2014-05-02T16:07:23-0700 path=/repo/12/add_hook msg=&quot;db txn commit&quot; duration=0.12 HandlersHandler接口定义的日志输出到哪，以及如何格式化。Handler接口受net/http句柄接口启发： 123type Handler interface &#123; Log(r *Record) error&#125; Handlers会过滤并格式化日志记录行，或转发到其他Handlers。该包ethereum/go-ethereum/log实现了几个常用的日志模式，用以创建可扩展、自定义的日志结构。 以下例子展示了打印logfmt格式到标准输出的句柄:handler := log.StreamHandler(os.Stdout, log.LogfmtFormat()) 这是一个指向其他两个句柄的句柄。一个句柄仅仅将rpc包的记录以logfmt格式打印到标准输出上。另一个以JSON格式打印Error级别（及以上）记录到文件/var/log/service.json： 1234handler := log.MultiHandler( log.LvlFilterHandler(log.LvlError, log.Must.FileHandler(&quot;/var/log/service.json&quot;, log.JSONFormat())), log.MatchFilterHandler(&quot;pkg&quot;, &quot;app/rpc&quot; log.StdoutHandler())) 记录文件名和行号该包（ethereum/go-ethereum/log）实现了三种句柄，用以记录debugging信息到上下文。CallerFileHandler，CallerFuncHandler，CallerStackHandler。下例记录了文件和行号： 1234h := log.CallerFileHandler(log.StdoutHandler)log.Root().SetHandler(h)...log.Error(&quot;open file&quot;, &quot;err&quot;, err) 输出如下：lvl=eror t=2014-05-02T16:07:23-0700 msg=&quot;open file&quot; err=&quot;file not found&quot; caller=data.go:42 下例记录了调用栈信息： 1234h := log.CallerStackHandler(&quot;%+v&quot;, log.StdoutHandler)log.Root().SetHandler(h)...log.Error(&quot;open file&quot;, &quot;err&quot;, err) 输出如下：lvl=eror t=2014-05-02T16:07:23-0700 msg=&quot;open file&quot; err=&quot;file not found&quot; stack=&quot;[pkg/data.go:42 pkg/cmd/main.go]&quot;%+v参数指示句柄记录源文件相对编译时GOPATH的路径。详细参考github.com/go-stack/stack包的实现。 自定义HandlersHnalder句柄如此的简单，通常不需要自定义句柄。接下里，我们创建一个写入句柄A的句柄，但它写入失败时，会携带写入失败的错误信息重新写入另一个句柄B。在依靠网络socket记录日志，但失败时希望将之写入磁盘时这很有用。 12345678910111213type BackupHandler struct &#123; Primary Handler Secondary Handler&#125;func (h *BackupHandler) Log (r *Record) error &#123; err := h.Primary.Log(r) if err != nil &#123; r.Ctx = append(ctx, &quot;primary_err&quot;, err) return h.Secondary.Log(r) &#125; return nil&#125; 该模式非常有用，因此该包实现了一个经典的版本FailoverHandler。 记录重开销的操作有时，需要记录一些需要非常重的计算才能得到的值，但当你的日志级别不够时，你不希望做这个计算。 该包提供了一个简单的模式来标记一个希望惰性计算的操作，仅在当它即将被写入时计算，因此在其他上级句柄中不会触发计算。你需要使用log.Lazy包装一个无参的函数。例如： 12345func factorRSAKey() (factors []int) &#123; // return the factors of a very large number&#125;log.Debug(&quot;factors&quot;, log.Lazy&#123;factorRSAKey&#125;) 若该信息未被打印（如错误的日志级别），factorRSAKey不会被计算。 动态的context值log.Lazy模式也可用于上下文参数中，假设一个游戏中含有Player对象： 12345type Player struct &#123; name string alive bool log.Logger&#125; 你总希望打印出玩家的名字自己它是否活着，因此你会这样创建player对象： 12p := &amp;Player&#123;name: &quot;straysh&quot;, alive: true&#125;p.Logger = log.New(&quot;name&quot;, p.name, &quot;alive&quot;, p.alive) 此时，即时玩家已经死了，日志句柄仍然会打印活着，因为日志上下文是在创建时被初始化的。使用log.Lazy包装，我们可以defer计算玩家是否活着的函数，因此日志行会反射出玩家当前的状态而无论何时调用日志函数。 123p := &amp;Player&#123;name: &quot;straysh&quot;, alive: true&#125;isAlive := func bool &#123;return p.alive&#125;player.Logger = log.New(&quot;name&quot;, p.name, &quot;alive&quot;: log.Lazy(isAlive)) 终端格式若log15检测到输出是终端，会配置默认句柄（即log.StdoutHanlder）并使用TerminalFormat格式。该格式更好的适配了终端输出，包括不同级别的颜色。 Error Handling因为log15允许你绕过类型系统，有几种方法可以使你指定无效参数到日志函数中。例如，给无参函数log.Lazy传一个参数，或给键传一个非字符串的值。由于日志库是一个典型的报告错误的技术，日志函数返回错误将会非常麻烦。相反，log15在处理错误时保证： 任何包含error的日志行，在打印正常日志信息时，也会打印出错误信息 任何包含error的日志行，会包含一个键LOG15_ERROR，可以方便的检测是否传递了一个无效的值。 理解了这一点，你可能会有这样的疑惑：为什么Handler接口在它的Log函数里会返回一个error。Handlers只有在尝试写入外部源失败时，被鼓励返回errors，如syslog无响应。这样能允许诸如FailoverHandler在这些失败后能协作。 作为library使用log15更适合被包装成自己的日志库。其最佳实践是默认关闭所有的输出方式，而使用一个公用用的Logger实例，并提供配置方法。如： 123456789package yourlibimport &quot;github.com/inconshreveable/log15&quot;var Log = log.New()func init()&#123; Log.SetHanlder(log.DiscardHandler())&#125; 库的使用者，可能这样使用它： 1234567import &quot;github.com/inconshreveable/log15&quot;import &quot;example.com/yourlib&quot;func main()&#123; handler := // custom handler setup yourlib.Log.Sethandler(handler)&#125; 绑定上下文的最佳实践假设我在开发一个浏览器： 12345678910111213141516type Tab struct &#123; url string render *RenderingContext // ... Logger&#125;func NewTab(url string) *Tab &#123; return &amp;Tab &#123; // ... url: url, Logger: log.New(&quot;url&quot;, url), &#125;&#125; 当常见一个tab页时，我将url作为上下文绑定到logger上，这样可以很容易在日志中跟踪它。此时，不管我对该tab进行何操作，都会使用合成的logger自动记录tab标题：tab.Debug(&quot;moved position&quot;, &quot;idx&quot;, tab.idx) 这有一个问题，若tab的url值改变了？我们可以使用log.Lazy确保总是记录了当前的url，但如此就无法记录该tab完整的生命周期。 我们可以使用一个随机的十六进制数来，这叫做surrogate keys: 123456789import logext &quot;github.com/inconshreveable/log15/ext&quot;t := &amp;Tab &#123; // ... url: url,&#125;t.Logger = log.New(&quot;id&quot;, logext.RandId(8), &quot;url&quot;, log.Lazy&#123;t.getUrl&#125;)return t 这样，我们就有了对应该tab的唯一的标识，同时我们记录下了日志函数调用时的url值。 Must有一组Must句柄，它们在发生错误时并不返回错误信息而是直接panic。例如： 12log.Must.FileHandler(&quot;/path&quot;, log.JSONFormat)log.Must.NetHandler(&quot;tcp&quot;, &quot;:1234&quot;, log.JSONFormat)","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"},{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"gorm FAQs","slug":"2019-gorm-FAQs","date":"2019-12-04T15:21:59.000Z","updated":"2019-12-05T04:49:30.854Z","comments":true,"path":"2019/12/04/2019-gorm-FAQs/","link":"","permalink":"https://straysh.github.life/2019/12/04/2019-gorm-FAQs/","excerpt":"","text":"BeforeUpdate执行成功,但updated_at仍是时间字符串,写入数据库错误12345(/app/Golang/blog/models/tag.go:75) [2019-12-04 23:27:41] Error 1265: Data truncated for column &#x27;updated_at&#x27; at row 1 (/app/Golang/blog/models/tag.go:75) [2019-12-04 23:27:41] [0.39ms] UPDATE `blog_tag` SET `name` = &#x27;杂谈&#x27;, `updated_at` = &#x27;2019-12-04 23:27:41&#x27; WHERE (id=2) 问题的根源在GoPath/pkg/mod/github.com/jinzhu/gorm@v1.9.11/callback_update.go的ini函数中:gorm:update_time_stamp 在 gorm:before_update 后执行了. 123456789DefaultCallback.Update().Register(&quot;gorm:assign_updating_attributes&quot;, assignUpdatingAttributesCallback)DefaultCallback.Update().Register(&quot;gorm:begin_transaction&quot;, beginTransactionCallback)DefaultCallback.Update().Register(&quot;gorm:before_update&quot;, beforeUpdateCallback)DefaultCallback.Update().Register(&quot;gorm:save_before_associations&quot;, saveBeforeAssociationsCallback)DefaultCallback.Update().Register(&quot;gorm:update_time_stamp&quot;, updateTimeStampForUpdateCallback)DefaultCallback.Update().Register(&quot;gorm:update&quot;, updateCallback)DefaultCallback.Update().Register(&quot;gorm:save_after_associations&quot;, saveAfterAssociationsCallback)DefaultCallback.Update().Register(&quot;gorm:after_update&quot;, afterUpdateCallback)DefaultCallback.Update().Register(&quot;gorm:commit_or_rollback_transaction&quot;, commitOrRollbackTransactionCallback) BeforeUpdate执行成功并设置updated_at为整形时间戳后, updateTimeStampForUpdateCallback执行又将updated_at设置成了时间字符串 123456// updateTimeStampForUpdateCallback will set `UpdatedAt` when updatingfunc updateTimeStampForUpdateCallback(scope *Scope) &#123; if _, ok := scope.Get(&quot;gorm:update_column&quot;); !ok &#123; scope.SetColumn(&quot;UpdatedAt&quot;, scope.db.nowFunc()) &#125;&#125; 而created_at因为先判断了非空if createdAtField.IsBlank而没有这个bug: 123456789101112131415161718// updateTimeStampForCreateCallback will set `CreatedAt`, `UpdatedAt` when creatingfunc updateTimeStampForCreateCallback(scope *Scope) &#123; if !scope.HasError() &#123; now := scope.db.nowFunc() if createdAtField, ok := scope.FieldByName(&quot;CreatedAt&quot;); ok &#123; if createdAtField.IsBlank &#123; createdAtField.Set(now) &#125; &#125; if updatedAtField, ok := scope.FieldByName(&quot;UpdatedAt&quot;); ok &#123; if updatedAtField.IsBlank &#123; updatedAtField.Set(now) &#125; &#125; &#125;&#125; 解决方案重新注册gorm:update_time_stamp事件,重写updateTimeStampForUpdateCallback方法: 1234567891011...db.Callback().Update().Replace(&quot;gorm:update_time_stamp&quot;, updateTimeStampForUpdateCallback);...// updateTimeStampForUpdateCallback will set `UpdatedAt` when updatingfunc updateTimeStampForUpdateCallback(scope *gorm.Scope) &#123; if _, ok := scope.Get(&quot;gorm:update_column&quot;); !ok &#123; _ = scope.SetColumn(&quot;UpdatedAt&quot;, time.Now().Unix()) &#125;&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"Hello World","slug":"2019-hello-world","date":"2019-12-04T15:21:59.000Z","updated":"2020-02-26T04:22:39.318Z","comments":true,"path":"2019/12/04/2019-hello-world/","link":"","permalink":"https://straysh.github.life/2019/12/04/2019-hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[]},{"title":"php_framework_of_filter_01","slug":"2019-php-framework-of-filter-01","date":"2019-04-20T03:47:37.000Z","updated":"2019-12-01T10:13:39.824Z","comments":true,"path":"2019/04/20/2019-php-framework-of-filter-01/","link":"","permalink":"https://straysh.github.life/2019/04/20/2019-php-framework-of-filter-01/","excerpt":"","text":"思考$_GET/$_POST/$_SERVER与INPUT_GET/INPUT_POST/INPUT_SERVER的区别 php中获取输入参数通常使用$_GET/$_POST/$_SERVER等, 它们是超全局变量, 在任意代码中可以直接访问.详细参考php.net #Note: This is a ‘superglobal’, or automatic global, variable. This simply means that it is available in all scopes throughout a script. There is no need to do global $variable; to access it within functions or methods. 从php5.2起支持filter系列过滤器函数, 配合INPUT_GET/INPUT_POST/INPUT_SERVER等很方便的做参数接收和过滤/校验. 简而言之,$_XXX系列可以任意使用, 而INPUT_XXX系列只能配合filter_XXX函数使用. filter.default = full_special_chars 1ini_set(&quot;filter.default&quot;, &quot;full_special_chars&quot;); 实际测试发现上述代码并未生效. 只能修改php.ini启用. 参考php.net, 启用该项配置后, $_GET/$_POST会自动套用该规则来过滤输入参数. 而想要获取原始输入值, 必须使用INPUT_XXX系列函数. 启用 php.ini filter.default = full_special_chars$_POST[‘name’] //output: “&lt;”filter_input(INPUT_POST, ‘name’) //output: “&lt;”filter_input(INPUT_POST, ‘name’, FILTER_UNSAFE_RAW) //output: “&lt;”filter_input(INPUT_POST, ‘name’, FILTER_SANITIZE_STRING) //output: “” P.S.: 个人更倾向于$_XX来获取参数并配合filtre_var做过滤， 但这个方案确实可以减少菜鸟滥用$_GET/$_POST的危害","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"wangyuechejianguan_jieru","slug":"2019-wangyuechejianguan-jieru","date":"2019-04-09T07:19:35.000Z","updated":"2020-03-10T15:36:30.110Z","comments":true,"path":"2019/04/09/2019-wangyuechejianguan-jieru/","link":"","permalink":"https://straysh.github.life/2019/04/09/2019-wangyuechejianguan-jieru/","excerpt":"","text":"网络预约出租汽车监管信息交互平台总体技术要求4.6 网约车平台公司运价信息接口123456789101112131415161718192021222324252627282930create table if not exists `companry_fare`( `company_fare_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;车辆所在城市（注册地行政区划）&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码,由网约车平台公司统一编码,应确保唯一性&#x27;, `fare_type_note` varchar(128) comment &#x27;运价类型说明&#x27;, `fare_valid_on` char(14) comment &#x27;运价有效期起,YYYYMMDDhhmmss&#x27;, `fare_valid_off` char(14) comment &#x27;运价有效期止&#x27;, `start_fare` varchar(10) comment &#x27;起步价,元&#x27;, `start_mile` varchar(10) comment &#x27;起步里程,km&#x27;, `unit_price_per_mile` varchar(10) comment &#x27;计程单价(按公里),元&#x27;, `unit_price_per_minute` varchar(10) comment &#x27;计程单价(按分钟),元&#x27;, `up_price` varchar(10) comment &#x27;单程加价单价,元&#x27;, `up_price_start_mile` varchar(10) comment &#x27;单程加价公里,km&#x27;, `morning_peak_time_on` varchar(8) comment &#x27;营运早高峰时间起,HHmm(24时)&#x27;, `morning_peak_time_off` varchar(8) comment &#x27;营运早高峰时间止,HHmm(24时)&#x27;, `evening_peak_time_on` varchar(8) comment &#x27;营运晚高峰时间起,HHmm(24时)&#x27;, `evening_peak_time_off` varchar(8) comment &#x27;营运晚高峰时间止,HHmm(24时)&#x27;, `other_peak_time_on` varchar(8) comment &#x27;其他营运高峰时间起,HHmm(24时)&#x27;, `other_peak_time_off` varchar(8) comment &#x27;其他营运高峰时间止,HHmm(24时)&#x27;, `peak_unit_price` varchar(10) comment &#x27;高峰时间单程加价单价,元&#x27;, `peak_price_start_mile` varchar(10) comment &#x27;高峰时间单程加价公里,km&#x27;, `low_speed_price_per_minute` varchar(10) comment &#x27;低速计时加价(按分钟)&#x27;, `night_price_per_mile` varchar(10) comment &#x27;夜间费(按公里)&#x27;, `night_price_per_minute` varchar(10) comment &#x27;夜间费(按分钟)&#x27;, `other_price` varchar(10) comment &#x27;其他加价金额&#x27;, `state` char(1) comment &#x27;状态,0:有效,1:无效&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;网约车平台公司运价信息&#x27;; 4.7 车辆基本信息接口123456789101112131415161718192021222324252627282930313233343536373839create table if not exists `vehicle` ( `vehicle_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;车辆所在城市（注册地行政区划）&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `plate_color` varchar(32) comment &#x27;车牌颜色,见JT/T 697.7-2014中5.6&#x27;, `seats` varchar(10) not null comment &#x27;核定载客位&#x27;, `brand` varchar(64) comment &#x27;车辆厂牌&#x27;, `model` varchar(64) comment &#x27;车辆型号&#x27;, `vehicle_type` varchar(64) comment &#x27;车辆类型（以行驶证为准）&#x27;, `owner_name` varchar(64) comment &#x27;车辆所有人&#x27;, `vehicle_color` varchar(32) comment &#x27;车身颜色&#x27;, `engine_id` varchar(32) comment &#x27;发动机号&#x27;, `vin` char(17) comment &#x27;车辆VIN码&#x27;, `certify_date_a` char(8) comment &#x27;车辆注册日期&#x27;, `fuel_type` varchar(32) comment &#x27;车辆燃料类型,见JT/T697.7-2014中4.1.4.15&#x27;, `engine_displace` varchar(32) comment &#x27;发动机排量,毫升&#x27;, `photo_id` varchar(18) comment &#x27;车辆照片文件编号,本字段传输照片文件编号,照片文件通过6.1节FTPS接口传输;格式jpg;按照车辆行驶证照片的标准&#x27;, `certificate` varchar(64) comment &#x27;运输证字号,见JT/T 415-2006中5.4.1,地市字别可包含三个汉子&#x27;, `trans_agency` varchar(256) comment &#x27;车辆运输证发证机构(全称)&#x27;, `trans_area` varchar(256) comment &#x27;车辆经营区域&#x27;, `trans_date_start` char(8) comment &#x27;车辆运输证有效期起,YYYYMMDD&#x27;, `trans_date_stop` char(8) comment &#x27;车辆运输证有效期止,YYYYMMDD&#x27;, `certify_date_b` char(8) comment &#x27;车辆初次登记日期,YYYYMMDD&#x27;, `fix_state` varchar(64) comment &#x27;车辆检修状态,0:未检修,1:已检修,2:未知&#x27;, `next_fix_date` char(8) comment &#x27;车辆下次年检时间,YYYYMMDD&#x27;, `check_state` char(2) comment &#x27;车辆年度审验状态,见JT/T 415-2006中5.4.4&#x27;, `fee_print_id` varchar(32) comment &#x27;发票打印设备序列号&#x27;, `gps_brand` varchar(256) comment &#x27;卫星定位装置品牌&#x27;, `gps_model` varchar(64) comment &#x27;卫星定位装置型号&#x27;, `gps_imei` varchar(128) comment &#x27;卫星定位装置IMEI号&#x27;, `gps_install_date` char(8) comment &#x27;卫星定位装置安装日期,YYYYMMDD&#x27;, `register_date` char(8) comment &#x27;车辆信息向服务所在地出租汽车行政主管部门报备日期,YYYYMMDD&#x27;, `commercial_type` char(1) comment &#x27;服务类型,1:网络预约出租车,2:巡游出租车,3:私人小客车合乘&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码&#x27;, `state` char(1) comment &#x27;状态,0:有效,1:无效&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;车辆基本信息&#x27;; 4.8 车辆保险信息接口12345678910111213create table if not exists `vehicle_insurance`( `vehicle_insurance_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `insur_com` varchar(64) comment &#x27;保险公司名称&#x27;, `insur_num` varchar(64) comment &#x27;保险号&#x27;, `insur_type` varchar(32) comment &#x27;保险类型&#x27;, `insur_count` varchar(10) comment &#x27;保险金额,元&#x27;, `insur_eff` char(8) comment &#x27;保险生效时间,YYYYMMDD&#x27;, `insur_exp` char(8) comment &#x27;保险到期时间,YYYYMMDD&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;车辆保险信息&#x27;; 4.9 网约车车辆里程信息接口123456789create table if not exists `vehicle_total_mile`( `vehicle_total_mile_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `total_mile` varchar(64) comment &#x27;行驶总里程,km&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;网约车车辆里程信息&#x27;; 4.10 驾驶员基本信息接口1234567891011121314151617181920212223242526272829303132333435363738394041424344create table if not exists `driver`( `driver_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划(驾驶员所在平台)&#x27;, `driver_name` varchar(64) comment &#x27;机动车驾驶员姓名&#x27;, &#x27;driver_phone&#x27; varchar(32) comment &#x27;驾驶员手机号&#x27;, `driver_gender` varchar(2) comment &#x27;驾驶员性别,见JT/T69.7-2014中4.1.2.1.3&#x27;, `driver_birthday` char(8) comment &#x27;驾驶员出生日期,YYYYMMDD&#x27;, `driver_nationality` varchar(32) comment &#x27;国籍&#x27;, `driver_nation` varchar(32) comment &#x27;驾驶员民族,见JT/T697.7-2014中4.1.2.1.7&#x27;, `driver_marital_status` varchar(64) comment &#x27;驾驶员婚姻状况,未婚;已婚;离异&#x27;, `driver_language_level` varchar(64) comment &#x27;驾驶员外语能力&#x27;, `driver_education` varchar(64) comment &#x27;驾驶员学历,见JT/T697.7-2014中4.1.2.1.11&#x27;, `driver_census` varchar(256) comment &#x27;户口登记机关名称&#x27;, `driver_address` varchar(256) comment &#x27;户口住址或长住地址&#x27;, `driver_contact_address` varchar(256) comment &#x27;驾驶员通信地址&#x27;, `photo_id` varchar(128) comment &#x27;驾驶员照片文件编号&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `license_photo_id` varchar(128) comment &#x27;机动车驾驶证扫描件编号&#x27;, `driver_type` varchar(16) comment &#x27;准架车型,见JT/T697.7-2014中5.16&#x27;, `get_driver_license_date` char(8) comment &#x27;初次领取驾驶证日期,YYYYMMDD&#x27;, `driver_license_on` char(8) comment &#x27;驾驶证有效期限起,YYYYMMDD&#x27;, `driver_license_off` char(8) comment &#x27;驾驶证有效期限止,YYYYMMDD&#x27;, `taxi_driver` char(1) comment &#x27;是否巡游出租车驾驶员,1:是,0:否&#x27;, `certificate_no` varchar(128) comment &#x27;网络预约出租车驾驶员资格证号&#x27;, `network_car_issue_organization` varchar(256) comment &#x27;网络预约出租车驾驶员证发证机构(全称)&#x27;, `network_car_issue_date` char(8) comment &#x27;资格证发证日期,YYYYMMDD&#x27;, `get_network_car_proof_date` char(8) comment &#x27;初次领取资格证日期,YYYYMMDD&#x27;, `network_car_proof_on` char(8) comment &#x27;资格证有效期起,YYYYMMDD&#x27;, `network_car_proof_off` char(8) comment &#x27;资格证有效期止,YYYYMMDD&#x27;, `register_date` char(8) comment &#x27;报备日期,YYYYMMDD&#x27;, `full_time_driver` char(1) comment &#x27;是否专职驾驶员,1:是,0:否&#x27;, `in_driver_blacklist` char(1) comment &#x27;是否在驾驶员黑名单内,1:是,0:否&#x27;, `commercial_type` char(1) comment &#x27;服务类型,1:网络预约出租车,2:巡游出租车,3:私人小客车合乘&#x27;, `contract_company` varchar(256) comment &#x27;驾驶员合同(或协议)签署公司&#x27;, `contract_on` char(8) comment &#x27;合同(或协议)有效期起,YYYYMMDD&#x27;, `contract_off` char(8) comment &#x27;合同(或协议)有效期止,YYYYMMDD&#x27;, `emergency_contact` varchar(64) comment &#x27;紧急情况联系人&#x27;, `emergency_contact_phone` varchar(32) comment &#x27;紧急情况联系人电话&#x27;, `emergency_contact_address` varchar(256) comment &#x27;紧急情况联系人通信地址&#x27;, `state` char(1) comment &#x27;状态,0:有效,1:失效&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员基本信息&#x27;; 4.11 网约车驾驶员培训信息接口12345678910111213create table if not exists `driver_educate`( `driver_educate_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `course_name` varchar(64) comment &#x27;驾驶员培训课程名称&#x27;, `course_date` char(8) comment &#x27;培训课程日期&#x27;, `start_time` char(8) comment &#x27;培训开始时间&#x27;, `stop_time` char(8) comment &#x27;培训结束时间&#x27;, `duration` varchar(10) comment &#x27;培训时长&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;网约车驾驶员培训信息&#x27;; 4.12 驾驶员移动终端信息接口12345678910111213create table if not exists `driver_app`( `driver_app_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, &#x27;driver_phone&#x27; varchar(32) comment &#x27;驾驶员手机号&#x27;, `net_type` char(1) comment &#x27;手机运营商,1:联通,2:移动,3:电信,4:其他&#x27;, `app_version` varchar(32) comment &#x27;使用app版本号&#x27;, `map_type` char(1) comment &#x27;使用地图类型,1:百度地图,2:高德地图,3:其他&#x27;, `state` char(1) comment &#x27;状态,0:有效,1:失效&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员移动终端&#x27;; 4.13 驾驶员统计信息接口123456789101112create table if not exists `driver_stat`( `driver_app_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `cycle` char(6) comment &#x27;统计周期&#x27;, `order_count` varchar(10) comment &#x27;完成订单次数&#x27;, `traffic_violation_count` varchar(32) comment &#x27;交通违章次数&#x27;, `complained_count` varchar(32) comment &#x27;被投诉次数&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员统计信息&#x27;; 4.14 乘客基本信息接口1234567891011create table if not exists `passenger`( `passenger_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `register_date` char(8) comment &#x27;注册日期&#x27;, `passenger_phone` varchar(32) comment &#x27;乘客手机号&#x27;, `passenger_name` varchar(64) comment &#x27;乘客称谓&#x27;, `passenger_gender` varchar(2) comment &#x27;乘客性别&#x27;, `state` char(1) comment &#x27;状态,0:有效,1:失效&#x27;, `flag` char(1) comment &#x27;操作标识符,1:新增,2:更新,3:删除&#x27;, `update_time` char(14) comment &#x27;更新日期,YYYYMMDDhhmmss&#x27;)engine=innodb default charset utf8 comment &#x27;乘客基本信息&#x27;; 5 订单信息交换接口5.1 订单发起接口1234567891011121314151617create table if not exists `order_create`( `order_create_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `depart_time` char(14) comment &#x27;预计用车时间,YYYYMMDDhhmmss&#x27;, `order_time` char(14) comment &#x27;订单发起时间,YYYYMMDDhhmmss&#x27;, `passenger_note` varchar(28) comment &#x27;乘客备注&#x27;, `departure` varchar(128) comment &#x27;预计出发地点详细地址&#x27;, `dep_longitude` varchar(10) comment &#x27;预计出发地点经度&#x27;, `dep_latitude` varchar(10) comment &#x27;预计出发地点维度&#x27;, `destination` varchar(128) comment &#x27;预计目的地&#x27;, `dest_longitude` varchar(10) comment &#x27;预计目的地经度&#x27;, `dest_latitude` varchar(10) comment &#x27;预计目的地维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识,1:GCJ-02测绘局标准,2:WGS84 GPS标准,3:BD-09百度标准,4:CGCS2000北斗标准,0:其他&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码&#x27;)engine=innodb default charset utf8 comment &#x27;订单发起&#x27;; 5.2 订单成功接口12345678910111213create table if not exists `order_match`( `order_match_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `longitude` varchar(10) comment &#x27;车辆经度&#x27;, `latitude` varchar(10) comment &#x27;车辆维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证编码&#x27;, `driver_phone` varchar(32) comment &#x27;驾驶员手机号&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `distribute_time` char(14) comment &#x27;派单成功时间&#x27;)engine=innodb default charset utf8 comment &#x27;订单成功(匹配)&#x27;; 5.3 订单撤销接口1234567891011create table if not exists `order_cancel`( `order_cancel_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) not null comment &#x27;注册地行政区划&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `order_time` char(14) comment &#x27;订单时间&#x27;, `cancel_time` char(14) comment &#x27;订单撤销时间&#x27;, `operator` varchar(64) comment &#x27;撤销发起方,1:乘客,2:驾驶员,3:平台公司&#x27;, `cancel_type_code` varchar(32) comment &#x27;撤销类型代码,1:乘客提前撤销,2:驾驶员提前撤销,3:平台公司撤销,4:乘客违约撤销,5:驾驶员违约撤销&#x27;, `cancel_reason` varchar(128) comment &#x27;撤销或违约原因&#x27;)engine=innodb default charset utf8 comment &#x27;订单撤销&#x27;; 6 经营信息交换接口6.1 车辆经营上线接口12345678910create table if not exists `operate_login`( `operate_login_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `login_time` char(14) comment &#x27;车辆经营上线时间&#x27;, `longitude` varchar(10) comment &#x27;上线经度&#x27;, `latitude` varchar(10) comment &#x27;上线维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;)engine=innodb default charset utf8 comment &#x27;车辆经营上线&#x27;; 6.2 车辆经营下线接口12345678910create table if not exists `operate_logout`( `operate_logout_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `login_time` char(14) comment &#x27;车辆经营下线时间&#x27;, `longitude` varchar(10) comment &#x27;下线经度&#x27;, `latitude` varchar(10) comment &#x27;下线维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;)engine=innodb default charset utf8 comment &#x27;车辆经营下线&#x27;; 6.3 经营出发接口1234567891011121314create table if not exists `operate_depart`( `operate_depart_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `order_id` varchar(64) comment &#x27;订单号&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `dep_longitude` varchar(10) comment &#x27;出发经度&#x27;, `dep_latitude` varchar(10) comment &#x27;出发维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `dep_time` char(14) comment &#x27;上车时间&#x27;, `wait_mile` varchar(10) comment &#x27;空驶里程,km&#x27;, `wait_time` varchar(10) comment &#x27;等待时间,秒&#x27;)engine=innodb default charset utf8 comment &#x27;车辆经营出发&#x27;; 6.4 经营到达接口1234567891011create table if not exists `operate_arrive`( `operate_arrive` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `order_id` varchar(64) comment &#x27;订单号&#x27;, `dest_longitude` varchar(10) comment &#x27;车辆到达经度&#x27;, `dest_latitude` varchar(10) comment &#x27;车辆到达维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `dest_time` char(14) comment &#x27;上车时间&#x27;, `drive_mile` varchar(10) comment &#x27;载客里程,km&#x27;, `drive_time` varchar(10) comment &#x27;载客时间,秒&#x27;)engine=innodb default charset utf8 comment &#x27;经营到达&#x27;; 6.5 经营支付接口12345678910111213141516171819202122232425262728293031323334353637383940414243create table if not exists `operate_pay`( `operate_pay_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `order_id` varchar(64) comment &#x27;订单号&#x27;, `on_area` char(6) comment &#x27;上车位置行政区划编码&#x27;, `driver_name` varchar(64) comment &#x27;机动车驾驶员姓名&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `book_dep_time` char(14) comment &#x27;预计上车时间&#x27;, `wait_time` varchar(10) comment &#x27;等待时间&#x27;, `dep_longitude` varchar(10) comment &#x27;车辆出发经度&#x27;, `dep_latitude` varchar(10) comment &#x27;车辆出发维度&#x27;, `dep_area` varchar(128) comment &#x27;上车地点&#x27;, `dep_time` char(14) comment &#x27;上车时间&#x27;, `dest_longitude` varchar(10) comment &#x27;车辆到达经度&#x27;, `dest_latitude` varchar(10) comment &#x27;车辆到达维度&#x27;, `dest_area` varchar(128) comment &#x27;下车地点&#x27;, `dest_time` char(14) comment &#x27;下车时间&#x27;, `book_model` varchar(64) comment &#x27;预定车型&#x27;, `model` varchar(64) comment &#x27;实际车型&#x27;, `drive_mile` varchar(10) comment &#x27;载客里程&#x27;, `drive_time` varchar(10) comment &#x27;载客时间&#x27;, `wait_mile` varchar(10) comment &#x27;空载里程&#x27;, `fact_price` varchar(10) comment &#x27;实收金额&#x27;, `price` varchar(10) comment &#x27;应收金额&#x27;, `cash_price` varchar(10) comment &#x27;现金支付金额&#x27;, `line_name` varchar(64) comment &#x27;电子支付机构&#x27;, `line_price` varchar(10) comment &#x27;电子支付金额&#x27;, `pos_name` varchar(64) comment &#x27;POS机支付机构&#x27;, `pos_price` varchar(10) comment &#x27;POS机支付金额&#x27;, `benfit_price` varchar(10) comment &#x27;优惠金额&#x27;, `book_tip` varchar(10) comment &#x27;预约服务费&#x27;, `passenger_tip` varchar(10) comment &#x27;附加费&#x27;, `peak_up_price` varchar(10) comment &#x27;高峰时段时间加价金额&#x27;, `night_up_price` varchar(10) comment &#x27;夜间时段里程加价金额&#x27;, `fare_up_price` varchar(10) comment &#x27;远途加价金额&#x27;, `other_up_price` varchar(10) comment &#x27;其他加价金额&#x27;, `pay_state` varchar(32) comment &#x27;结算状态,0:未结算,1:已结算,2:未知&#x27;, `pay_time` char(14) comment &#x27;乘客结算时间&#x27;, `order_match_time` char(14) comment &#x27;订单完成时间&#x27;, `invoice_status` varchar(32) comment &#x27;发票状态,0:未开票,1:已开票,2:未知&#x27;)engine=innodb default charset utf8 comment &#x27;经营支付&#x27;; 7 定位信息交换接口7.1 驾驶员定位信息接口12345678910111213141516create table if not exists `position_driver`( `position_driver_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `driver_region_code` char(6) comment &#x27;驾驶员报备地行政区划代码&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `position_time` char(14) comment &#x27;定位时间&#x27;, `longitude` varchar(10) comment &#x27;下线经度&#x27;, `latitude` varchar(10) comment &#x27;下线维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `direction` varchar(10) comment &#x27;方向角,0-359,顺时针方向&#x27;, `elevation` varchar(10) comment &#x27;海拔高度&#x27;, `speed` varchar(10) comment &#x27;瞬时速度&#x27;, `biz_status` varchar(10) comment &#x27;营运撞他,1:载客,2:接单,3:空驶,4:停运&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员定位信息&#x27;; 与驾驶员定位信息重复?7.2 车辆定位信息接口123456789101112131415161718create table if not exists `position_vehicle`( `position_vehicle_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `vehicle_region_code` char(6) comment &#x27;车辆报备地行政区划代码&#x27;, `position_time` char(14) comment &#x27;定位时间&#x27;, `longitude` varchar(10) comment &#x27;下线经度&#x27;, `latitude` varchar(10) comment &#x27;下线维度&#x27;, `speed` varchar(10) comment &#x27;瞬时速度&#x27;, `direction` varchar(10) comment &#x27;方向角,0-359,顺时针方向&#x27;, `elevation` varchar(10) comment &#x27;海拔高度&#x27;, `mileage` varchar(10) comment &#x27;行驶里程&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `warn_status` varchar(10) comment &#x27;预警状态,参考JT/T808&#x27;, `veh_status` varchar(10) comment &#x27;车辆状态,参考JT/T808&#x27;, `biz_status` varchar(10) comment &#x27;营运撞他,1:载客,2:接单,3:空驶,4:停运&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;)engine=innodb default charset utf8 comment &#x27;车辆定位信息&#x27;; 8 服务质量信息交换接口8.1 乘客评价信息接口12345678910create table if not exists `rated_passenger`( `rated_passenger_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `evaluate_time` char(14) comment &#x27;评价时间&#x27;, `service_score` varchar(10) comment &#x27;服务满意度,五分制&#x27;, `driver_score` varchar(10) comment &#x27;驾驶员满意度,五分制&#x27;, `vehicle_score` varchar(10) comment &#x27;车辆满意度,五分制&#x27;, `detail` varchar(128) comment &#x27;评价内容&#x27;)engine=innodb default charset utf8 comment &#x27;乘客评价信息&#x27;; 8.2 乘客投诉信息接口12345678create table if not exists `rated_passenger_complaint`( `rated_passenger_complaint_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `complaint_time` char(14) comment &#x27;投诉时间&#x27;, `detail` varchar(256) comment &#x27;投诉内容&#x27;, `result` varchar(128) comment &#x27;处理结果&#x27;)engine=innodb default charset utf8 comment &#x27;乘客投诉信息&#x27;; 8.3 驾驶员处罚信息接口12345678create table if not exists `rated_driver_punish`( `rated_driver_punish_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `punish_time` char(14) comment &#x27;处罚时间&#x27;, `punish_reason` varchar(128) comment &#x27;处罚原因&#x27;, `punish_result` varchar(128) comment &#x27;处罚结果&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员处罚信息&#x27;; 8.4 驾驶员信誉信息接口12345678create table if not exists `rated_driver`( `rated_driver_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `level` varchar(10) comment &#x27;机动车驾驶证编号&#x27;, `test_date` char(8) comment &#x27;服务质量信誉考核日期,YYYYMMDD&#x27;, `test_department` varchar(128) comment &#x27;服务质量荣誉考核机构&#x27;)engine=innodb default charset utf8 comment &#x27;驾驶员信誉信息&#x27;; 9 私人小客车合乘信息交换接口9.1 私人小客车合乘信息服务平台进本信息接口9.2 私人小客车合乘驾驶员行程发布接口1234567891011121314151617181920create table if not exists `share_route`( `share_route_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) comment &#x27;行政区划代码&#x27;, `route_id` varchar(64) comment &#x27;驾驶员发起行程编号&#x27;, `driver_name` varchar(64) comment &#x27;驾驶员姓名&#x27;, `driver_phone` varchar(32) comment &#x27;驾驶员手机号&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `departure` varchar(128) comment &#x27;行程出发地点&#x27;, `dep_longitude` varchar(10) comment &#x27;车辆出发经度&#x27;, `dep_latitude` varchar(10) comment &#x27;车辆出发维度&#x27;, `destination` varchar(128) comment &#x27;行程到达地点&#x27;, `dest_longitude` varchar(10) comment &#x27;到达地经度&#x27;, `dest_latitude` varchar(10) comment &#x27;到达地维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `route_create_time` char(14) comment &#x27;行程发布时间&#x27;, `route_mile` varchar(10) comment &#x27;行程预计里程&#x27;, `route_note` varchar(256) comment &#x27;行程备注&#x27;)engine=innodb default charset utf8 comment &#x27;私人小客车合乘驾驶员行程发布&#x27;; 9.3 私人小客车合乘订单接口123456789101112131415161718create table if not exists `share_order`( `share_order_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) comment &#x27;行政区划代码&#x27;, `route_id` varchar(64) comment &#x27;驾驶员发起行程编号&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `book_depart_time` char(14) comment &#x27;预计上车时间&#x27;, `departure` varchar(128) comment &#x27;预计上车地点&#x27;, `dep_longitude` varchar(10) comment &#x27;预计上车经度&#x27;, `dep_latitude` varchar(10) comment &#x27;预计上车维度&#x27;, `destination` varchar(128) comment &#x27;预计下车地点&#x27;, `dest_longitude` varchar(10) comment &#x27;预计下车地经度&#x27;, `dest_latitude` varchar(10) comment &#x27;预计下车地维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `order_ensure_time` char(14) comment &#x27;订单确认时间&#x27;, `passenger_num` varchar(10) comment &#x27;乘客人数&#x27;, `passenger_note` varchar(256) comment &#x27;乘客备注&#x27;)engine=innodb default charset utf8 comment &#x27;私人小客车合乘订单&#x27;; 9.4 私人小客车合乘订单支付接口12345678910111213141516171819202122232425262728293031323334353637create table if not exists `share_pay`( `share_pay_id` bigint unsigned not null auto_increment primary key, `company_id` varchar(32) comment &#x27;公司标识&#x27;, `address` char(6) comment &#x27;行政区划代码&#x27;, `route_id` varchar(64) comment &#x27;驾驶员发起行程编号&#x27;, `order_id` varchar(64) comment &#x27;订单编号&#x27;, `driver_phone` varchar(32) comment &#x27;驾驶员手机号&#x27;, `license_id` varchar(32) comment &#x27;机动车驾驶证号&#x27;, `vehicle_no` varchar(32) comment &#x27;车辆号牌&#x27;, `fare_type` varchar(16) comment &#x27;运价类型编码&#x27;, `book_depart_time` char(14) comment &#x27;预计上车时间&#x27;, `depart_time` char(14) comment &#x27;实际上车时间&#x27;, `departure` varchar(128) comment &#x27;上车地点&#x27;, `dep_longitude` varchar(10) comment &#x27;上车地点经度&#x27;, `dep_latitude` varchar(10) comment &#x27;上车地点维度&#x27;, `dest_time` char(14) comment &#x27;下车时间&#x27;, `destination` varchar(128) comment &#x27;下车地点&#x27;, `dest_longitude` varchar(10) comment &#x27;下车地经度&#x27;, `dest_latitude` varchar(10) comment &#x27;下车地维度&#x27;, `encrypt` char(1) comment &#x27;坐标加密标识&#x27;, `drive_mile` varchar(10) comment &#x27;载客里程&#x27;, `drive_time` varchar(10) comment &#x27;载客时间&#x27;, `fact_price` varchar(10) comment &#x27;实收金额&#x27;, `price` varchar(10) comment &#x27;应收金额&#x27;, `cash_price` varchar(10) comment &#x27;现金支付金额&#x27;, `line_name` varchar(64) comment &#x27;电子支付机构&#x27;, `line_price` varchar(10) comment &#x27;电子支付金额&#x27;, `benfit_price` varchar(10) comment &#x27;优惠金额&#x27;, `share_fuel_fee` varchar(10) comment &#x27;燃料成本分摊金额&#x27;, `share_highway_toll` varchar(10) comment &#x27;路桥通行分摊金额&#x27;, `passenger_tip` varchar(10) comment &#x27;附加费&#x27;, `share_other` varchar(10) comment &#x27;其他费用分摊金额&#x27;, `pay_state` varchar(32) comment &#x27;结算状态&#x27;, `passenger_num` varchar(10) comment &#x27;乘客人数&#x27;, `pay_time` char(14) comment &#x27;乘客结算时间&#x27;, `order_match_time` char(14) comment &#x27;订单完成时间&#x27;)engine=innodb default charset utf8 comment &#x27;私人小客车合乘订单支付&#x27;; 模板1create table if not exists ``()engine=innodb default charset utf8 comment &#x27;&#x27;;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"php_rabbitmq","slug":"2019-php-rabbitmq","date":"2019-04-07T12:49:26.000Z","updated":"2019-12-01T10:13:39.736Z","comments":true,"path":"2019/04/07/2019-php-rabbitmq/","link":"","permalink":"https://straysh.github.life/2019/04/07/2019-php-rabbitmq/","excerpt":"","text":"名词释义 Queue: RabbitMQ管理着多个队列，例如订单队列、邮件队列、延迟任务队列等。 Connection: 客户端程序与RabbitMQ的连接实例 Channel: 客户端向指定的队列发送消息（例如向订单队列插入一条订单），需要通过Channel来发送（不是Connection） Exchange: 当一条消息需要发送到多个队列中时，不可能追条向队列发送。Exchange负责接收一条消息，并按消息头中的RoutingKey/BindingKey向相应的Queue发送消息。 BindingKey: 消息生产者向Exchange发送消息时，header中携带的对着描述 RoutingKey: 消息接收者从Exchange取消息时，header中携带的规则描述。Exchange根据RoutingKey和所有与该Exchange绑定的BindingKey匹配，向满足规则的Queue发送消息，从而实现一条消息发往过个Queue","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"MySQL锁","slug":"2018-MySQL锁","date":"2018-09-18T13:29:44.000Z","updated":"2020-03-10T15:37:04.262Z","comments":true,"path":"2018/09/18/2018-MySQL锁/","link":"","permalink":"https://straysh.github.life/2018/09/18/2018-MySQL%E9%94%81/","excerpt":"Myisam表锁锁模式 共享锁（S锁，亦读锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排它锁。获取共享锁的事务只能读数据，不能写数据。 排它锁（X锁，亦写锁）：如果事务T对数据A加上排它锁后，则其他事务不能再对A加任务类型的锁。获取排它锁的事务既能读数据，也能写数据。 第一行为请求锁模式， 第一列为当前锁模式 None 读锁 写锁 读锁 是 是 否 写锁 是 否 否 当前锁为读锁，请求锁为读锁，正常读取。（图1） 当前锁为读锁，请求锁为写锁，阻塞。（图2） 当前锁为写锁，请求锁为读锁，阻塞。（图3） 当前锁为写锁，请求锁为写锁，阻塞。（图4）","text":"Myisam表锁锁模式 共享锁（S锁，亦读锁）：如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排它锁。获取共享锁的事务只能读数据，不能写数据。 排它锁（X锁，亦写锁）：如果事务T对数据A加上排它锁后，则其他事务不能再对A加任务类型的锁。获取排它锁的事务既能读数据，也能写数据。 第一行为请求锁模式， 第一列为当前锁模式 None 读锁 写锁 读锁 是 是 否 写锁 是 否 否 当前锁为读锁，请求锁为读锁，正常读取。（图1） 当前锁为读锁，请求锁为写锁，阻塞。（图2） 当前锁为写锁，请求锁为读锁，阻塞。（图3） 当前锁为写锁，请求锁为写锁，阻塞。（图4） 图1. 图2. 图3. 图4. 简而言之: Myisam表的读操作，不会阻塞对同一表的读操作，但是会阻塞写操作。 Myisam表的写操作，会阻塞对同一表的读、写操作。 Myisam表的读、写操作之间，以及写操作之间是串行的。 一般，Myisam在执行查询前，会自动执行表的加锁、解锁操作，不需要用户手动加锁、解锁。但下例不同，请思考： 12select sum(t1.score) as &#x27;score&#x27; from t1;select sum(t2.score) as &#x27;score&#x27; from t2; 上面的sql是有问题的。因为读完t1表再读t2表时，t2表的数据可能已经发生的变化，不再是期望的同一时刻的状态，修改为： 1234lock table t1 read, t2 read;select sum(t1.score) as &#x27;score&#x27; from t1;select sum(t2.score) as &#x27;score&#x27; from t2;unlock tables; 并发插入通常Myisam的新数据会插入到数据文件的末尾，但是当做一些upate、delete操作之后，数据文件不再是连续的，数据文件中会有空洞。此时再插入新数据，会先检查这些空洞是否能容纳新数据。如果可以，则插入空洞，否则插入文件末尾。 Myisam里读写是串行的，为了降低锁竞争的频率，需要设置concurrent_insert: concurrent_insert=0，不允许并发插入 concurrent_insert=1，允许对没有空洞的表并发插入，新数据位于末尾。 concurrent_insert=2，不管表有没有空挡，都允许在数据文件末尾插入。 缺省情况下，写操作优先级高于读操作。即使是先发送的读请求，后发送的写请求，此时也会有限处理写。这样，当连续多个写时，所有的读请求会被阻塞。因此： max_write_lock_count=1，当处理完一个写后，暂停写，给读操作流出机会。 low-priority-updates=1，直接降低写操作的优先级，给读操作更高的优先级 Innodb表锁Innodb与Myisam的不同 支持事务 采用行级锁 不支持全文索 Innodb行锁模式以及加锁方式 共享锁（S锁）：允许一个事务去读一行，阻止其他事务获取相同数据集的排它锁。 排它锁（X锁）：允许获得排它锁的事务更新数据，阻止其他事务获得相同数据集的共享读锁和排他写锁。 意向共享锁（IS锁）： 意向排它锁（IX锁）： X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 如果请求锁模式和当前锁模式兼容，则并发；否则阻塞。意向锁是Innodb自动加的，不需要手动操作。对于UPDATE、DELEE、INSERT语句，Innodb会自动加排它锁；对普通的SELECT语句，不加锁。手动显示的使用锁： 共享锁： SELECT * FROM t WHERE … LOCK IN SHARE MODE; 排它锁： SELECT * FROM t WHERE … FOR UPDATE; Innodb行锁实现方式Innodb的行锁是通过给索引加锁实现的。因此，仅当通过索引条件检索，Innodb才使用行锁，否则使用表锁 当一个事务加锁时，另一个事务在同一索引上再加锁则阻塞：","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"Scrapy爬虫系列-预备篇","slug":"2016-Scrapy爬虫系列-预备篇","date":"2016-07-14T03:31:02.000Z","updated":"2019-12-05T05:55:12.453Z","comments":true,"path":"2016/07/14/2016-Scrapy爬虫系列-预备篇/","link":"","permalink":"https://straysh.github.life/2016/07/14/2016-Scrapy%E7%88%AC%E8%99%AB%E7%B3%BB%E5%88%97-%E9%A2%84%E5%A4%87%E7%AF%87/","excerpt":"","text":"以前一直是在Web开发框架下(YII、Laravel)的Console模块中开发自己的爬虫,再结合shell脚本,发起多进程并发抓取数据.这一套下来,复杂且艰难之处在于要在shell和php之间来回调试,并且我对shell的错误处理甚是粗陋,再加上诸如find、grep、awk各种命令的参数繁多,整个开发的过程痛并快乐着. 最近开始研究python的爬虫框架Scrapy.与专业的爬虫框架相比,我以前的工作模式说是刀耕火种都不为过呀! 我会有数篇文章来记录Scrapy的学历历程,但是首先,介绍一些预备知识(不涉及语言本身) 一、 新人不要使用virtualenvvirtualenv很好用.但是对新人而言,费时费力的维护不同版本的python就是舍本逐末的行为.在安装virtualenv之前请先执行python --version检查系统自带的pytohn版本.若是2.7.*,就直接使用系统的python就好了;否则安装2.7版本的python. P.S.:为什么要2.7.*,因为Scrapy只支持pyton2.7 Scrapy支持那些Python版本？ Scrapy仅仅支持Python 2.7。 Python2.6的支持从Scrapy 0.20开始被废弃了。 二、python中的编码问题unicode/utf-8/gbk/ascii1. 什么是ASCIIASCII(美国信息交换标准代码),最后一次更新则是在1986年，至今为止共定义了128个字符,包括26个基本拉丁字母、阿拉伯数目字和英式标点符号.并且最多只能表示256个字符.ascii是单字节的 2.什么是gbk256个字符显然不可能支持中文、俄文、韩文、日文等,所以几乎每个语言都有自己的一套编码标准,例如GBK*/BIG*.gbk是两字节的. 3.什么是utf-8各个语言各自有一套编码标准,但是各个标准之间是不兼容的.utf-8解决了这个问题,世界一统,大家都用utf-8.utf-8是(最多)三字节的.除了utf-8,还有utf-16,utf-32 4.什么是unicode其实上面说的不太准确,世界一统的字符集叫unicode,理论上unicode可以无限扩充.但unicode有个致命缺点:浪费存储空间!所以才有了utf-8,utf-8是为了解决这个问题,对unicode的传输和存储的规则.然而对于我们面向应用的开发者,保持utf-8整齐划一就可以了,不需要再向上操作unicode. 5.python中的unicode/utf-8按上文所述,unicode才是真正的字符串, utf-8是更底层的存储格式用可阅读的形式(humanreadable)打印出来的字符串.因此,在python中: unicode-&gt;encode(‘utf-8’)得到普通字符串 是正确的 普通字符串-&gt;decode(‘utf-8’)得到unicode字符串 是正确的 对unicode字符串decode(&#39;utf-8&#39;) 是错误的!! 对普通字符串encode(&#39;utf-8&#39;) 也是错误的!!123456# coding=utf-8foo = u&#x27;中&#x27; # unicode字符串print repr(foo) # u&#x27;\\u4e2d&#x27;print len(foo) # 1print repr(foo.encode(&#x27;utf-8&#x27;)) # &#x27;\\xe4\\xb8\\xad&#x27;print len(foo.encode(&#x27;utf-8&#x27;)) # 3 三、为了全部统一使用utf-8,我们应该做什么?–python和mysql之间的编码问题 首先在python文件首行添加# coding=utf-8 连接数据库时,指定charset=utf-8 连接数据库后,立即执行set names utf-8 mysql的配置文件中,理论上是不需要更改的(在每一处需要指定字符集的地方都显示的指定utf-8),但为避免疏漏出错,你需要123[mysqld]character-set-server=utf8collation-server=utf8_general_ci 四、 Incorrect string value: &#39;\\xF0\\x9F\\x98\\x82&#39; for column ... 或者 Invalid utf8 character string: &#39;F09F98&#39;这是Mysql在处理四字节的utf-8字符串造成的错误.Mysql&gt;=5.5开始,声称解决了这个问题:增加了一个叫做utf8mb4的编码.utf8mb4是扩充之后的unicode,实际上还是utf8,同php、python2.7中的utf8. 换句话说,若需要支持utf8mb4,在php和python中你不需要做任何更改,仍然正常使用utf-8即可;但是在mysql中你需要: 在建表语句中明确表示使用utf8mb4 123create table demo_douban_book.all_books ( ... ...)engine=innodb default charset=utf8mb4 collate utf8mb4_unicode_ci; 需要特别注意的是, 使用了utf8mb4之后,会对索引产生影响(mysql对索引有最大字节长度限制,而使用utf8mb4在相同字节长度下字符个数却少了) 另外,如果遇到了LookupError: unknown encoding: utf8mb4这个错误,连接数据库之前: 12import codecscodecs.register(lambda name: codecs.lookup(&#x27;utf8&#x27;) if name == &#x27;utf8mb4&#x27; else None) 因为根本没有utf8mb4这种东西,需要给它加个别名,告诉pythonutf8mb4就是utf-8","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"laravel队列的使用(五)-dig-into-source","slug":"2016-laravel队列的使用-五-dig-into-source","date":"2016-07-07T03:46:09.000Z","updated":"2019-12-05T05:51:54.620Z","comments":true,"path":"2016/07/07/2016-laravel队列的使用-五-dig-into-source/","link":"","permalink":"https://straysh.github.life/2016/07/07/2016-laravel%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8-%E4%BA%94-dig-into-source/","excerpt":"","text":"Laravel队列源代码解析这边文章中,将简单梳理一下Laravel队列的源代码,主要弄清楚代码的执行流. 我们将从消费队列的两个终端命令开始并以queue:work为重点 一、./artisan queue:listen –timeout=30 –tries=3首先打开项目根下的artisan文件,可以看到调用了Illuminate\\Console\\Kernel对象的handle()方法 1234567$kernel = $app-&gt;make(Illuminate\\Contracts\\Console\\Kernel::class);$status = $kernel-&gt;handle( $input = new Symfony\\Component\\Console\\Input\\ArgvInput, new Symfony\\Component\\Console\\Output\\ConsoleOutput); 定位到Illuminate\\Console\\Kernel::handle()函数, try-catch中的主代码只有两行.其中$this-&gt;bootstrap()启动/绑定/延迟启动 一些系统服务. 12$this-&gt;bootstrap();return $this-&gt;getArtisan()-&gt;run($input, $output); 继续定位到getArtisan()函数, 这里实例化了一个Artisan对象并返回.结合上面的代码,知道这里依次执行了Artisan::resolveCommands()和Artisan::run() 1234567use Illuminate\\Console\\Application as Artisan;...if (is_null($this-&gt;artisan)) &#123; return $this-&gt;artisan = (new Artisan($this-&gt;app, $this-&gt;events, $this-&gt;app-&gt;version())) -&gt;resolveCommands($this-&gt;commands);&#125;return $this-&gt;artisan; 继续dig, 在Artisan的父类Symfony\\Component\\Console\\Application as SymfonyApplication中找到run方法 run方法中$exitCode = $this-&gt;doRun($input, $output); doRun方法中$exitCode = $this-&gt;doRunCommand($command, $input, $output); doRunCommand方法中$exitCode = $command-&gt;run($input, $output); 到这里,看到调用了$command对象的run方法.$command来自doRun方法中$command = $this-&gt;find($name);.这里不再继续调试,直接给出$command对象的一些信息 1234567891011Illuminate\\Queue\\Console\\ListenCommand &#123;#634 #name: &quot;queue:listen&quot; #description: &quot;Listen to a given queue&quot; #listener: Illuminate\\Queue\\Listener &#123;#635 #environment: null #sleep: 3 #maxTries: 0 #workerCommand: &quot;&#x27;/usr/bin/php5&#x27; &#x27;artisan&#x27; queue:work %s --queue=%s --delay=%s --memory=%s --sleep=%s --tries=%s&quot; #outputHandler: null &#125;... ... 追踪Illuminate\\Queue\\Console\\ListenCommand::fire() 123$this-&gt;listener-&gt;listen( $connection, $queue, $delay, $memory, $timeout); 其中的$this-&gt;listener 123456789101112131415Illuminate\\Queue\\Listener &#123;#635 #environment: &quot;local&quot; #sleep: 3 #maxTries: &quot;3&quot; #workerCommand: &quot;&#x27;/usr/bin/php5&#x27; &#x27;artisan&#x27; queue:work %s --queue=%s --delay=%s --memory=%s --sleep=%s --tries=%s&quot; #outputHandler: Closure &#123;#22 class: &quot;Illuminate\\Queue\\Console\\ListenCommand&quot; this: Illuminate\\Queue\\Console\\ListenCommand &#123;#634 …&#125; parameters: array:2 [ &quot;$type&quot; =&gt; [] &quot;$line&quot; =&gt; [] ] line: &quot;107 to 109&quot; &#125;&#125; 追踪到listen函数 1234567public function listen($connection, $queue, $delay, $memory, $timeout = 60)&#123; $process = $this-&gt;makeProcess($connection, $queue, $delay, $memory, $timeout); while (true) &#123; $this-&gt;runProcess($process, $memory); &#125;&#125; 追踪runProcess函数,找到Process类的 1234567public function run($callback = null)&#123; $this-&gt;start($callback); return $this-&gt;wait();&#125;# start函数中,最终看到执行了命令&#x27;/usr/bin/php5&#x27; &#x27;artisan&#x27; queue:work &#x27;&#x27; --queue=&#x27;default&#x27; --delay=0 --memory=128 --sleep=3 --tries=3 --env=&#x27;local&#x27; 到这里,已经很清楚了../artisan queue:listen启动了一个死循环, 不停的尝试执行./artisan queue:work 二、./artisan queue:work –daemon –tries=3同上, 不同之处在于, doRunCommand方法中$exitCode = $command-&gt;run($input, $output);中$command 12345Illuminate\\Queue\\Console\\WorkCommand &#123;#610 #name: &quot;queue:work&quot; #description: &quot;Process the next job on a queue&quot; #worker: Illuminate\\Queue\\Worker &#123;#611... ... 追踪Illuminate\\Queue\\Console\\WorkCommand::fire() 123$response = $this-&gt;runWorker( $connection, $queue, $delay, $memory, $this-&gt;option(&#x27;daemon&#x27;)); 1234567891011121314151617181920protected function runWorker($connection, $queue, $delay, $memory, $daemon = false)&#123; if ($daemon) &#123; $this-&gt;worker-&gt;setCache($this-&gt;laravel[&#x27;cache&#x27;]-&gt;driver()); $this-&gt;worker-&gt;setDaemonExceptionHandler( $this-&gt;laravel[&#x27;Illuminate\\Contracts\\Debug\\ExceptionHandler&#x27;] ); return $this-&gt;worker-&gt;daemon( $connection, $queue, $delay, $memory, $this-&gt;option(&#x27;sleep&#x27;), $this-&gt;option(&#x27;tries&#x27;) ); &#125; return $this-&gt;worker-&gt;pop( $connection, $queue, $delay, $this-&gt;option(&#x27;sleep&#x27;), $this-&gt;option(&#x27;tries&#x27;) );&#125; runWorker中的$this-&gt;worker 123Illuminate\\Queue\\Worker &#123;#611 #manager: Illuminate\\Queue\\QueueManager &#123;#612 #app: Illuminate\\Foundation\\Application &#123;#3 首先对于不带--daemon的命令,那么上文中最后提到./artisan queue:work执行的就是$this-&gt;worker-&gt;pop.从队列列表中pop一个任务出来执行.执行完成之后sleep若干秒,进入while(true)死循环,无限重试pop.或者在内存使用超过--memory限制时,调用die终止进程. 然后, 继续dig --daemon模式, 找到Illuminate\\Queue\\Worker::daemon() 123456789101112131415161718public function daemon($connectionName, $queue = null, $delay = 0, $memory = 128, $sleep = 3, $maxTries = 0)&#123; $lastRestart = $this-&gt;getTimestampOfLastQueueRestart(); while (true) &#123; if ($this-&gt;daemonShouldRun()) &#123; $this-&gt;runNextJobForDaemon( $connectionName, $queue, $delay, $sleep, $maxTries ); &#125; else &#123; $this-&gt;sleep($sleep); &#125; if ($this-&gt;memoryExceeded($memory) || $this-&gt;queueShouldRestart($lastRestart)) &#123; $this-&gt;stop(); &#125; &#125;&#125; 这里还是一个死循环.依次执行 尝试daemonShouldRun(), 若网站启用了maintaince模式则返回FALSE,否则广播&#39;illuminate.queue.looping&#39;事件 runNextJobForDaemon()函数中,仍然是尝试从队列列表中pop任务出来执行 若daemonShouldRun()检测到FALSE, sleep若干秒 内存检测,在内存使用超过--memory限制时,调用die终止进程.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"laravel队列的使用(四)-beanstalk-driver","slug":"2016-laravel队列的使用-四-beanstalk-driver","date":"2016-07-07T02:45:11.000Z","updated":"2019-12-05T05:52:34.268Z","comments":true,"path":"2016/07/07/2016-laravel队列的使用-四-beanstalk-driver/","link":"","permalink":"https://straysh.github.life/2016/07/07/2016-laravel%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8-%E5%9B%9B-beanstalk-driver/","excerpt":"","text":"Beanstalk驱动的队列示例在开始介绍如何在Laravel中使用Beanstalkd之前,我们先安装一个Beanstalkd的Web版管理工具.在文章的最后,会介绍队列中常用的其他几个api. 123456# composer会将Beanstalk Console以及它的所有依赖库安装到beanstalk_console目录.composer create-project ptrofimov/beanstalk_console -s dev beanstalk_console# 启动管理后台服务php -S localhost:7654 -t public# 启动beanstalkdbeanstalkd -l 127.0.0.1 -p 11300 &amp; Step0 首先,一定确保beanstalkd已经启动且端口是3000, 一定确保beanstalk_console已经启动.通过浏览器访问localhost:7654.点击Add server,添加一个beanstalkd服务器. Step1 在.env中设置QUEUE_DRIVER=beanstalkd ,启用beanstalkd驱动 Step2 将DemoJob::register()中抛异常的测试代码注释掉,然后通过浏览器访问api,生产一个队列任务.在beanstalk_console中,你将看到: Step3 消费队列不需要额外的代码编写,只需要在终端执行以下代码: 123./artisan queue:listen --timeout=30 --tries=3或者./artisan queue:work --daemon --tries=3 Step4 在管理后台可以看到,队列任务已经被消费掉了 Laravel队列常用API在DemoJob类中,可以调用以下API,更加精确的控制队列操作 $this-&gt;relase($waitSeconds) $waitSeconds秒后,将任务重新压入队列.(压入队列后任务不一定立即执行,取决于优先级等其他因素) $this-&gt;delete() 将任务从队列中移除.如本例中当检查email字段发现其格式不正确时,就应当删除该队列任务并return $this&gt;attempts() 这个函数中包含了最大尝试的逻辑,最大尝试次数是通过命令启动队列时传入的--tries=3.通常这个api不需要手动调用,除非你要定制该函数. 另外,通过$this-&gt;job可以拿到Illuminate\\Queue\\Jobs\\BeanstalkdJob对象,从而调用独属于Beanstalkd的api.具体的api可以查看源代码.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"laravel队列的使用(三)-beanstalk-driver","slug":"2016-laravel队列的使用-三-beanstalk-driver","date":"2016-07-06T08:16:53.000Z","updated":"2019-12-01T10:13:40.404Z","comments":true,"path":"2016/07/06/2016-laravel队列的使用-三-beanstalk-driver/","link":"","permalink":"https://straysh.github.life/2016/07/06/2016-laravel%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8-%E4%B8%89-beanstalk-driver/","excerpt":"","text":"数据库驱动的队列示例这次增加对失败的队列任务的处理,详细内容从Step5开始 Step1 在.env中设置QUEUE_DRIVER=database ,启用数据库驱动 Step2 生成jobs表 12php artisan queue:tablephp artisan migrate Step3 其他代码不变.现在通过浏览器访问api, 生产一个测试队列任务,在jobs表中: 1234567891011mysql&gt; select * from jobs \\G*************************** 1. row *************************** id: 1 queue: default payload: &#123;&quot;job&quot;:&quot;Illuminate\\\\Queue\\\\CallQueuedHandler@call&quot;,&quot;data&quot;:&#123;&quot;command&quot;:&quot;O:16:\\&quot;App\\\\Jobs\\\\DemoJob\\&quot;:6:&#123;s:6:\\&quot;mailer\\&quot;;N;s:5:\\&quot;queue\\&quot;;N;s:5:\\&quot;delay\\&quot;;N;s:6:\\&quot;\\u0000*\\u0000job\\&quot;;N;s:8:\\&quot;username\\&quot;;s:8:\\&quot;username\\&quot;;s:5:\\&quot;email\\&quot;;s:19:\\&quot;jobhancao@gmail.com\\&quot;;&#125;&quot;&#125;&#125; attempts: 0 reserved: 0 reserved_at: NULLavailable_at: 1467793393 created_at: 14677933931 row in set (0.00 sec) Step4 消费队列不需要额外的代码编写,只需要在终端执行以下代码: 123./artisan queue:listen --timeout=30 --tries=3或者./artisan queue:work --daemon --tries=3 queue:work若以–daemon方式启动,将不需要重复绑定和注册框架的服务,CPU利用率更高.但此模式下,如文件句柄、画图句柄一定要及时释放,避免内存溢出. Step5 对失败队列的处理.下面我们来模拟队列失败的场景.在这之前,先执行以下命令创建failed_jobs表: 1php artisan queue:failed-table Step6 在DemoJob::register()函数中手动抛出异常,模拟代码出现bug 1234567//发送注册邮件private function register()&#123; throw new \\Exception(&quot;测试异常&quot;); // 为简便我们把邮件写入log中 Log::info(&quot;mail to &#123;$this-&gt;username&#125;/&#123;$this-&gt;email&#125; sent by DemoJob&quot;);&#125; Step7 重复Step3-Step4 生产一个队列任务并消费之 1234567891011$./artisan queue:listen --timeout=30 --tries=3 [Exception] 测试异常 [Exception] 测试异常 [Exception] 测试异常 Failed: Illuminate\\Queue\\CallQueuedHandler@call Step8 在failed_jobs表中: 12345678mysql&gt; select * from failed_jobs \\G*************************** 1. row *************************** id: 1connection: database queue: default payload: &#123;&quot;job&quot;:&quot;Illuminate\\\\Queue\\\\CallQueuedHandler@call&quot;,&quot;data&quot;:&#123;&quot;command&quot;:&quot;O:16:\\&quot;App\\\\Jobs\\\\DemoJob\\&quot;:6:&#123;s:6:\\&quot;mailer\\&quot;;N;s:5:\\&quot;queue\\&quot;;N;s:5:\\&quot;delay\\&quot;;N;s:6:\\&quot;\\u0000*\\u0000job\\&quot;;N;s:8:\\&quot;username\\&quot;;s:8:\\&quot;username\\&quot;;s:5:\\&quot;email\\&quot;;s:19:\\&quot;jobhancao@gmail.com\\&quot;;&#125;&quot;&#125;&#125; failed_at: 2016-07-06 16:48:041 row in set (0.00 sec) Step9 failed_job表中记录着失败的队列任务,在生产环境中可以用来debug, 对于已经失败的任务,执行以下命令重新将之压入队列并消费: 1./artisan queue:retry all","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"laravel队列的使用(二)-database-driver","slug":"2016-laravel队列的使用-二-database-driver","date":"2016-07-06T08:13:53.000Z","updated":"2019-12-01T10:13:39.516Z","comments":true,"path":"2016/07/06/2016-laravel队列的使用-二-database-driver/","link":"","permalink":"https://straysh.github.life/2016/07/06/2016-laravel%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8-%E4%BA%8C-database-driver/","excerpt":"","text":"数据库驱动的队列示例这次增加对失败的队列任务的处理,详细内容从Step5开始 Step1 在.env中设置QUEUE_DRIVER=database ,启用数据库驱动 Step2 生成jobs表 12php artisan queue:tablephp artisan migrate Step3 其他代码不变.现在通过浏览器访问api, 生产一个测试队列任务,在jobs表中: 1234567891011mysql&gt; select * from jobs \\G*************************** 1. row *************************** id: 1 queue: default payload: &#123;&quot;job&quot;:&quot;Illuminate\\\\Queue\\\\CallQueuedHandler@call&quot;,&quot;data&quot;:&#123;&quot;command&quot;:&quot;O:16:\\&quot;App\\\\Jobs\\\\DemoJob\\&quot;:6:&#123;s:6:\\&quot;mailer\\&quot;;N;s:5:\\&quot;queue\\&quot;;N;s:5:\\&quot;delay\\&quot;;N;s:6:\\&quot;\\u0000*\\u0000job\\&quot;;N;s:8:\\&quot;username\\&quot;;s:8:\\&quot;username\\&quot;;s:5:\\&quot;email\\&quot;;s:19:\\&quot;jobhancao@gmail.com\\&quot;;&#125;&quot;&#125;&#125; attempts: 0 reserved: 0 reserved_at: NULLavailable_at: 1467793393 created_at: 14677933931 row in set (0.00 sec) Step4 消费队列不需要额外的代码编写,只需要在终端执行以下代码: 123./artisan queue:listen --timeout=30 --tries=3或者./artisan queue:work --daemon --tries=3 queue:work若以–daemon方式启动,将不需要重复绑定和注册框架的服务,CPU利用率更高.但此模式下,如文件句柄、画图句柄一定要及时释放,避免内存溢出. Step5 对失败队列的处理.下面我们来模拟队列失败的场景.在这之前,先执行以下命令创建failed_jobs表: 1php artisan queue:failed-table Step6 在DemoJob::register()函数中手动抛出异常,模拟代码出现bug 1234567//发送注册邮件private function register()&#123; throw new \\Exception(&quot;测试异常&quot;); // 为简便我们把邮件写入log中 Log::info(&quot;mail to &#123;$this-&gt;username&#125;/&#123;$this-&gt;email&#125; sent by DemoJob&quot;);&#125; Step7 重复Step3-Step4 生产一个队列任务并消费之 1234567891011$./artisan queue:listen --timeout=30 --tries=3 [Exception] 测试异常 [Exception] 测试异常 [Exception] 测试异常 Failed: Illuminate\\Queue\\CallQueuedHandler@call Step8 在failed_jobs表中: 12345678mysql&gt; select * from failed_jobs \\G*************************** 1. row *************************** id: 1connection: database queue: default payload: &#123;&quot;job&quot;:&quot;Illuminate\\\\Queue\\\\CallQueuedHandler@call&quot;,&quot;data&quot;:&#123;&quot;command&quot;:&quot;O:16:\\&quot;App\\\\Jobs\\\\DemoJob\\&quot;:6:&#123;s:6:\\&quot;mailer\\&quot;;N;s:5:\\&quot;queue\\&quot;;N;s:5:\\&quot;delay\\&quot;;N;s:6:\\&quot;\\u0000*\\u0000job\\&quot;;N;s:8:\\&quot;username\\&quot;;s:8:\\&quot;username\\&quot;;s:5:\\&quot;email\\&quot;;s:19:\\&quot;jobhancao@gmail.com\\&quot;;&#125;&quot;&#125;&#125; failed_at: 2016-07-06 16:48:041 row in set (0.00 sec) Step9 failed_job表中记录着失败的队列任务,在生产环境中可以用来debug, 对于已经失败的任务,执行以下命令重新将之压入队列并消费: 1./artisan queue:retry all","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"laravel队列的使用(一)","slug":"2016-laravel队列的使用-一","date":"2016-07-06T08:11:28.000Z","updated":"2019-12-01T10:13:40.304Z","comments":true,"path":"2016/07/06/2016-laravel队列的使用-一/","link":"","permalink":"https://straysh.github.life/2016/07/06/2016-laravel%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%BF%E7%94%A8-%E4%B8%80/","excerpt":"","text":"一些对数据一致性要求不高的场景下(如行为日志、各种统计数字如浏览数/评论数),可以使用事件/监听将这部分代码从主逻辑上剥离出来. 优点有二:①提高主逻辑的效率/降低主逻辑的复杂度 ②这部分异步代码执行失败不会对中断主逻辑的执行. 在Lravel中使用event/listener很容易实现.但有时我们需要延时任务,例如用户注册成功5分钟后给用户发送一封邮件通知.这时就需要引入延时队列了. 参考: Lavavel中的事件和任务, 各在什么场景下使用? 一个同步执行的队列示例同event一样,在app/config/queue.php中&#39;default&#39; =&gt; env(&#39;QUEUE_DRIVER&#39;, &#39;sync&#39;), 设置默认的队列驱动. 当前Lravel支持的驱动列表在app/config/queue.php::connections数组中 123456sync =&gt; 同步模式,在主逻辑中直接执行,便于调试 database =&gt; 使用mysql作为驱动,需要使用`./artisan queue:table &amp;&amp; ./artisan migrate`创建jobs表beanstalkd =&gt; beanstalkd(一个独立的队列服务)作为驱动, 在下篇文章中将详细讲解如何安装并使用beanstalkdsqs =&gt; Amazon SQS: aws/aws-sdk-php ~3.0iron =&gt; 类似sql的云端队列服务redis =&gt; Redis: predis/predis ~1.0 Step1 在.env中设置QUEUE_DRIVER=sync 或者删除该行,启用同步队列 Step2 在app/Jobs目录中新建BaseJob.php 1234567891011&lt;?php namespace App\\Jobs;use Illuminate\\Contracts\\Queue\\ShouldQueue;use Illuminate\\Queue\\InteractsWithQueue;use Illuminate\\Contracts\\Bus\\SelfHandling;class BaseJob extends Job implements SelfHandling, ShouldQueue&#123; use InteractsWithQueue;&#125; Step3 在app/Jobs目录中新建DemoJob.php 12345678910111213141516171819202122232425262728293031&lt;?php namespace App\\Jobs;use Illuminate\\Contracts\\Mail\\Mailer;use Illuminate\\Support\\Facades\\Log;class DemoJob extends BaseJob&#123; public $mailer; function __construct($username, $email) &#123; $this-&gt;username = $username; $this-&gt;email = $email; &#125; public function handle(Mailer $mailer) &#123; $this-&gt;mailer = $mailer; $this-&gt;register(); return TRUE; &#125; //发送注册邮件 private function register() &#123; // 为简便我们把邮件写入log中 Log::info(&quot;mail sent by DemoJob&quot;); &#125;&#125; Step4 生产一个队列任务.在任意可访问的api中添加如下方法, 123456789use App\\Jobs\\DemoJob;class TestController extends Controller&#123; public function getIndex() &#123; dd($this-&gt;dispatch( new DemoJob(&quot;username&quot;, &quot;jobhancao@gmail.com&quot;) )); &#125;&#125; log输出如下: 1[2016-07-06 16:07:30] local.INFO: mail to username/jobhancao@gmail.com sent by DemoJob","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"Samba杂疑","slug":"2016-Samba杂疑","date":"2016-06-13T08:42:21.000Z","updated":"2019-12-01T10:13:39.492Z","comments":true,"path":"2016/06/13/2016-Samba杂疑/","link":"","permalink":"https://straysh.github.life/2016/06/13/2016-Samba%E6%9D%82%E7%96%91/","excerpt":"","text":"现象:Window连接samba服务器正常, 其他linux客户机(centos、Ubuntu、Mac)经常timeout.解决方法:在samba服务器上,检查/etc/sysconfig/network以及/etc/hosts两个文件,将主机名-ip映射加入. 例如: 123root@y125 ~]$cat /etc/hosts127.0.0.1 y125 localhost::1 y125 localhost","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"中文分词","slug":"2015-中文分词","date":"2015-11-18T02:32:05.000Z","updated":"2019-12-01T10:13:40.372Z","comments":true,"path":"2015/11/18/2015-中文分词/","link":"","permalink":"https://straysh.github.life/2015/11/18/2015-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/","excerpt":"","text":"ICTCLAS(中科院) phpscws robbe php PHP 安装 Robbe 中文分词扩展 phpcws php scws 中文分词 (凡有需要指定编码之处均为utf8)github地址hightman/scws下载源码包(不要直接clone github源代码,下载release分支)1wget -q -O - http://www.xunsearch.com/scws/down/scws-1.2.1.tar.bz2 | tar xjf - 编译12345cd path/to/scws-1.2.1autoconf./configre --prefix=path/to/php_scwsmakesudo make install 编译php扩展12345cd phpextphpize./configure --with-scws=path/to/php_scwsmakemake install 新增一个php_scws.ini的php模块配置项 12345678sudo vim /etc/php5/modules-available/php_scws.ini[scws]; 注意请检查 php.ini 中的 extension_dir 的设定值是否正确, 否则请将 extension_dir 设为空，; 再把 extension = scws.so 或 php_scws.dll 指定绝对路径。extension = scws.soscws.default.charset = utf8scws.default.fpath = path/to/etc 重启apache之后,若phpinfo()中有scws但php -m|grep scws没有,需要在/etc/php5/cli/中也增加一个php_scws.ini配置文件 php接口以及demo 下载词典文件词典文件下载地址http://www.xunsearch.com/scws/download.php, [简体中文(UTF-8) (3.9MB，28万词，2015/04/02更新)] . 将加压后的dict.utf8.xdb文件复制到/path/to/php_scws/etc/目录下,并chmod a+r ./* 注:压缩包内有两个php文件dump_xdb_file.php和make_xdb_file.php,已经失效了.我测试的时候将源词典解压再压缩回去,测试样例’陈凯歌’跑失败了. 命令行测试12345678scws -c utf8 -i cityname.txt -d /home/softwares/php_scws/etc/dict.utf8.xdb:/home/softwares/php_scws/etc/food.utf8.xdb 鱼香肉丝 陈凯歌 并 不是 《 无极 》 的 唯一 著作权人 ， 一 部 电影 的 整体 版权 归 电影 制片厂 所有 。 一 部 电影 的 作者 包括 导演 、 摄影 、 编剧 等 创作 人员 ， 这些 创作 人员 对 他们 的 创作 是 有 版权 的 。 不 经过 制片人 授权 ， 其他人 不能 对 电影 做 拷贝 、 发行 、 反映 ， 不能 通过 网络 来 传播 ， 既 不能 把 电影 改编 成 小说 、 连环画 等 其他 艺术 形式 发表 ， 也 不能 把 一 部 几 个 小时 才能 放 完 的 电影 改编 成 半 个 小时 就 能 放 完 的 短片 。 著作权 和 版权 在 我国 是 同一个 概念 ， 是 法律 赋予 作品 创作者 的 专有 权利 。 所谓 专有 权利 就是 没有 经过 权利人 许可 又 不是 法律 规定 的 例外 ， 要 使用 这个 作品 ， 就 必须 经过 作者 授权 ， 没有 授权 就是 侵权 。 自定义词典自定义词典格式:用\\t分隔 12# WORD TF IDF ATTR鱼香肉丝 14.01 8.10 n 生成词典: 1scws-gen-dict -i out/food.txt -o food.utf8.xdb -c utf8 附 常用中文分词 哈工大：语言云（语言技术平台云 LTP-Cloud） 东北大学NiuTrans统计机器翻译系统：东北大学自然语言处理实验室 中科院张华平博士ICTCLAS ：NLPIR汉语分词系统 波森科技：首页 - BosonNLP 结巴：fxsjy/jieba · GitHub Ansj分词：中国自然语言开源组织 附:汉语文本词性标注标记集(北大版)北大标注集： 12345678910111213141516171819202122232425262728293031323334353637383940代码 名称 帮助记忆的诠释 Ag 形语素 形容词性语素。形容词代码为a，语素代码ｇ前面置以A。 a 形容词 取英语形容词adjective的第1个字母。 ad 副形词 直接作状语的形容词。形容词代码a和副词代码d并在一起。 an 名形词 具有名词功能的形容词。形容词代码a和名词代码n并在一起。 b 区别词 取汉字“别”的声母。 c 连词 取英语连词conjunction的第1个字母。 Dg 副语素 副词性语素。副词代码为d，语素代码ｇ前面置以D。 d 副词 取adverb的第2个字母，因其第1个字母已用于形容词。 e 叹词 取英语叹词exclamation的第1个字母。 f 方位词 取汉字“方” 的声母。 g 语素 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。 h 前接成分 取英语head的第1个字母。 i 成语 取英语成语idiom的第1个字母。 j 简称略语 取汉字“简”的声母。 k 后接成分 l 习用语 习用语尚未成为成语，有点“临时性”，取“临”的声母。 m 数词 取英语numeral的第3个字母，n，u已有他用。 Ng 名语素 名词性语素。名词代码为n，语素代码ｇ前面置以N。 n 名词 取英语名词noun的第1个字母。 nr 人名 名词代码n和“人(ren)”的声母并在一起。 ns 地名 名词代码n和处所词代码s并在一起。 nt 机构团体 “团”的声母为t，名词代码n和t并在一起。 nz 其他专名 “专”的声母的第1个字母为z，名词代码n和z并在一起。 o 拟声词 取英语拟声词onomatopoeia的第1个字母。 p 介词 取英语介词prepositional的第1个字母。 q 量词 取英语quantity的第1个字母。 r 代词 取英语代词pronoun的第2个字母,因p已用于介词。 s 处所词 取英语space的第1个字母。 Tg 时语素 时间词性语素。时间词代码为t,在语素的代码g前面置以T。 t 时间词 取英语time的第1个字母。 u 助词 取英语助词auxiliary 的第2个字母,因a已用于形容词。 Vg 动语素 动词性语素。动词代码为v。在语素的代码g前面置以V。 v 动词 取英语动词verb的第一个字母。 vd 副动词 直接作状语的动词。动词和副词的代码并在一起。 vn 名动词 指具有名词功能的动词。动词和名词的代码并在一起。 w 标点符号 x 非语素字 非语素字只是一个符号，字母x通常用于代表未知数、符号。 y 语气词 取汉字“语”的声母。 z 状态词 取汉字“状”的声母的前一个字母。 计算所标注集（V5.0）：0. 说明计算所汉语词性标记集（共计99个，22个一类，66个二类，11个三类）主要用于中国科学院计算技术研究所研制的汉语词法分析器、句法分析器和汉英机器翻译系统。本标记集主要参考了以下词性标记集： 北大《人民日报》语料库词性标记集； 北大2002新版词性标记集（草稿）； 清华大学汉语树库词性标记集； 教育部语用所词性标记集（国家推荐标准草案2002版）； 美国宾州大学中文树库（ChinesePennTreeBank）词性标记集；由于计算所的汉语词法分析器主要采用北大《人民日报》语料库进行参数训练，因此本词性标记集主要以北大《人民日报》语料库的词性标记集为蓝本，并参考了北大《汉语语法信息词典》中给出的汉语词的语法信息。本标记集在制定过程中主要考虑了以下几方面的因素： 有助于提高汉语词法分析器的切分和标注正确率； 有助于提高汉语句法分析器的正确率； 有助于汉英机器翻译系统进行翻译； 易于从北大《人民日报》语料库词性标记集进行转换； 对于语法功能不同的词，在不造成词法分析和句法分析歧义区分困难的情况下，尽可能细分子类。基于以上考虑，我们在标注过程中尽量避免那些容易出错的词性标记，而采用那些不容易出错、而对提高汉语词法句法分析正确率有明显作用的标记。例如，在动词的子类中，我们参考了宾州大学中文树库的做法，把汉语动词“是”和“有”分别做成单独的标记，而没有采用“系动词”的标记。因为同样是“是”这个动词，其句法功能很多，作“系动词”只是其中一种功能，而要区分这些功能是非常困难的，会导致词法分析的正确率下降。在名词子类中，我们区分了“汉语人名”、“日语人名”和“翻译人名”，这不仅仅是因为这三种人名要采用不同的参数进行训练与识别，而且在汉英机器翻译中也要采用不同的分析算法进行翻译。又如，我们把表示时间的“数词＋‘年’”（如“1995年”）合并成一个时间词，而表示年头的“数词＋‘年’”分别标注为“数词”和“量词”，这是因为我们通过实验发现这种区分在词法分析阶段通过统计方法可以达到较高的正确率，而且这种区分对于后续的句法分析和机器翻译有非常重要的作用。对于某些词类（助词和标点符号），基本上是一个封闭集，而这些词类中各个词的语法功能相差很大，在这种情况下，我们尽可能地细分其子类。另外，与其他词性标记集类似，在我们的标记体系中，小类只是大类中一些有必要区分的一些特例，但小类的划分不满足完备性。 名词 (1个一类，7个二类，5个三类)名词分为以下子类：n 名词nr 人名nr1 汉语姓氏nr2 汉语名字nrj 日语人名nrf 音译人名ns 地名nsf 音译地名nt 机构团体名nz 其它专名nl 名词性惯用语ng 名词性语素 时间词(1个一类，1个二类)t 时间词tg 时间词性语素 处所词(1个一类)s 处所词 方位词(1个一类)f 方位词 动词(1个一类，9个二类)v 动词vd 副动词vn 名动词vshi 动词“是”vyou 动词“有”vf 趋向动词vx 形式动词vi 不及物动词（内动词）vl 动词性惯用语vg 动词性语素 形容词(1个一类，4个二类)a 形容词ad 副形词an 名形词ag 形容词性语素al 形容词性惯用语 区别词(1个一类，2个二类)b 区别词 bl 区别词性惯用语8. 状态词(1个一类)z 状态词9. 代词(1个一类，4个二类，6个三类)r 代词rr 人称代词rz 指示代词rzt 时间指示代词rzs 处所指示代词rzv 谓词性指示代词ry 疑问代词ryt 时间疑问代词rys 处所疑问代词ryv 谓词性疑问代词rg 代词性语素10. 数词(1个一类，1个二类)m 数词mq 数量词11. 量词(1个一类，2个二类)q 量词qv 动量词qt 时量词12. 副词(1个一类)d 副词13. 介词(1个一类，2个二类)p 介词pba 介词“把”pbei 介词“被”14. 连词(1个一类，1个二类)c 连词cc 并列连词15. 助词(1个一类，15个二类)u 助词uzhe 着ule 了 喽uguo 过ude1 的 底ude2 地ude3 得usuo 所udeng 等 等等 云云uyy 一样 一般 似的 般udh 的话uls 来讲 来说 而言 说来 uzhi 之ulian 连 （“连小学生都会”） 叹词(1个一类)e 叹词 语气词(1个一类)y 语气词(delete yg) 拟声词(1个一类)o 拟声词 前缀(1个一类)h 前缀 后缀(1个一类)k 后缀 字符串(1个一类，2个二类)x 字符串xx 非语素字xu 网址URL 标点符号(1个一类，16个二类)w 标点符号wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( [ { &lt;wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) ] { &gt;wyz 左引号，全角：“ ‘ 『wyy 右引号，全角：” ’ 』wj 句号，全角：。ww 问号，全角：？ 半角：?wt 叹号，全角：！ 半角：!wd 逗号，全角：， 半角：,wf 分号，全角：； 半角： ;wn 顿号，全角：、wm 冒号，全角：： 半角： :ws 省略号，全角：…… …wp 破折号，全角：—— －－ ——－ 半角：— —-wb 百分号千分号，全角：％ ‰ 半角：%wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Sphinx","slug":"Sphinx","permalink":"https://straysh.github.life/tags/Sphinx/"}]},{"title":"Golang 待读列表","slug":"2015-Golang-待读列表","date":"2015-11-13T06:50:45.000Z","updated":"2019-12-01T10:13:39.960Z","comments":true,"path":"2015/11/13/2015-Golang-待读列表/","link":"","permalink":"https://straysh.github.life/2015/11/13/2015-Golang-%E5%BE%85%E8%AF%BB%E5%88%97%E8%A1%A8/","excerpt":"","text":"理解 GO 语言的内存使用","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"https://straysh.github.life/tags/Golang/"}]},{"title":"R环境安装","slug":"2015-R环境安装","date":"2015-10-28T03:07:36.000Z","updated":"2019-12-01T10:13:39.264Z","comments":true,"path":"2015/10/28/2015-R环境安装/","link":"","permalink":"https://straysh.github.life/2015/10/28/2015-R%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/","excerpt":"","text":"R环境安装R环境安装R Documentdation","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"R","slug":"R","permalink":"https://straysh.github.life/tags/R/"}]},{"title":"Nodejs FAQ","slug":"2015-Nodejs-FAQ","date":"2015-10-27T06:26:45.000Z","updated":"2019-12-01T10:13:40.208Z","comments":true,"path":"2015/10/27/2015-Nodejs-FAQ/","link":"","permalink":"https://straysh.github.life/2015/10/27/2015-Nodejs-FAQ/","excerpt":"","text":"npm config set prefix /usr/local","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Nodejs","slug":"Nodejs","permalink":"https://straysh.github.life/tags/Nodejs/"}]},{"title":"移动开发规范","slug":"2015-移动开发规范","date":"2015-10-20T06:11:16.000Z","updated":"2019-12-01T10:13:40.156Z","comments":true,"path":"2015/10/20/2015-移动开发规范/","link":"","permalink":"https://straysh.github.life/2015/10/20/2015-%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91%E8%A7%84%E8%8C%83/","excerpt":"","text":"字体设置使用无衬线字体 123body &#123; font-family: &quot;Helvetica Neue&quot;, Helvetica, STHeiTi, sans-serif;&#125; iOS 4.0+ 使用英文字体 Helvetica Neue，之前的iOS版本降级使用 Helvetica。中文字体设置为华文黑体STHeiTi。 需补充说明，华文黑体并不存在iOS的字体库中(http://support.apple.com/kb/HT5878)， 但系统会自动将华文黑体 STHeiTi 兼容命中系统默认中文字体黑体-简或黑体-繁 1234Heiti SC Light 黑体-简 细体 （iOS 7后废弃）Heiti SC Medium 黑体-简 中黑Heiti TC Light 黑体-繁 细体Heiti TC Medium 黑体-繁 中黑 原生Android下中文字体与英文字体都选择默认的无衬线字体 1234.0 之前版本英文字体原生 Android 使用的是 Droid Sans，中文字体原生 Android 会命中 Droid Sans Fallback4.0 之后中英文字体都会使用原生 Android 新的 Roboto 字体其他第三方 Android 系统也一致选择默认的无衬线字体 基础交互设置全局的CSS样式，避免图中的长按弹出菜单与选中文本的行为 1234567a, img &#123; -webkit-touch-callout: none; /* 禁止长按链接与图片弹出菜单 */&#125;html, body &#123; -webkit-user-select: none; /* 禁止选中文本（如无文本选中需求，此为必选项） */ user-select: none;&#125; 参考资料: CSS选择器-火狐开发文档 CSS选择器","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"Mariadb vs Mysql","slug":"2015-Mariadb-vs-Mysql","date":"2015-10-07T02:47:45.000Z","updated":"2019-12-01T10:13:39.380Z","comments":true,"path":"2015/10/07/2015-Mariadb-vs-Mysql/","link":"","permalink":"https://straysh.github.life/2015/10/07/2015-Mariadb-vs-Mysql/","excerpt":"","text":"遇到的问题 Headers and client library minor version mismatch. Headers:50544 Library:50627123456789101112131415161718192021222324252627282930313233343536CentOS Linux release 7.1.1503 (Core)Linux y125 3.10.0-229.11.1.el7.x86_64 #1 SMP Thu Aug 6 01:06:18 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux;已安装的php以及mysql rpm包root@lm01 ~]$rpm -qa|grep mysqlphp56w-mysql-5.6.13-1.w7.x86_64mysql-community-client-5.6.26-2.el7.x86_64mysql-community-common-5.6.26-2.el7.x86_64mysql-community-devel-5.6.26-2.el7.x86_64mysql-community-libs-5.6.26-2.el7.x86_64mysql-community-server-5.6.26-2.el7.x86_64mysql-community-release-el7-5.noarchroot@lm01 ~]$rpm -qa|grep phpphp56w-pdo-5.6.13-1.w7.x86_64php56w-pecl-imagick-3.1.2-1.w7.x86_64php56w-mysql-5.6.13-1.w7.x86_64php56w-cli-5.6.13-1.w7.x86_64php56w-mbstring-5.6.13-1.w7.x86_64php56w-xml-5.6.13-1.w7.x86_64php56w-intl-5.6.13-1.w7.x86_64php56w-common-5.6.13-1.w7.x86_64php56w-opcache-5.6.13-1.w7.x86_64php56w-process-5.6.13-1.w7.x86_64php56w-5.6.13-1.w7.x86_64php56w-pear-1.9.4-2.w7.noarch;php-mysqli扩展mysqliMysqlI Support =&gt; enabledClient API library version =&gt; 5.6.26Active Persistent Links =&gt; 0Inactive Persistent Links =&gt; 0Active Links =&gt; 0Client API header version =&gt; 5.5.44-MariaDBMYSQLI_SOCKET =&gt; /var/lib/mysql/mysql.sock 如上, php5.6我安装的是php56w-mysql, Client API header version和Mysql版本不一致,会提示如下错误: 1Headers and client library minor version mismatch. Headers:50544 Library:50627 网上有人说使用php56w-mysqlnd可以解决问题,然而并没有什么卵用(或许是coreseek的sphinx源码bug?): 12Warning: mysql_connect(): Server sent charset (0) unknown to the client 持续digging之下,感觉有以下途径或许可以尝试: 最简单的办法,但是尚未搜索到合适的答案,可以参考这里 将mysql版本降至Mysql5.5,并重新编译php的mysql扩展 使用Mariadb替换Mysql 权衡之下,使用方案二.过程如下 12345678yum remove mysql*;rpm -qa|grep mysql 检查是否卸载干净了;检查/etc/yum.repos.d/目录下有没有mysql*文件,删除之yum -y install http://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm/etc/yum.repos.d/mysql-community.repo;找到 Mysql5.5 section,将enabled=0修改为1,将其他Mysql源全部关闭,如下: 1234567891011121314151617181920212223242526272829303132333435363738[mysql-connectors-community]name=MySQL Connectors Communitybaseurl=http://repo.mysql.com/yum/mysql-connectors-community/el/6/$basearch/enabled=1gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql[mysql-tools-community]name=MySQL Tools Communitybaseurl=http://repo.mysql.com/yum/mysql-tools-community/el/6/$basearch/enabled=1gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.5[mysql55-community]name=MySQL 5.5 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.5-community/el/6/$basearch/enabled=1gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Enable to use MySQL 5.6[mysql56-community]name=MySQL 5.6 Community Serverbaseurl=http://repo.mysql.com/yum/mysql-5.6-community/el/6/$basearch/enabled=0gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# Note: MySQL 5.7 is currently in development. For use at your own risk.# Please read with sub pages: https://dev.mysql.com/doc/relnotes/mysql/5.7/en/[mysql57-community-dmr]name=MySQL 5.7 Community Server Development Milestone Releasebaseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/$basearch/enabled=0gpgcheck=1gpgkey=file:/etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 接下来 1yum install mysql mysql-devel mysql-server Mysql5.5已经安装完毕,下一步,安装php的mysql扩展 123yum install php56w-mysql;不幸的事故再次发生Package 1:mariadb-libs-5.5.44-1.el7_1.x86_64 is obsoleted by mysql-community-libs-5.5.46-2.el6.x86_64 which is already installed 忽然想起我的ubuntu上php也是5.6,而Mysql正好是5.5,直接拷贝so文件过来使用,但是编译过的default_socket位置不一致,于是 12345678910111213ln -s /var/lib/mysql/mysqld.sock /var/run/mysqld/mysqld.sock;最后systemctl restart httpd.servicemysqliMysqlI Support =&gt; enabledClient API library version =&gt; 5.5.46Active Persistent Links =&gt; 0Inactive Persistent Links =&gt; 0Active Links =&gt; 0Client API header version =&gt; 5.5.44MYSQLI_SOCKET =&gt; /var/run/mysqld/mysqld.sock 问题暂时解决了! 后记: 降版本并不是最佳的选择, 以后还是要时常关注这方面的消息.或许迁移道Mariadb是更好的选择.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"待读列表","slug":"2015-待读列表","date":"2015-09-15T01:20:38.000Z","updated":"2019-12-01T10:13:39.548Z","comments":true,"path":"2015/09/15/2015-待读列表/","link":"","permalink":"https://straysh.github.life/2015/09/15/2015-%E5%BE%85%E8%AF%BB%E5%88%97%E8%A1%A8/","excerpt":"","text":"Declaring module exports (Node.js, AMD) 解释了使用exports的好处与坏处 Writing Modular JavaScript With AMD, CommonJS &amp; ES Harmony 纯粹的AMD模块方法 子容器垂直居中于父容器的方案","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"伪类::before和::after","slug":"2015-伪类-before和-after","date":"2015-09-08T01:53:01.000Z","updated":"2019-12-01T10:13:39.888Z","comments":true,"path":"2015/09/08/2015-伪类-before和-after/","link":"","permalink":"https://straysh.github.life/2015/09/08/2015-%E4%BC%AA%E7%B1%BB-before%E5%92%8C-after/","excerpt":"","text":"CSS 有两个说不上常用的伪类 :before 和 :after，偶尔会被人用来添加些自定义格式什么的，但是它们的功用不仅于此。前几天发现了 Creative Link Effects 这个非常有意思的介绍创意链接特效的页面，里面惊人的效果大量使用到的特性除了 transform 属性进行变形之外，就是接下来要介绍的这两个伪元素了。 基本用法在了解进阶的应用之前，先来了解一下语法规则。平常仅仅需要将这两个伪元素用于添加一些自定义字符时，只需使用伪类使用的单冒号写法，以保证浏览器的兼容性： 1p:before &#123;&#125; 不过，在 CSS3 中为了区别伪元素和伪类为伪元素使用了双冒号，因此如果使用了 display 或者 width 等属性时使得显示脱离了原本元素后，建议按照标准双写。过于老的浏览器可能会存在支持问题，不过伪元素大多是配合 CSS3 使用，就无所谓向下兼容了： 1img::after &#123;&#125; 这两个伪类下特有的属性 content ，用于在 CSS 渲染中向元素逻辑上的头部或尾部添加内容。注意这些添加不会改变文档内容，不会出现在 DOM 中，不可复制，仅仅是在 CSS 渲染层加入。比较有用的是以下几个值： [String] - 使用引号包括一段字符串，将会向元素内容中添加字符串。示例： 1a:after &#123; content: &quot;↗&quot;; &#125; attr() – 调用当前元素的属性，可以方便的比如将图片的 Alt 提示文字或者链接的 Href 地址显示出来。示例： 1a:after &#123; content:&quot;(&quot; attr(href) &quot;)&quot;; &#125; url() / uri() – 用于引用媒体文件。示例： 1h1::before &#123; content: url(logo.png); &#125; counter() – 调用计数器，可以不使用列表元素实现序号功能。具体请参见 counter-increment 和 counter-reset 属性的用法。示例： 1h2:before &#123; counter-increment: chapter; content: &quot;Chapter &quot; counter(chapter) &quot;. &quot; &#125; 进阶技巧清除浮动是一个时常会遇到的问题，不少人的解决办法是添加一个空的 div 应用 clear:both; 属性。现在，无需增加没有意义的元素，仅需要以下样式即可在元素尾部自动清除浮动： 12.clear-fix &#123; *overflow: hidden; *zoom: 1; &#125; .clear-fix:after &#123; display: table; content: &quot;&quot;; width: 0; clear: both; &#125; 许多人喜欢给 blockquote 引用段添加巨大的引号作为背景，这种时候我们就可以用 :before 来代替 background 了，即可以给背景留下空间，还可以直接使用文字而非图片： 123456789blockquote::before &#123; content: open-quote; position: absolute; z-index: -1; color: #DDD; font-size: 120px; font-family: serif; font-weight: bolder; &#125; 特殊妙用除了简单的添加字符，配合 CSS 强大的定位和特效特性，完全可以达到给简单的元素另外附加最多两个容器的效果。有一点需要注意的是，如果不需要内容仅配合样式属性做出效果，内容属性也不能为空，即 content:”” 。否则，其他的样式属性一概不会生效。 鼠标移上链接，出现方括号： 12345678910111213a &#123; position: relative; display: inline-block; outline: none; text-decoration: none; color: #000; font-size: 32px; padding: 5px 10px; &#125; a:hover::before, a:hover::after &#123; position: absolute; &#125; a:hover::before &#123; content: &quot;\\5B&quot;; left: -20px; &#125; a:hover::after &#123; content: &quot;\\5D&quot;; right: -20px; &#125; 同样，我们只需要配合 display: block 和 position: absolute ，就可以将其当成两个容器，拼合成悬浮出现双边框的特效： 1234567891011121314151617181920212223242526272829303132a &#123; position: relative; display: inline-block; outline: none; text-decoration: none; color: #000; font-size: 32px; padding: 0 10px; &#125; /* 大框 */ a:hover::before, a:hover::after &#123; content: &quot;&quot;; display: block; position: absolute; top: -15%%; left: -14%%; width: 120%; height: 120%; border-style: solid; border-width: 4px; border-color: #DDD; &#125; /* 小框 */ a:hover::after &#123; top: 0%; left: 0%; width: 100%; height: 100%; border-width: 2px; &#125; 参考资料 ::before (:before) ::after (:after) content","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"Mobile H5 App","slug":"2015-Mobile-H5-App","date":"2015-09-07T11:58:08.000Z","updated":"2019-12-01T10:13:39.920Z","comments":true,"path":"2015/09/07/2015-Mobile-H5-App/","link":"","permalink":"https://straysh.github.life/2015/09/07/2015-Mobile-H5-App/","excerpt":"","text":"bug report 组件","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"网站icon","slug":"2015-网站icon","date":"2015-09-07T11:37:07.000Z","updated":"2019-12-01T10:13:40.168Z","comments":true,"path":"2015/09/07/2015-网站icon/","link":"","permalink":"https://straysh.github.life/2015/09/07/2015-%E7%BD%91%E7%AB%99icon/","excerpt":"","text":"Topics icon vs shortcut icon image/vnd.microsoft.icon There are several ways to create a favicon. The best way for you depends on various factors: The time you can spend on this task. For many people, this is “as quick as possible”.The efforts you are willing to make. Like, drawing a 16x16 icon by hand for better results.Specific constraints, like supporting a specific browser with odd specs. First method: Use a favicon generatorIf you want to get the job done well and quickly, you can use a favicon generator. This one creates the pictures and HTML code for all major desktop and mobiles browsers. Full disclosure: I’m the author of this site. Advantages of such solution: it’s quick and all compatibility considerations were already addressed for you. Second method: Create a favicon.ico (desktop browsers only)As you suggest, you can create a favicon.ico file which contains 16x16 and 32x32 pictures (note that Microsoft recommends 16x16, 32x32 and 48x48). Then, declare it in your HTML code: 1&lt;link rel=&quot;shortcut icon&quot; href=&quot;/path/to/icons/favicon.ico&quot;&gt; This method will work with all desktop browsers, old and new. But most mobile browsers will ignore the favicon. About your suggestion of placing the favicon.ico file in the root and not declaring it: beware, although this technique works on most browsers, it is not 100% reliable. For example Windows Safari cannot find it (granted: this browser is somehow deprecated on Windows, but you get the point). This technique is useful when combined with PNG icons (for modern browsers). Third method: Create a favicon.ico, a PNG icon and an Apple Touch icon (all browsers)In your question, you do not mention the mobile browsers. Most of them will ignore the favicon.ico file. Although your site may be dedicated to desktop browsers, chances are that you don’t want to ignore mobile browsers altogether. You can achieve a good compatibility with: favicon.ico, see above. A 192x192 PNG icon for Android Chrome A 180x180 Apple Touch icon (for iPhone 6 Plus; other device will scale it down as needed). Declare them with 123&lt;link rel=&quot;shortcut icon&quot; href=&quot;/path/to/icons/favicon.ico&quot;&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; href=&quot;/path/to/icons/favicon-192x192.png&quot; sizes=&quot;192x192&quot;&gt;&lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;180x180&quot; href=&quot;/path/to/icons/apple-touch-icon-180x180.png&quot;&gt; This is not the full story, but it’s good enough in most cases.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"源码编译coreseek4_1","slug":"2015-源码编译coreseek4-1","date":"2015-09-07T05:49:53.000Z","updated":"2019-12-01T10:13:40.476Z","comments":true,"path":"2015/09/07/2015-源码编译coreseek4-1/","link":"","permalink":"https://straysh.github.life/2015/09/07/2015-%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91coreseek4-1/","excerpt":"","text":"准备工作1$yum install make gcc g++ gcc-c++ libtool autoconf automake imake mysql-devel libxml2-devel expat-devel 下载源代码12345$wget http://www.coreseek.cn/uploads/csft/4.0/coreseek-4.1-beta.tar.gz$tar zxf coreseek-4.1-beta.tar.gz$cd coreseek-4.1-beta$$lscsft-4.1 mmseg-3.2.14 README.txt testpack 安装mmseg123456$cd mmseg-3.2.14$./bootstrap #输出的warning信息可以忽略，如果出现error则需要解决$./configure --prefix=/usr/local/mmseg3$make$make install$cd .. 安装coreseek12$cd csft-4.1$sh buildconf.sh #按照官方文档执行该命令会报错.需要做以下修改: vim buildconf.sh, 将 &amp;&amp; automake --foreign \\ 修改为 &amp;&amp; automake --foreign --add-missing \\ vim configure.ac,将 AM_INIT_AUTOMAKE([-Wall -Werror foreign]) 修改为 AM_INIT_AUTOMAKE([-Wall foreign]), 即删除 -Werror并在 AC_PROG_RANLIB 的下一行加上 AM_PROG_AR vim src/sphinxexpr.cpp将 T val = ExprEval ( this-&gt;m_pArg, tMatch ); 修改为 T val = this-&gt;ExprEval ( this-&gt;m_pArg, tMatch );, 需修改3处 重新执行 sh buildconf.sh 1234$./configure --prefix=/usr/local/coreseek --without-unixodbc --with-mmseg --with-mmseg-includes=/usr/local/mmseg3/include/mmseg/ --with-mmseg-libs=/usr/local/mmseg3/lib/ --with-mysql$make $make install$cd .. 测试mmseg分词, coreseek搜索123456789101112131415161718$cd testpack$cat var/test/test/xml #应该正确显示中文,否则需要设置字符集zh_CN.UTF-8$mmseg -d /usr/local/mmseg3/etc var/test/test.xml #注意确保mmseg3/etc下有uni.lib, 且此处测试时test.xml引用的是相对路径$indexer -c etc/csft.conf --all$search -c etc/csft.conf 网络搜索Coreseek Fulltext 4.1 [ Sphinx 2.0.2-dev (r2922)]Copyright (c) 2007-2011,Beijing Choice Software Technologies Inc (http://www.coreseek.com) using config file &#x27;etc/csft.conf&#x27;...index &#x27;xml&#x27;: query &#x27;网络搜索 &#x27;: returned 1 matches of 1 total in 0.000 secdisplaying matches:1. document=1, weight=1590, published=Thu Apr 1 22:20:07 2010, author_id=1words:1. &#x27;网络&#x27;: 1 documents, 1 hits2. &#x27;搜索&#x27;: 2 documents, 5 hits 开发环境样本配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184## Sphinx configuration file sample## WARNING! While this sample file mentions all available options,# it contains (very) short helper descriptions only. Please refer to# doc/sphinx.html for details.################################################################################ 商家索引 data source definition#############################################################################source restaurant_src&#123; type = mysql sql_host = localhost sql_user = root sql_pass = 123456 sql_db = lifemenu_restaurant sql_port = 3306 # optional, default is 3306 sql_query_pre = SET NAMES UTF8 sql_query_range = SELECT MIN(id),MAX(id) FROM restaurant_sphinx sql_range_step = 10000# CONCAT(suburb, &#x27; &#x27;, postcode) as suburb_postcode \\ sql_query = select id, id as restaurant_id, restaurant_name, phone, price, rating, \\ city_id, address \\ from restaurant_sphinx \\ WHERE id&gt;=$start AND id&lt;=$end sql_field_string = restaurant_name sql_field_string = phone# sql_attr_multi = uint cuisine from ranged-query; \\# SELECT restaurant_id as id, cuisine_id as cuisine FROM restaurant_cuisine WHERE id&gt;=$start AND id&lt;=$end; \\# SELECT MIN(id), MAX(id) FROM restaurant_cuisine;# sql_attr_uint = country_id# sql_attr_uint = state_id sql_attr_uint = city_id# sql_attr_string = suburb_postcode sql_attr_uint = price sql_attr_float = rating sql_attr_string = address# sql_attr_uint = collected_amount# sql_attr_uint = tabletalk_amount&#125;############################################################################### index definition#############################################################################index restaurant_index&#123; source = restaurant_src path = /usr/local/coreseek/var/data/restaurant docinfo = extern mlock = 0 morphology = none min_word_len = 1 dict=crc html_strip = 0 #以下配置为中文分词核心配置 #stopwords = /path/to/stopwords.txt #uni.lib词典的制作 http://www.coreseek.cn/opensource/mmseg/ charset_dictpath = /usr/local/mmseg3/etc/ charset_type = zh_cn.utf-8 # 中文分词中以下两行必须严格一致 #charset_table = ... ngram_len = 0&#125;############################################################################### indexer settings#############################################################################indexer&#123; # memory limit, in bytes, kiloytes (16384K) or megabytes (256M) # optional, default is 128M, max is 2047M, recommended is 256M to 1024M mem_limit = 1024M # maximum IO calls per second (for I/O throttling) # optional, default is 0 (unlimited) # # max_iops = 40 # maximum IO call size, bytes (for I/O throttling) # optional, default is 0 (unlimited) # # max_iosize = 1048576 # maximum xmlpipe2 field length, bytes # optional, default is 2M # # max_xmlpipe2_field = 4M # write buffer size, bytes # several (currently up to 4) buffers will be allocated # write buffers are allocated in addition to mem_limit # optional, default is 1M # # write_buffer = 1M # maximum file field adaptive buffer size # optional, default is 8M, minimum is 1M # # max_file_field_buffer = 32M # how to handle IO errors in file fields # known values are &#x27;ignore_field&#x27;, &#x27;skip_document&#x27;, and &#x27;fail_index&#x27; # optional, default is &#x27;ignore_field&#x27; # # on_file_field_error = skip_document # how to handle syntax errors in JSON attributes # known values are &#x27;ignore_attr&#x27; and &#x27;fail_index&#x27; # optional, default is &#x27;ignore_attr&#x27; # # on_json_attr_error = fail_index # whether to auto-convert numeric values from strings in JSON attributes # with auto-conversion, string value with actually numeric data # (as in &#123;&quot;key&quot;:&quot;12345&quot;&#125;) gets stored as a number, rather than string # optional, allowed values are 0 and 1, default is 0 (do not convert) # # json_autoconv_numbers = 1 # whether and how to auto-convert key names in JSON attributes # known value is &#x27;lowercase&#x27; # optional, default is unspecified (do nothing) # # json_autoconv_keynames = lowercase # lemmatizer cache size # improves the indexing time when the lemmatization is enabled # optional, default is 256K # # lemmatizer_cache = 512M&#125;############################################################################### searchd settings#############################################################################searchd&#123; listen = 9312 listen = 127.0.0.1:9306:mysql41 listen = 192.168.1.125:9307:mysql41 read_timeout = 5 max_children = 30 max_matches = 1000 seamless_rotate = 0 preopen_indexes = 0 unlink_old = 1 #query_log_format = sphinxql pid_file = /usr/local/coreseek/var/log/searchd.pid log = /usr/local/coreseek/var/log/searchd.log query_log = /usr/local/coreseek/var/log/query.log binlog_path = # disable binlog #覆盖默认参数,因为以下参数已经废弃 compat_sphinxql_magics = 0&#125;############################################################################### common settings############################################################################## --eof-- 参考资料: coreseek常见问题","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Sphinx","slug":"Sphinx","permalink":"https://straysh.github.life/tags/Sphinx/"}]},{"title":"Flexbox","slug":"2015-Flexbox","date":"2015-08-29T07:01:31.000Z","updated":"2019-12-01T10:13:39.636Z","comments":true,"path":"2015/08/29/2015-Flexbox/","link":"","permalink":"https://straysh.github.life/2015/08/29/2015-Flexbox/","excerpt":"","text":"一个完整的Flexbox指南 Flexbox，更优雅的布局 flexbox：更加优雅的Web布局 CSS3 flexbox弹性布局实例 A Complete Guide to Flexbox A Complete Guide to Flexbox CSS3 flexbox弹性布局实例 flexbox：更加优雅的Web布局 Flexbox，更优雅的布局","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"抢购系统","slug":"2015-抢购系统","date":"2015-08-07T07:14:47.000Z","updated":"2019-12-01T10:13:39.372Z","comments":true,"path":"2015/08/07/2015-抢购系统/","link":"","permalink":"https://straysh.github.life/2015/08/07/2015-%E6%8A%A2%E8%B4%AD%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"http://blog.csdn.net/array7/article/details/46477423","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"wordcloud","slug":"2015-wordcloud","date":"2015-08-07T06:48:08.000Z","updated":"2019-12-01T10:13:39.780Z","comments":true,"path":"2015/08/07/2015-wordcloud/","link":"","permalink":"https://straysh.github.life/2015/08/07/2015-wordcloud/","excerpt":"","text":"http://zjdian.com/2014/10/24/2014-10-24-r-package-wordcloud/http://www.niubua.com/?p=1684&amp;utm_source=tuicool 1、An Introduction to Rhttp://cran.r-project.org/doc/manuals/R-intro.html#Preface 2、simpleR Using R for Introductory Statisticshttp://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf 3、Using R for Data Analysis and Graphicshttp://wwwmaths.anu.edu.au/~johnm/r/usingR.pdf 4、R数据源http://wwwmaths.anu.edu.au/~johnm/r/dsets/ 5、R上做spatial analysishttp://scc.stat.ucla.edu/page_attachments/0000/0094/spatial_R_1_09S.pdf 6、Workshop: Applied Spatial Statistics in Rhttp://www.people.fas.harvard.edu/~zhukov/spatial.html 7、The R Project for Statistical Computinghttp://www.r-project.org/ 8、R-Project 中文网http://www.rproject.cn/ 9、The R Project for Statistical Computinghttp://www.r-project.org/ 10、UCLA学习资源http://www.ats.ucla.edu/stat/ 11、学习资料http://ftp.ctex.org/mirrors/CRAN/other-docs.html 12、谢益辉http://yihui.name/cn/ 13、格物堂http://yishuo.org/ 14、R作图-现代统计图形.pdfhttp://ishare.iask.sina.com.cn/f/17517158.html 15、R语言学习笔记http://panda0411.com/category/%e6%88%91%e4%b9%9f%e4%b8%8d%e7%9f%a5%e9%81%93%e6%9c%89%e4%bb%80%e4%b9%88%e7%94%a8/%e7%bb%9f%e8%ae%a1%e5%88%86%e6%9e%90/ 16、R语言：优雅、卓越的统计分析及绘图环境http://www.programmer.com.cn/10588/http://cos.name/2012/05/r-you-ready/ 17、谢益辉，郑冰(2008). R 语言的历史背景、发展历程和现状. 1st China R Conferencehttp://cos.name/wp-content/uploads/2008/12/the-history-of-r-language-and-current-developments.ppt 18、R Bloggershttp://www.r-bloggers.com/ 19、rwiki：http://rwiki.sciviews.org/doku.php 20、Rmetrics：https://www.rmetrics.org/ 21、The Omega Project for Statistical Computinghttp://www.omegahat.org/ 22、Google和facebook如何应用R进行数据挖掘http://www.chinakdd.com/article-2455MSh71C75413.html#0-tsina-1-25515-397232819ff9a47a7b7e80a40613cfe1 23、R in a Nutshell数据集：http://examples.oreilly.com/9780596801717/ 24、统计之都：http://cos.name/cn/ 25、Statistics and R for Linguists - a reading list http://szamitogepesnyelveszet.blogspot.hu/2012/08/statistics-and-r-for-linguists-reading.html","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"R","slug":"R","permalink":"https://straysh.github.life/tags/R/"}]},{"title":"Top 5 Redis use cases","slug":"2015-Top-5-Redis-use-cases","date":"2015-08-07T04:22:29.000Z","updated":"2019-12-01T10:13:40.216Z","comments":true,"path":"2015/08/07/2015-Top-5-Redis-use-cases/","link":"","permalink":"https://straysh.github.life/2015/08/07/2015-Top-5-Redis-use-cases/","excerpt":"","text":"原文: http://objectrocket.com/blog/how-to/top-5-redis-use-cases In this post, we’ll explain some of the most common Redis use cases and different characteritics that are influencing these choices. 1. Session CacheOne of the most apparent use cases for Redis is using it as a session cache. The advantages of using Redis over other session stores, such as Memcached, is that Redis offers persistence. While maintaining a cache isn’t typically mission critical with regards to consistency, most users wouldn’t exactly enjoy if all their cart sessions went away, now would they? Luckily, with the steam Redis has picked up over the years, it’s pretty easy to find documentation on how to use Redis appropriately for session caching. Even the well-known ecommerce platform Magento has a plug in for Redis! 2. Full Page Cache (FPC)Outside of your basic session tokens, Redis provides a very easy FPC platform to operate in. Going back to consistency, even across restarts of Redis instances, with disk persistence your users won’t see a decrease in speed for their page loads—a drastic change from something like PHP native FPC. To use Magento as an example again, Magento offers a plugin to utilize Redis as a full page cache backend. As well, for your WordPress users out there, Pantheon has an awesome plugin named wp-redis to help you achieve the fastest page loads you’ve ever seen! 3. QueuesTaking advantage of Redis’ in memory storage engine to do list and set operations makes it an amazing platform to use for a message queue. Interacting with Redis as a queue should feel native to anyone used to using push/pop operations with lists in programming languages such as Python. If you do a quick Google search on “Redis queues,” you’ll soon see that there are tons of open-source projects out there aimed at making Redis an awesome backend utility for all your queuing needs. Celery, as an example, has a backend using Redis as a broker that you can check out here. 4. Leaderboards/CountingRedis does an amazing job at increments and decrements since it’s in-memory. Sets and sorted sets also make our lives easier when trying to do these kinds of operations, and Redis just so happens to offer both of these data structures. So to pull the top 10 users from a sorted set—we’ll call it “user_scores”—one can simply run the following: 1ZRANGE user_scores 0 10 Of course, this is assuming you’re ranking users on an incremental score. If you wanted to return both the users and their score, you could run something such as: 1ZRANGE user_scores 0 10 WITHSCORES Agora Games has an amazing example, using Ruby, of a leaderboard using Redis as it’s datastore that can be found here. 5. Pub/SubLast (but certainly not least) is Redis’s Pub/Sub feature. The use cases for Pub/Sub are truly boundless. I’ve seen people use it for social network connections, for triggering scripts based on Pub/Sub events, and even a chat system built using Redis Pub/Sub! (No, really, check it out.) Of all the features Redis provides, I feel like this one always gets the least amount of love, even though it has so much to offer users.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://straysh.github.life/tags/Redis/"}]},{"title":"get和post获取网络文件","slug":"2015-get和post获取网络文件","date":"2015-08-07T03:02:53.000Z","updated":"2019-12-01T10:13:39.320Z","comments":true,"path":"2015/08/07/2015-get和post获取网络文件/","link":"","permalink":"https://straysh.github.life/2015/08/07/2015-get%E5%92%8Cpost%E8%8E%B7%E5%8F%96%E7%BD%91%E7%BB%9C%E6%96%87%E4%BB%B6/","excerpt":"","text":"file_get_content 以 GET 方式获取文件12$url = &#x27;http://www.straysh.com&#x27;;file_get_content($url) fopen 以 GET 方式获取文件12345678$fp = fopen($url, &#x27;r&#x27;);stream_get_meta_data($fp);while(!feof($fp))&#123; $result .= fgets($fp, 1024);&#125;var_dump($result);fclose($fp); file_get_content 以 POST 方式获取文件123456789101112$params = [&#x27;id&#x27;=&gt;&#x27;7&#x27;];$params = http_build_query($params);$options = [ &#x27;http&#x27;=&gt; [ &#x27;method&#x27; =&gt; &#x27;POST&#x27;, &#x27;header&#x27; =&gt; &#x27;Content-type: application/x-www-form-urlencoded\\r\\nContent-Length&#x27;.strlen($params).&#x27;\\r\\n&#x27;, &#x27;content&#x27; =&gt; $params ]];$context = stream_context_create($options);file_get_content($url, false, $context); fsockopen 以 GET 方式获取文件,包括header和body.注: fsockopen 需要开启 allow_url_fopen 123456789101112131415161718192021222324252627282930313233function get_url($url, $cookie=false)&#123; $url = parse_url($url); $query = $url[&#x27;path&#x27;].&#x27;?&#x27;.$url[&#x27;query&#x27;]; $fp = fsockopen($url[&#x27;host&#x27;], $url[&#x27;port&#x27;] ? $url[&#x27;port&#x27;] : 80, $errno, $err, 30); if(!$fp) return false; $request = &quot;GET &#123;$query&#125; HTTP/1.1\\r\\n&quot;; $request .= &quot;Host:&#123;$url[&#x27;host&#x27;]&#125;\\r\\n&quot;; $request .= &quot;Connection:Close\\r\\n&quot;; if($cookie) $request .= &#x27;Cookie: &#123;$cookie&#125;\\r\\n&#x27;; $request .= &quot;\\r\\n&quot;; fwrite($fp, $request); while(!feof($fp)) &#123; $result .= fgets($fp, 1024); &#125; fclose($fp); return $result;&#125;//获取body部分function getUrlHtml($url, $cookie=false)&#123; $body = false; $rawdata = get_url($url, $cookie); if($rawdata) &#123; $body = stristr($rawdata, &quot;\\r\\n\\r\\n&quot;); $body = substr($body, 4, strlen($body)); &#125; return $body;&#125; fsockopen 以 POST 方式获取文件,包括header和body.123456789101112131415161718192021222324252627$url = parse_url($url);if($referer==&#x27;&#x27;) $referer = &#x27;111&#x27;;if(!isset($url[&#x27;port&#x27;])) $url[&#x27;port&#x27;] = 80;foreach($data as $k=&gt;$v)&#123; $values[] = &quot;&#123;$k&#125;=&quot;.urlencode($v);&#125;$data_string = implode(&quot;&amp;&quot;, $values);$request = &quot;POST &#123;$url[&#x27;path&#x27;]&#125; HTTP/1.1\\r\\n&quot;;$request .= &quot;Host:$&#123;url[&#x27;host&#x27;]&#125;\\r\\n&quot;;$request .= &quot;Referer:&#123;$referer&#125;\\r\\n&quot;;$request .= &quot;Content-type:application/x-www-form-urlencoded\\r\\n&quot;;$request .= &quot;Content-length:&quot;.strlen($data_string).&quot;\\r\\n&quot;;$request .= &quot;Connection:close\\r\\n&quot;;$request .= &quot;Cookie: &#123;$cookie&#125;\\r\\n&quot;;$request .= &quot;\\r\\n&quot;;$request .= $data_string.&quot;\\r\\n&quot;;$fp = fsockopen($url[&#x27;host&#x27;], $url[&#x27;port&#x27;]);fputs($fp, $request);while(!feof($fp))&#123; $result .= fgets($fp, 1024);&#125;fclose($fp);//$result 使用curl123456$ch = curl_init();curl_setopt($ch, CURLOPT_URL, $url);curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);curl_setopt($ch, CURLOPT_CONNECTTIMEOUT, 5);$result = curl_exec($ch);curl_close($ch);","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"原型和构造函数","slug":"2015-原型和构造函数","date":"2015-08-07T02:18:26.000Z","updated":"2019-12-01T10:13:39.368Z","comments":true,"path":"2015/08/07/2015-原型和构造函数/","link":"","permalink":"https://straysh.github.life/2015/08/07/2015-%E5%8E%9F%E5%9E%8B%E5%92%8C%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0/","excerpt":"","text":"提问者：自己遇到的前端面试问题，请高手来帮忙解决。其他的感兴趣的同学也可以来看看，自己能不能答对。123456789101112131415161718var A = function() &#123;&#125;; A.prototype = &#123;&#125;; var B = &#123;&#125;; console.log(A.constructor);//Function console.log(B.constructor);//Object var a = new A(); A.prototype = &#123;&#125;; var b = new A(); b.constructor = A.constructor; console.log(a.constructor == A);//false console.log(a.constructor == b.constructor);//false console.log(a instanceof A);//false console.log(b instanceof A);//true 小弟对最后两个console.log的结果不明白，我觉得应该是 12console.log(a instanceof A);// true console.log(b instanceof A);//true 但是在浏览器中试过了，确实是上面的答案，求解，提前谢谢各位。 解答1：首先说明 instanceof 和 constructor 没有半毛钱关系，所以题主的问题有效代码如下： 123456789var A = function() &#123;&#125;; A.prototype = &#123;&#125;;// 这里的空对象为对象1 var a = new A(); A.prototype = &#123;&#125;;// 这里的空对象为对象2 var b = new A(); console.log(a instanceof A);//false console.log(b instanceof A);//true 特别注意我添加的两个注释，对象1和对象2并非同一个对象！ 再来解释instanceof，具体可以参考ECMAScript官方文档和IBM 开发者社区的解释，简而言之，instanceof运算符返回 A 的 prototype 对象是否存在 a 的原型链中。 那么上面代码就可以用下面的图示说明： 可以看到，a 的原型链上已经不存在 A 的 prototype 对象，因此console.log(a instanceof A);//false，而 b 的原型链上存在 A 的 prototype 对象，因此console.log(b instanceof A);//true 解答2： a instancof A 检查a的原型链中是否存在A.prototype 2)每一个js对象都有一个proto属性(标准表示[[prototype]]) proto是普通对象的隐式属性，在new的时候，会指向prototype所指的对象;new出来的对象是没有prototype属性的 proto实际上是某个对象的属性，而prototype则是属于构造函数的属性,prototype指向的是一个实体对象，也就是其有proto属性; 通过proto属性的串联构建了一个对象的原型访问链，起点为一个具体的对象，终点在Object.prototype,其proto( [[ prototype ]]) )为null 3)constrcutor 为对象的构造函数对象，存在于prototype对象(原型对象)中，只要不对prototype对象重新复制,constructor都指向构造函数自身 默认的构造函数为function object() 那么我们来分析下题目 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//和 function A()&#123;&#125; 相同，只不过原题通过匿名函数表达式的方式而非函数声明的方式来生成一个函数对象 var A = function() &#123;&#125;; A.prototype = &#123;&#125;; //此时对构造函数对象A的prototype属性重新复制,constructor属性不见了，此时 console.log(A.prototype.constructor == A);//false, console.log(A.prototype.constructor);// function Object() //如果不执行 A.prototype = &#123;&#125;; 那么 console.log(A.prototype.constructor == A);//true , console.log(A.prototype.constructor);// function A() //为了和第2次出现 A.prototype = &#123;&#125;;有直观的区别，我们添加以下语句 A.prototype = &#123;first:&#x27;first&#x27;&#125;; var B = &#123;&#125;; console.log(A.constructor);//Function 函数的构造函数为 function Function() console.log(B.constructor);//Object 普通object的构造函数为 function Object() var a = new A();//新建一个对象a, //此时执行 console.log(a.constructor);//function Object() console.log(a.constructor==A);//false console.log(a instanceof A);//true a.__proto__指向A构造函数的prototype对象 Object &#123;first: &quot;first&quot;&#125; A.prototype = &#123;&#125;;//修改构造函数A的prototype属性，与第1次出现有所区别，添加以下语句 A.prototype = &#123;second:&quot;second&quot;&#125;;//此时的A构造函数的prototype属性值和定义A的时候就不同了 //这个时候我们再调用 //console.log(a.constructor==A); //console.log(a instanceof A); // 会出现什么情况呢 console.log(a.constructor==A);//false console.log(a instanceof A);//false console.log( a.__proto__ );//Object &#123;first: &quot;first&quot;&#125; 和 当前A.prototype已经不同了，也就是说构造函数prototype的变话不影响已经创建的对象， //这个道理和 var b=2,a=b;console.log(a); console.log(b); b=3; ;console.log(a); console.log(b);相同 console.log(a.constructor);//function Object() console.log(A.constructor);//function Function () var b = new A();//在新建一个对象B,在修改了的A.prototype的基础上 console.log( b.__proto__ );//Object &#123;second:&quot;second&quot;&#125; b.constructor = A.constructor;//此时不会修改原型链上的constructor属性，实在对象不上添加了一个 名为constructor的普通属性，和原型上的constructor完全无关 //多说一句,读取属性值或调用方法时会涉及到原型链上属性的查找规则，设置属性不适用，直接把属性添加到b上 (会有些细小的不同，具体可参考你不知道的javascript P144 5.1.2) // b.constructor 为 Function console.log(a.constructor == A);//false a.constructor 为 Object console.log(a.constructor == b.constructor);//false b.constructor 为 Function // A.prototype -&gt; &#123;second:&quot;second&quot;&#125;; // b.__proto__ -&gt; Object &#123;second:&quot;second&quot;&#125; // a. __proto__ -&gt;Object &#123;first: &quot;first&quot;&#125; console.log(a instanceof A);//false console.log(b instanceof A);//true 解答3 首先，instanceof 到底比较的什么？instanceof 比较的是否能在实例的原型对象链中找到 与构造函数（第二个参数）的prototype属性所指向的原型对象，能找到就返回true，反之false； 方法A的原型被篡改为 Object （A.prototype = {}）经过这一步之后， 实例 a.[[proto]] = function A(){}.prototype !!//注意，此时这个加粗的prototype已经变成了{}了! 而constructor是原型对象的属性，所以 a.constructor == function Object(){} !!// {}根据自身的原型链找到 A.[[proto]] = function Function(){}.prototype // A.constructor = function Function(){} // 显然 A.constructor != a.constructor ; 下一个比较就同理了。 解答4constructor是挂在prototype下的，当A.prototype={}的时候，constructor被删除了。所以a是false,而b又从新设置了constructor指向A,这时候b是true","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"bash FAQ","slug":"2015-bash-FAQ","date":"2015-08-06T02:16:31.000Z","updated":"2020-02-26T03:28:45.509Z","comments":true,"path":"2015/08/06/2015-bash-FAQ/","link":"","permalink":"https://straysh.github.life/2015/08/06/2015-bash-FAQ/","excerpt":"","text":"pstree 1234567891011121314151617181920straysh ~]$ps -ef|grep nginxroot 7342 1962 0 10:11 ? 00:00:00 nginx: master process nginxwww 7343 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7344 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7345 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7346 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7347 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7348 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7349 7342 0 10:11 ? 00:00:00 nginx: worker processwww 7350 7342 0 10:11 ? 00:00:00 nginx: worker processstraysh 7595 7328 0 10:17 pts/4 00:00:00 grep --color=auto nginxstraysh ~]$pstree -ph 7342nginx(7342)─┬─nginx(7343) ├─nginx(7344) ├─nginx(7345) ├─nginx(7346) ├─nginx(7347) ├─nginx(7348) ├─nginx(7349) └─nginx(7350) ps 123straysh ~]$ps -Lf 1656UID PID PPID LWP C NLWP STIME TTY STAT TIME CMDroot 1656 1 1656 0 1 08:52 ? Ss 0:00 /usr/sbin/apache2 -k start pstack 123456789$ pstack 4551Thread 7 (Thread 1084229984 (LWP 4552)):#0 0x000000302afc63dc in epoll_wait () from /lib64/tls/libc.so.6#1 0x00000000006f0730 in ub::EPollEx::poll ()#2 0x00000000006f172a in ub::NetReactor::callback ()#3 0x00000000006fbbbb in ub::UBTask::CALLBACK ()#4 0x000000302b80610a in start_thread () from /lib64/tls/libpthread.so.0#5 0x000000302afc6003 in clone () from /lib64/tls/libc.so.6#6 0x0000000000000000 in ?? () find and mv 1find cache_bak/ -type f |xargs -i mv &#x27;&#123;&#125;&#x27; cache/ quick delete files 1find . -type f -d 重命名文件 1rename -v &#x27;s/1_//&#x27; *.html 去除重复的行 1awk &#x27;!x[$0]++&#x27; 8.txt &gt; 8_no_dup.txt 删除文件 1find . -type f -exec grep -H &#x27;您要查看的商户不存在&#x27; &#123;&#125; \\; -delete 从文本中匹配特定的字符串 1curl -s &quot;http://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=0&amp;rsv_idx=1&amp;tn=baidu&amp;wd=热干面&amp;rsv_pq=bd5722370004bbe0&amp;rsv_t=5d19sLQJWuSZFFDWWaH%2Bd%2BVzn9pzgmbLu23Z%2FnUrfAjj%2FaEyTSFswdcEbX0&amp;rsv_enter=1&amp;rsv_sug3=12&amp;rsv_sug1=2&amp;rsv_sug2=0&amp;inputT=3330&amp;rsv_sug4=3628&quot; | sed -nr &#x27;s/.*百度为您找到相关结果约(.*)个.*/\\1/p&#x27; 字符串替换 1234#http://tldp.org/LDP/abs/html/string-manipulation.htmlresult=1,280,000result=$&#123;result//,/&#125;echo $&#123;result&#125; #1280000","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"浮动","slug":"2015-浮动","date":"2015-08-04T07:59:59.000Z","updated":"2019-12-01T10:13:39.456Z","comments":true,"path":"2015/08/04/2015-浮动/","link":"","permalink":"https://straysh.github.life/2015/08/04/2015-%E6%B5%AE%E5%8A%A8/","excerpt":"","text":"浮动的定义A float is a box that is shifted to the left or right on the current line. The most interesting characteristic of a float (or “floated” or “floating” box) is that content may flow along its side (or be prohibited from doing so by the “clear” property). Content flows down the right side of a left-floated box and down the left side of a right-floated box. 浮动优先清除浮动","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"js动画","slug":"2015-js动画","date":"2015-08-04T06:45:25.000Z","updated":"2019-12-01T10:13:40.272Z","comments":true,"path":"2015/08/04/2015-js动画/","link":"","permalink":"https://straysh.github.life/2015/08/04/2015-js%E5%8A%A8%E7%94%BB/","excerpt":"","text":"游戏动画，Flash动画里一个比较重要的概念是 帧频 ，即每秒播放多少帧动画，一般动画是30帧/秒，单位为fps（frames per second）。 对于匀速运动来说：如果一个动画的持续时间duration为500ms，帧频frequence为30fps，则总帧数frames为 (500/1000)*30 = 15 ，即该动画过程有15个“画面”，每走一帧，都计算出一个画面： 画面当前位置 = 开始位置 + (当前帧/总帧数)(结束位置-开始位置) ，如果当前帧是最后一帧，则动画结束。其中setTimeout或setInterval每隔 (500/15)ms 时间段调用一次函数，即计算一个画面。 来看下线性运动Linear缓动算法函数，t表示当前帧，b表示开始位置，c表示发生偏移的距离值，即当前位置-开始位置，d表示总帧数，符合上面的推理解释，对于其他的算法函数，道理其实都是一样，只不过在运动过程中的曲线不同，有些呈现抛物线，有些呈现线性指数，对于数学感兴趣的可以研究下这些算法函数，我也是略知皮毛： 123Linear: function (t, b, c, d) &#123; return c * t / d + b;&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"SVN_FAQ","slug":"2015-SVN-FAQ","date":"2015-08-04T02:19:56.000Z","updated":"2019-12-01T10:13:39.360Z","comments":true,"path":"2015/08/04/2015-SVN-FAQ/","link":"","permalink":"https://straysh.github.life/2015/08/04/2015-SVN-FAQ/","excerpt":"","text":"从trunk向branch合并12cd /branchsvn merge ^/trunk 从branch合并到trunk1svn merge -rooxx:HEAD ^/branch/abcd ^/trunk 回滚一个文件到指定版本1svn revert -r125:123 foo.php 撤销所有修改1svn revert -R .","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"SVN","slug":"SVN","permalink":"https://straysh.github.life/tags/SVN/"}]},{"title":"apache_faqs","slug":"2015-apache-faqs","date":"2015-08-04T02:16:47.000Z","updated":"2019-12-02T06:28:14.479Z","comments":true,"path":"2015/08/04/2015-apache-faqs/","link":"","permalink":"https://straysh.github.life/2015/08/04/2015-apache-faqs/","excerpt":"","text":"反向代理12345678910111213141516171819202122232425&lt;VirtualHost *:80&gt; ... ... #反向代理设置 ProxyPass /dev http://activity.lifemenu.local:8010 ProxyPassReverse /dev http://activity.lifemenu.local:8010&lt;/VirtualHost&gt;&lt;VirtualHost *:8010&gt; ServerName activity.lifemenu.local DocumentRoot /data0/www/Lifemenu/branches/weixin_20151026/public ErrorLog /data3/logs/apache/error.log CustomLog /data3/logs/apache/access.log combined &lt;Directory /data0/www/Lifemenu/branches/weixin_20151026/public&gt; AllowOverride All Options -Indexes +FollowSymLinks -MultiViews Require all granted &lt;/Directory&gt;&lt;/VirtualHost&gt;#针对单独的ip做反向代理&lt;Location /bar&gt; Allow from 1.2.3.4 2.3.4.5 ... ProxyPass http://example.com/bar ProxyPassReverse http://example.com/bar&lt;/Location&gt;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Apache","slug":"Apache","permalink":"https://straysh.github.life/tags/Apache/"}]},{"title":"nginx+lua+shell动态剪切图片","slug":"2015-nginx-lua-shell动态剪切图片","date":"2015-08-04T02:15:03.000Z","updated":"2019-12-01T10:13:39.936Z","comments":true,"path":"2015/08/04/2015-nginx-lua-shell动态剪切图片/","link":"","permalink":"https://straysh.github.life/2015/08/04/2015-nginx-lua-shell%E5%8A%A8%E6%80%81%E5%89%AA%E5%88%87%E5%9B%BE%E7%89%87/","excerpt":"","text":"假设http://static.yumcircle.com/1.jpg这个是源图片 那么在调用缩略图时，使用http://static.yumcircle.com/1!200×100.jpg 就会生成一张200宽100高的图片 !200×100就是具体的参数 ok，下面说一下我定义的参数: 1.固定尺寸缩放(这个参数会将源图强制缩放到这个尺寸，所以可能会有所变形) !200×100 将源图缩放为宽200x高100 !200 将源图缩放为宽200x高200 !200×100-50 将源图缩放为宽200x高100 并且图片质量为50 （这个是为了给手机端使用的，因为手机端可能需要图片的size更小一些) !200-50 将源图缩放为宽200x高200 并且图片质量为50 2.等比缩放 :w200 将源图宽缩放为200，高度=原图宽高比自适应，（意思是，强制将源图的宽缩到200，高按原图比例缩放) :h200 将源图高缩放为200，宽自适应 :m200 将源图以（宽，高那个值大，以哪个为准，进行缩放，比如源图是300×400，那就会以高为准，先将高缩到200），但是如果宽高都没有达到，而不处理同时也支持 :w200-50 :h200-50 :m200-50 的图片质量 3.中心剪辑 @200×300 将源图以（宽，高那个值小，以哪个为准，进行缩放，并在缩放后的图片，以另一边中间点（就是正中间，进行剪辑） @200×300-50 同时支持图片质量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#固定大小location ~ (.*)!(\\d+)x(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public; #目标图片路径 set $destPath /data0/www/yumCircle/public; #处理类型 set $type 1; if (!-f $request_filename)&#123; #rewrite ~* /public/images/tpl-design/profile-photo-02.jpg; rewrite_by_lua_file conf/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高相等location ~ (.*)!(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 3; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高相等 质量location ~ (.*)!(\\d+)-(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 4; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高指定且等比location ~ (.*):(w|h|m)(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 5; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高指定且等比 质量location ~ (.*):(w|h|m)(\\d+)-(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 6; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高指定且剪切location ~ (.*)\\@(\\d+)x(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 7; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;#宽高指定且剪切 质量location ~ (.*)\\@(\\d+)x(\\d+)-(\\d+).(gif|jpg|jpeg|png)$ &#123; root /data0/www/yumCircle/public/images/uploaded/thumbnails; #bucketname = static set $bucketname static; #原图片路径 set $srcPath /data0/www/yumCircle/public/images/uploaded/origin; #目录图片路径 set $destPath /data0/www/yumCircle/public/images/uploaded/thumbnails; #处理类型 set $type 8; if (!-f $request_filename) &#123; rewrite_by_lua_file /data0/www/yumCircle/image_resize_thumb.lua; &#125; #expires 30d;&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://straysh.github.life/tags/Nginx/"}]},{"title":"豆瓣电影TOP250","slug":"2015-豆瓣电影TOP250","date":"2015-08-03T09:07:39.000Z","updated":"2019-12-01T10:13:39.520Z","comments":true,"path":"2015/08/03/2015-豆瓣电影TOP250/","link":"","permalink":"https://straysh.github.life/2015/08/03/2015-%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1TOP250/","excerpt":"","text":"一些深度影迷可能会想到 imdb.com (互联网电影数据库) 所采用的贝叶斯公式[见附注]，这个公式的思路就是通过每部影片的［评分人数］作为调节排序的杠杆：如果这部影片的评分人数低于一个预设值，则影片的最终得分会向全部影片的平均分拉低。 由此可见，平衡评分人数和得分，避免小众高分影片排前，是这个计算方法的出发点。可问题在于：调节整个榜单的排序主要依赖于这个［评分人数预设值］。如果它设置的很低，那么最终的排序结果，就是每部影片自身评分从高到低在排序；如果它被设置得过高，那么只适用高曝光率的影片。据说 imdb.com 的这个预设值从500一路调整到了25000，遗憾的是这个算法仍然无法很好的解决他们的问题。 附录： imdb.com 的top榜单公开公式 12345678910The formula for calculating the Top Rated 250 Titles gives a true Bayesian estimate:weighted rating (WR) = (v ÷ (v+m)) × R + (m ÷ (v+m)) × Cwhere:R = average for the movie (mean) = (Rating) -单部电影的得分v = number of votes for the movie = (votes) -单部电影的有效评分人数m = minimum votes required to be listed in the Top 250 (currently 25000) -入选top250榜单所需最低的有效评分人数C = the mean vote across the whole report (currently 7.0) -所有影片的平均分 算法工程师如何改进豆瓣电影 TOP250","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://straysh.github.life/tags/Algorithm/"}]},{"title":"The Little Redis Book","slug":"2015-The-Little-Redis-Book","date":"2015-07-22T04:47:59.000Z","updated":"2019-12-01T10:13:39.872Z","comments":true,"path":"2015/07/22/2015-The-Little-Redis-Book/","link":"","permalink":"https://straysh.github.life/2015/07/22/2015-The-Little-Redis-Book/","excerpt":"","text":"关于本书许可本书《 The Little Redis Book 》基于 Attribution-NonCommercial 3.0 Unported license。你无须为本书付款。 你可以自由的复制，分发，修改和传阅本书。但请认可该书属于作者 Karl Seguin，并请勿将本书用于任何商业目的。 你可以在以下链接查看完整的 许可文档: http://creativecommons.org/licenses/by-nc/3.0/legalcode 关于作者Karl Seguin 在多领域有着丰富经验。他参与贡献 OSS 项目, 还是技术文档撰写人而且偶尔做做演讲。他写了许多关于 Redis 的文档，以及一些工具。他用 Redis，为休闲游戏开发者写了一个免费的评级和统计服务: mogade.com. Karl 还编写了 The Little MongoDB Book，一本关于 MongoDB 的免费且流行的好书。1 你可以在 http://openmymind.net 找到他的 Blog，或者通过 @karlseguin在 Twitter 上关注他。 鸣谢特别感谢 Perry Neal， 赐予我你的视野，精神，和热情。你赐予了我无尽的力量。感恩。 最新版本本书最新代码可以在这里获得:http://github.com/karlseguin/the-little-redis-book 中文版本Karl 在 the-little-redis-book 的 Github 链接中给出了 JasonLai256 的 the-little-redis-book 链接。但貌似 JasonLai256 最后一次更新是2012年了。内容上也和原文稍微有点出入，并且由于本人水平有限，无法提交自信正确的内容。因此重开一项目。如果你被搜索引擎引导到本工程，在此向你致歉，并希望有能力者且有时间者一同完善和同步本工程。你可以通过我的 邮箱 &#103;&#101;&#109;&#x69;&#110;&#105;&#x79;&#x65;&#108;&#108;&#x6f;&#119;&#x40;&#103;&#x6d;&#97;&#x69;&#108;&#x2e;&#99;&#x6f;&#109; 来联系我，或者通过 @geminiyellow 在 Twitter 上关注我。 最新中文版本基于在 karlseguin Feb 9, 2014 提交的 #36 SHA 是：3584df2c55ddcf9e2e3c06b3bd2d21723d3b1a54 简介在过去的几年中，在数据持久化和查询领域，技术和工具以惊人的速度在发展。可以这样说，终于再也不是关系型数据库独霸天下了，也就是说，数据库生态圈开始繁荣起来。 在众多的解决案和工具里面，对于我来说，Redis 是最激动人心的。为什么？首先因为它太容易学了。要掌握 Redis，用小时做单位足矣。其次，它在处理同一类问题的时候，用的方法基本类似。什么意思？Redis 并没有试图解决关于数据的一切问题。当你了解 Reids 之后，它能做什么不能做什么一目了然。当可以用它来做的时候，作为开发者，实在太爽了。 虽然你可以只用 Redis 结构件一个完整的系统，我想大多数人都会发现作为通用数据解决案的补充会更合适 - 不管是传统的关系型数据库，面向文档系统，或者是其他什么。它是那种用来实现特定功能的解决案。就是说，它更类似于一个索引引擎。你不会把你的整个应用都构筑在 Lucene 上。但当你需要一个好的搜索引擎的时候，它会提供更好的体验 - 不管对你还是你的用户。当然，这和 Redis 之于索引引擎之间的关系类似。 本书的目的在于为你掌握 Redis 打好基础。我们将把重点放在学习使用 Redis 的五种数据结构以及各种数据建模方法上。我们还会涉及一些关键的管理细节和调试技术。 开始大家的学习方式不一样: 有些人喜欢动手实践，有些人喜欢看视频，还有些喜欢读文章。但是要理解 Redis ，没有什么会比动手实践更有效了。Redis 的安装非常容易，还有一个简单的 shell，我们可以在上面实现一切。让我们花几分钟将它安装到你机器上并运行起来。 Windows 环境Redis 本身并不正式支持 Windows ，但也有可用选项。你当然是不会把它用到生产环境上的，但用在开发的时候，我从没遇到过什么限制。 来自微软开源技术公司(Microsoft Open Technologies, Inc.)的副本在这里 https://github.com/MSOpenTech/redis。同样,该解决案并不是为了生产环境准备的。 另一个方案，已经有一段时间了，在https://github.com/dmajkic/redis/downloads。你可以下载最新版本(应该是列表中最上面一个)。解压 zip 文件，根据你的环境架构，选择使用 64bit 或 32bit *nix 和 MacOSX 环境对于 *nix 和 Mac 用户，users, 从源码编译应该是你最好的选择。该版本，最新可用版本，在这里下载 http://redis.io/download。 编写本书时，最新版本是 2.6.2；用以下命令安装该版本: 1234wget http://redis.googlecode.com/files/redis-2.6.2.tar.gztar xzf redis-2.6.2.tar.gzcd redis-2.6.2make (当然，Redis 也可以通过各种包管理工具安装。比如说，MacOSX 用户通过 Homebrew 安装,只需要简单输入 brew install redis即可。) 如果你从源码编译，二进制文件被放在 src 文件夹下。进入 src 文件夹，通过 cd src。 运行和链接 Redis如果一切正常，一套可用的 Redis 二进制文件将在你手中诞生。Redis 有一套可执行文件。我们主要使用 Redis 服务和 Redis 命令行界面 (Redis-cli，一个类 DOS 客户端)。先让我们启动服务。在 Window 上，双击 redis-server。在 *nix/MacOSX 上，执行 ./redis-server。 如果你读一下启动信息，你会看到有个警告是关于 redis.conf 文件找不到的。Redis 会转而使用内建的默认项，这对我们接下来的学习毫无影响。 下一步，打开 Redis 控制台，双击 redis-cli (Windows) 或者执行 ./redis-cli (*nix/MacOSX)。它将会链接到本地运行的默认服务端口上 (6379)。 你可以测试一下是否所有运转正常，在命令行界面输入 info 。你应该会看到一大堆的键值对，它提供了大量的关于服务状态的信息。 如果你的安装有问题，我建议你到official Redis support group去寻求帮助。 Redis 驱动很快你就会学到，Redis 的 API 描述做得非常好，就像代码中的一组方法一样。它非常简单并易于编程。也就是说，不管你是用命令行工具，或者用你喜欢的语言，所做的事情基本类似。因此，如果你想从一个编程语言开始学习它，完全没有问题。如果你想的话，去 client page 下载相应的驱动。 第一章 - 基础知识Redis 有什么特别之处？它主要用来解决什么类型的问题？在使用过程中，开发者应该注意什么问题？在我们开始回答这些问题之前，首先让我们来了解一下，Redis 是什么。 Redis 通常被描述为一个基于内存的，可持久化的，键值对方式的存储。我觉得这个描述不太准确。Redis 确实把所有的数据都放到内存中(稍后详述)，并且它确实可以把数据写到硬盘上用以持久化，但是它不单单是一个简单的键值对存储。纠正这种误解是非常重要的，否则你对 Redis 的看法，以及对它所能解决问题的能力的理解就会变得狭隘起来。 实际上，在 Redis 提供的五种不同的数据结构中，只有一种是典型的键值对结构。深刻理解这五种数据结构，它们的工作原理，它们提供的方法，以及怎样用这些数据结构去建模，是学习理解 Redis 的关键。 首先，让我们来弄明白，这些数据结构的具体含义。 如果我们把数据结构这个概念放到关系型数据库世界的话，那么我们可以说，关系型数据库提供了唯一一种数据结构 - 表。表又复杂又灵活。基本没有什么问题是表不能处理的，包括建模，存储或者是管理数据。然而，这种通用性也不是没有缺点。比如说，并不是所有东西都是那么简单，不是那么快捷，看起来像它应有的样子一样。就算可以，我们也不会用一个大而全的结构，我们不是通常会用更小更专的结构吗？确实有些东西我们做不了(或者说，做得不好)，但是可以肯定的是这样做我们可以得到简单和快速，对吧？ 具体问题具体分析？我们不就是这样写代码的吗？你当然不会对所有数据都套用哈希表，也不会用 scalar 变量。对我来说，这正是 Redis 的做法。如果你要处理 scalars, lists, hashes, 或者 sets, 为什么不把他们直接存为 scalars, lists, hashes 和 sets？为什么仅仅是为了确认存在值，要去调用比 exists(key) 更复杂的方式或者要用比 O(1) (不管数据量有多少，查询的时间都是固定不变的)更慢的方式？ The Building Blocks数据库(Databases)Redis 对数据库的定义和你熟知的概念是一致的。数据库中包含一组数据。典型的数据库用例是，把所有应用的数据都集中起来，但是以应用为单位把数据分隔保存。 在 Redis 中，数据库定位非常简单，通过一个标识数字，默认开始标识是 0。如果你想切换到不同的数据库，你可以通过使用 select 命令。在命令行界面，输入 select 1。Redis 会响应一个 OK 信息然后你的提示符应该会变成类似 redis 127.0.0.1:6379[1]&gt; 这样。如果你想切回默认数据库，只要在命令行界面输入 select 0 就可以了。 命令，关键字和值(Commands, Keys and Values)虽然 Redis 不单是一个键值对存储，但是其核心，Redis 提供的五种数据结构至少都有一个 key 和一个 value。在我们开始更深入的讨论之前，理解 key 和 value 是非常重要的。 Key 定义了如何标识数据块。我们以后将会经常和 Key 打交道，但是现在，只要知道 key 看起来应该有像 users:leto 这样的格式就可以了。这样一个 key 一看就知道这条数据中有一个叫 leto 的用户的相关信息。冒号没什么意义，不过对 Redis 来说，用符号分隔 key 是一般常用方式。 Values 表示 key 的实际数据。它们可以是任何类型。你可以存储字符串，整数，或序列化对象(以 JSON, XML 或者其他什么格式)。大多数情况下，Redis 会把 value 作为字节数组对待，并不关心内容到底是什么。注意，使用驱动不一样处理序列化方式可能也不一样(有些会让你自己处理)，因此本书我们只讨论字符串，整数和 JSON。 让我们开始动手试试。输入下列命令: set users:leto &#39;&#123;&quot;name&quot;: &quot;leto&quot;, &quot;planet&quot;: &quot;dune&quot;, &quot;likes&quot;: [&quot;spice&quot;]&#125;&#39; 这是一个基本的 Redis 命令。首先我们实际执行的命令，在这里是 set。然后是它的参数。set 命令有两个参数: 我们设定的 key 和为 key 设置的 value。大多数情况下，不过不是所有，命令通常都带 key 参数(存在情况下，通常会是第一个)。猜猜怎么拿到刚才的值？你肯定知道(不知道嘛也没关系!): get users:leto 继续试试其他组合。Key 和 Value 是最基本的概念，get 和 set 命令是对它们最简单的操作。创建更多的 users，尝试不同类型的 key 和不同的 value。 查询(Querying)随着学习深入，有两件事变得越来越清楚。对 Redis 来说，key 是全部，而 value 无所谓。或者，换个说法，Redis 不允许你查询对象的值。上面的例子中，我们不可能查询那些生活在 dune 行星上的用户。 对一些人来说，这可能会造成些许困惑。我们的世界中，数据查询是那么灵活那么强大，可是 Redis 的做法看起来太原始太不务实了。不要被这种旧观念困扰你太久。记住，Redis 不是一揽子解决案。有些东西并不属于这里(由于查询的限制)。这样，在这种观念的引导下，在面临某些问题时，你会找到新的建模方案。 我们之后会看到更多的具体例子，不过重点在于我们应该理解 Redis 的这些基本事实。这有助于我们明白为什么 value 可以是任何类型 - Redis 根本不需要去读取或者理解他们。同样，这会帮助我们用新思维在这新世界考虑新的建模方案。 内存和持久化(Memory and Persistence)之前我们提到过，Redis 是一个基于内存的持久化存储。对于持久化，默认情况下，Redis 基于一定量 key 的变更，来触发对数据库进行快照，保存到硬盘上。你可以配置它，比如每 Y 秒钟内，如果有 X 个 key 改变了，那么将数据保存下来。默认情况下，Redis 会在每 60 秒，如果有 1000 及以上个 key 发生改变，将对数据快照保存。或每15分钟，即使少于9个 key 发生改变，也会把数据快照保存。 另外(或者和快照一起)，Redis 支持增量模式。一旦 key 发生变化，一个增量包会更新到硬盘上。某些情况下，允许数据60秒的更新延迟，用以换取性能上的提升，是值得的，虽然有可能会发生硬件或软件异常，导致数据丢失。在某些情况下确难以接受。Redis 还有一种可选方案，我们将会在第六章看到第三种选择，将持久化任务分流到从服务器上。 至于内存，Redis 把所有的数据都保存在内存中。这说明了使用 Redis 的成本并不低: RAM 仍然还是服务器硬件中最贵的部分。 我觉得应该有些开发者对数据会占用多少空间没什么概念。莎士比亚全集大概需要 5.5MB 的存储空间。至于扩展，其他方案倾向于IO-绑定 或者CPU-绑定。这些限制(RAM 或 IO)根据数据类型和你如何去排序和查询，会要求你把数据扩展切分到更多的机器上。除非你保存巨大的媒体文件到 Redis 中，否则基于内存的存储应该没有什么问题。而对 App 来说，这是个问题，你应该会倾向于用内存-绑定来取代IO-绑定 。 Redis 还支持虚拟内存。但是，这个功能貌似是失败了(Redis 开发者自己说的)，关于它的使用已经被声明为过期了。 (另一角度看，5.5MB 大小的莎士比亚全集可以压缩到 2MB。可是 Redis 不会自动执行压缩，你需要自己处理它。因为 Redis 把 value 作为字节数组来处理，没什么理由不让你通过压缩/解压数据来换取 RAM 。) 整合(Putting It Together)我们谈到了许多高层面的话题。在深入 Redis 之前，我想做的最后一件事情是把这些话题整合起来。具体来说，包括查询限制，数据结构和 Redis 用内存保存数据的方式。 当你把三件事情整合起来的时候，你得到一个很棒的结果:速度。有些人会这样认为，”Redis 当然会快啊，把所有的东西都放在内存了。” 不过这仅仅是一方面。Redis 与其他解决案相比的闪光点在于它特别的数据结构。 有多快？这取决于多方面 - 你用的是哪个命令，数据的类型，等等。不过测量 Redis 的性能通常可以用每秒执行多少万，或者多少十万次为单位来表示。你可以自己试着执行 redis-benchmark (和 redis-server 及 redis-cli 在同一文件夹下) 来测试它。 我曾经尝试过把一组使用传统建模的代码转换到 Redis 上。一个负载测试，在关系模型中它花了五分钟跑完。而在 Redis 中，它只用了大概 150ms。当然你不能期望所有的转换都能得到那么大的收益，但是我希望这能给你一个概念，我们说的速度的改变是什么。 理解 Redis 的这个特性非常重要，因为它会影响你怎么和它进行交互。有 SQL 背景的开发者通常会最小化跟数据库之间的来回交互次数。这对所有的系统都是一个好习惯，包括 Redis。 但是，由于我们简单的数据结构，有时候为达成我们的查询目标，我们需要多次查询 Redis 服务。这种数据访问方式，刚开始的时候可能会觉得不太自然，但是对于我们所能获取的性能来说，其损失真的是微不足道。 小结虽然我们几乎把 Redis 特性都介绍一遍，展开了很宽泛的讨论。不过别担心，弄不清楚也不要紧 - 比如说查询。在下一章我们将动手做，实践找出那些你想得到答案的所有问题。 这章中我们应该明白的几个点: Keys 是用来标识数据块(values)的字符串 Values 是一串任意字节数组，Redis 不关心这个 Redis 提供了 (实现了) 五种指定数据结构 综上，这让 Redis 易于使用并且很快，但是并不适用所有场景 第二章 - 数据结构现在是时候开始学习 Redis 的五种数据结构了。我们将会解释每种数据结构到底是什么，提供了什么方法，以及它们适用于何种类型的功能/数据。 到目前为止，我们理解的 Redis 结构包括命令，key 和 value。关于数据结构我们并没有涉及。在我们使用 set 的时候，Redis 是怎么知道用了何种数据结构的？实际上所有的命令都对应到了具体的数据结构上。比如说当你用 set 你会把 value 储存为字符串数据结构。当你用 hset 你会把它储存为一个哈希。由于 Redis 的关键字集很小，所以这是完全可以掌握的。 Redis’ website 的引用文档非常好。在这里没有必要再重复一次他们已经完成的工作。我们只介绍那些在理解数据结构时必须的最重要的命令。 这里没有比实践更有意思更重要了。你可以通过 flushdb 把数据库中的数据全部擦除，所以，别害羞我的小女孩，摇起来吧！ 字符串结构(Strings)字符串是 Redis 中最基本的数据结构。当你说键值对的时候，你肯定想到的是字符串。不要被名字迷惑，如前述，你的 value 可以是任何东西。我宁愿把它叫标量(Scalars)，不过大概只有我才这样。 我们已经看过一个用字符串的一般用例了，通过 key 保存对象实例。我们以后会经常用到类似这样的用法: set users:leto &#39;&#123;&quot;name&quot;: leto, &quot;planet&quot;: dune, &quot;likes&quot;: [&quot;spice&quot;]&#125;&#39; 另外， Redis 还有一些字符串通用操作。比如 strlen &lt;key&gt; 可以用来获取 key 的对应 value 的长度; getrange &lt;key&gt; &lt;start&gt; &lt;end&gt; 返回 key 的 value 的指定范围的值; append &lt;key&gt; &lt;value&gt; 追加值到当前值上 (或者不存在的时候生成)。动手试试看，下面是我得到的结果: 12345678&gt; strlen users:leto(integer) 50&gt; getrange users:leto 31 48&quot;\\&quot;likes\\&quot;: [\\&quot;spice\\&quot;]&quot;&gt; append users:leto &quot; OVER 9000!!&quot;(integer) 62 现在，你肯定会觉得，说得好，但这毫无意义。光从 JSON 中抽出一段范围或者追加一个值完全没有意义。你说得对，这里的想说明的是，一些命令，特别是对于字符串类型数据结构，只有在指定类型的数据中才有意义。 原先我们讲过，Redis 不关心你的值是什么。多数情况下这是对的。但是，一小部分字符串命令对于某些类型或结构的值非常有用。比如说，我们可以上面的 append 和 getrange 命令，在处理一些 custom space-efficient serialization 的时候非常有用。一个更具体的例子，你可是试试看 incr, incrby, decr 和 decrby 命令。下面字符串的值进行增减操作: 123456789&gt; incr stats:page:about(integer) 1&gt; incr stats:page:about(integer) 2&gt; incrby ratings:video:12333 5(integer) 5&gt; incrby ratings:video:12333 3(integer) 8 如你所想，Redis 的字符串结构对于分析操作非常有效。试试看 users:leto (非整形值) 会怎样 (你会拿到个异常)。 再来一个更高级的例子，setbit 和 getbit 命令。这有篇 wonderful post，关于 Spool 如何结合使用这两个命令，来高效的回答 “今天有多少独立用户访问了我” 这个问题的。一亿两千八百万用户，在笔记本上测试，50ms 内做出了回答，而且只占用了16MB的内存。 你不明白 bitmap 的工作原理没关系，不知道 Spool 怎么用这两个命令也没关系，只想让你明白 Redis 字符串操作比看起来要强大得多得多。好了话说回来，最常用的场景是我们上面给出的场景:排序(不管简单复杂)和计数。还有，因为根据 key 拿 value 超快，所以字符串结构也通常用于缓存数据。 哈希结构(Hashes)哈希结构是一个很好的例子，说明了为什么说 Redis 是个单纯的键值对存储是不对的。你看，在多数情况下，哈希结构看起来就跟字符串结构一样。但最大的不同是，它们还有另外一层中间层: 字段。所以，哈希的 set 和 get 是这样的: 12hset users:goku powerlevel 9000hget users:goku powerlevel 我们可以一次设定多个字段，一次获取多个字段，获取所有的字段和值，列出所有的字段清单或者删除指定字段: 12345hmset users:goku race saiyan age 737hmget users:goku race powerlevelhgetall users:gokuhkeys users:gokuhdel users:goku age 如你所见，相比纯字符串结构，哈希结构给了我们更多的控制权限。相比把用户单纯保存为一个序列化之后的字符串，我们可以用一个哈希做更精确的描述。好处就是你可以拉取和更新/删除指定的数据片段，而不用获取或者重写整个值。 从优化定义对象的角度出发学习哈希，比如说定义一个用户，是学习理解它的工作原理的关键。而且确实，从性能方面来看，更细颗粒的操作是必须的。那么，在下一章，我们将看看怎样用哈希结构组织数据结构以及怎样用它来优化查询。在我看来，这是哈希结构真正厉害的地方。 列表结构(Lists)列表结构可以让你，为指定的 key 保存和处理数组形式的 value 。你可以向数组插入值，获取第一个或者最后一个值，以及操作指定索引位置上的值。列表结构会维护这些值的排序，并且有基于索引的高效操作。比如我们可以创建一个 newusers 列表用来跟踪我们网站最新的注册用户: 12lpush newusers gokultrim newusers 0 49 首先我们 push 一个新用户到列表的最前面，然后我们再 trim 它，这样就只保持了最新的 50 个用户了。这是一个常见的模式。 ltrim 是一个 O(N) 操作，其中 N 是我们删除数据的数量。这个例子中，我们总是在一个单项插入之后做 trim ,所以它实际上会有一个恒定的 O(1) 性能(因为 N 总是等于 1)。 而下面这个例子，我们将第一次接触到，把 key 作为查询结果得到之后，再用于查询 value 的例子。比如我们想拿到最后 10 位用户的详细信息，我们可以这样操作: 12ids = redis.lrange(&#x27;newusers&#x27;, 0, 9)redis.mget(*ids.map &#123;|u| &quot;users:#&#123;u&#125;&quot;&#125;) 上面这个 Ruby 的小例子演示了我们之前说过的多次查询操作。 当然，列表结构的好处不单单是用来保存另外的 key 引用。value 可以是任何东西。你可以用列表结构来存储日志或者跟踪用户访问网站的路径足迹。如果你用来做游戏，你可以拿来记录玩家的动作队列。 集合结构(Sets)集合结构被用于存储唯一值，并且提供了一组基于集合的操作，比如说并集运算。集合是无序的，但是它提供了许多高效的基于值的操作。朋友圈就是最经典的使用集合的例子了: 12sadd friends:leto ghanima paul chani jessicasadd friends:duncan paul jessica alia 不管一个用户有多少个朋友，我们都可以迅速的说出 (O(1)) userX 是否是 userY 的朋友: 12sismember friends:leto jessicasismember friends:leto vladimir 而且我们可以看看是否两个或者多个用户之间是否有共同好友: 1sinter friends:leto friends:duncan 甚至直接可以把这个结果存到一个新 key 中: 1sinterstore friends:leto_duncan friends:leto friends:duncan 集合非常适用于这种难解的情况:需要标记或者跟踪那些有重复属性的值的时候(或者我们希望使用集合的交并操作的时候)。 有序集合结构(Sorted Sets)最后一个也是最强力的一个数据结构是有序集合结构。如果说哈希结构看起来像字符串结构，但是有字段，那么有序集合结构就像集合结构一样，但是有权重(score)。权重提供了排序和排名功能。如果我们想看朋友排名，我们可以这样: 1zadd friends:duncan 70 ghanima 95 paul 95 chani 75 jessica 1 vladimir 想找出 duncan 有多少朋友的权重是在 90 及以上的？ 1zcount friends:duncan 90 100 那怎么找出 chani 的排名呢？ 1zrevrank friends:duncan chani 我们用 zrevrank 来代替 zrank 是因为 Redis 默认排序是从低到高的(但这里我们需要从高到低排序)。有序集合最常见的用例就是排行榜系统了。事实上，任何你想以整数做为权重排序的东西，以及那些用权重可以很好处理的操作，都适用于有序集。 小结本章从概要层面来讲解了 Redis 的五种数据结构。使用 Redis 有一个很棒的特点就是，你能做的通常比你开始所认为的要来得多。对于 string 和 sorted sets ，肯定还有许多未被发现的用法。当你理解了正常的用例之后，你会发现 Redis 处理所有类型的问题都得心应手。还有，虽然 Redis 只提供了五种数据结构，以及相应的方法，但是不要觉得你需要把它们全用上。很少在建立一个功能的时候会这样做，只有某些很难的命令的时候才会考虑。 第三章 - 数据结构用例上一章中我们介绍了五种数据结构并针对它们适用的情况给出了一些例子。现在我们来看看更高级，更通用，的话题和设计模式。 Big O Notation在本书中，我们用 O(n) 或 O(1) 来表示 Big O notation。Big O notation 用于表示，处理某事物时基于指定处理元素的数量，将会出现怎样特定的行为。在 Redis 中，用它来表示，基于我们处理的数据的数量，命令执行的速度将会如何。 Redis 文档给出了它的每个命令的 Big O notation。还告诉我们影响性能的因素是什么。让我们来看看例子。 最快的应该是 O(1) ，一个常量。不管我们处理五条项目还是五百万条项目，都会有同样的性能。sismember 命令，用于查询一个值是否属于一个集合，就是 O(1)。sismember 是个强力的命令，很大一个原因就是快。Redis 中的大多数命令都是 O(1)。 Logarithmic, 或者说 O(log(N)), 应该是第二快的，因为它需要扫描的区间范围越来越小。通过使用这种类型的切分和处理方法，一个非常大的集合仅需要做几次迭代就会被迅速的分解。zadd 是一个 O(log(N)) 命令，N 是在有序集合中的元素个数。 之后是线性复杂度，或者说 O(N)。在表中查找没有做索引的列就是一个 O(N) 操作。就像用 ltrim 命令一样。但是，在 ltrim 中，N 不是列表的元素个数，而是要移除的元素的个数。比如用 ltrim 从有百万项目的列表中移除一条，会比从一个只有一千条项目的列表中移除十条要快。(虽然都挺快，可能快到你根本就测不到它们的差别)。 zremrangebyscore 用来从有序列表中删除那些权重在最小值和最高值之间的元素，拥有复杂度 O(log(N)+M)。这有点复杂。通过查阅文档我们可以看到 N 是集合中所有的元素的个数，而 M 是需要删除的元素的个数。也就是说，在性能方面，比起集合中所有元素的个数，需要删除的元素的个数对性能影响更明显。 sort 命令，我们在下一章会进行更详细的讨论，在这里我们要知道它有复杂度 O(N+M*log(M))。从它的性能特点来说，我们可以这样说，它是 Redis 最复杂命令中的一个。 还有另外一些复杂度，这里还有两个比较常用的是 O(N^2) 和 O(C^N)。N 越大，性能越差。Redis 没有这种复杂度的命令。 值得指出的是，Big O notation 说的是最坏情况。比如我们说某操作的复杂性是 O(N)，那我们就有可能一开始就找到它或者在最后才找到它。 Pseudo Multi Key Queries一个常见的情况是，你会想用不同关键字查到同样的值。比如说，你想查找一个用户信息，通过 email (比如说他第一次登陆的时候) 或者通过 id (当他已经登陆之后)。一个很糟糕的做法是，你用两条一样的字符串来保存冗余的用户对象: 12set users:leto@dune.gov &#x27;&#123;&quot;id&quot;: 9001, &quot;email&quot;: &quot;leto@dune.gov&quot;, ...&#125;&#x27;set users:9001 &#x27;&#123;&quot;id&quot;: 9001, &quot;email&quot;: &quot;leto@dune.gov&quot;, ...&#125;&#x27; 糟糕的原因是当你要维护这些数据的时候，这绝对是个噩梦，并且它们会占用你两倍内存。 如果 Redis 允许你把一个 key 链接到映射一个的话，那就最好不过了，可是不能(并且应该永远也不可能)。Reids 开发的一个主要驱动就是要保持代码和 API 的简洁。内部实现链接 key (还有好多我们可以用 key 来做的事情没说到呢) 毫无意义，如果你意识到 Redis 提供的另一个方案的话: 哈希结构 使用哈希结构，我们可以删除冗余内容: 12set users:9001 &#x27;&#123;&quot;id&quot;: 9001, &quot;email&quot;: &quot;leto@dune.gov&quot;, ...&#125;&#x27;hset users:lookup:email leto@dune.gov 9001 我们要做的仅仅是用字段作为伪二阶索引，并把它指向用户对象。如想通过 id 获取 用户，我们可以用普通的 get: 1get users:9001 想要通过 email 来获取用户，我们用 hget 配合 get (Ruby): 12id = redis.hget(&#x27;users:lookup:email&#x27;, &#x27;leto@dune.gov&#x27;)user = redis.get(&quot;users:#&#123;id&#125;&quot;) 这样的操作以后会经常用到，这就是哈希结构真正厉害的地方，不过如果你没这种需求，似乎这也不是一个很好的例子。 引用和索引(References and Indexes)我们已经看过许多关于怎样用一个 value 引用另一个的例子。在的列表结构的例子中看到过，在上一节的用哈希结构优化查询的例子中也看到过。总结一下就是，必须手工维护你的 value 之间的索引和引用。老实说，我觉得这真的好不爽，特别是当你想到要手工去维护/更新/删除这些引用的时候。不过在 Redis 中确实没什么好办法。 我们已经知道集合通常是怎样实现这种手工索引了: 1sadd friends:leto ghanima paul chani jessica 该集合的每个成员都指向一条保存有实际用户信息的 Redis 字符串。但如果 chani 改名了怎么办，或者删掉她的账号了怎么办？也许应该再跟踪一下反向关系: 1sadd friends_of:chani leto paul 维护另说，如果你像我这样做，肯定会被这些额外的索引值的处理和内存开销给吓到。在下一章，我们将会谈谈通过使用额外的查询次数降低性能开销(我们在第一章中已经简单的提到过了)。 如果你仔细想一下，其实关系型数据库也有一样的开销。索引会占用内存，必须扫描或者定位，然后找到需要的记录。当然这些开销被抽象得很好(他们为此作了许多优化，而且运作的非常好)。 再次，在 Redis 中手工管理引用确实不幸。但是对于你所担心的性能和内存的问题，都应该先测试一下。我想你会发现其实它不是问题。 Round Trips and Pipelining我们已经提到过，在 Redis 中，频繁访问服务器端是很常见的模式。因为有些操作你会不停的用到，值得我们去仔细看一下我们能从哪些特性中获取更多收益。 首先，许多命令都可以接收一个或者多个参数，或者有一个带有多个参数的子查询。我们早些时候看到的 mget ，带有多个 key 和可以返回多个 value: 12ids = redis.lrange(&#x27;newusers&#x27;, 0, 9)redis.mget(*ids.map &#123;|u| &quot;users:#&#123;u&#125;&quot;&#125;) 或者 sadd 命令向集合中添加一个或多个记录: 12sadd friends:vladimir pitersadd friends:paul jessica leto &quot;leto II&quot; chani Redis 也支持管道。通常，一个客户端向 Redis 发送一个请求，然后在下次请求之前会一直等待返回。而用管道你可以发送一堆请求却不用等待它们的响应。这不单降低了网络开销，也在性能上有显著提高。 值得指出的是， Redis 会用内存给命令排队，因此一个好办法是给它们做批处理。你需要根据你使用的命令来决定批处理应该有多大，更具体就是，用多大的参数。不过，如果你用的是 ~50 字符长度的 key 的话，你大约可以把批处理规模放宽到几千或者上万。 在管道中执行命令的顺序根据驱动不同而不同。比如在 Ruby 中你给 pipelined 方法传入一个代码块: 12345redis.pipelined do 9001.times do redis.incr(&#x27;powerlevel&#x27;) endend 如你所想，批处理会被管道加速。 事务(Transactions)Redis 所有的命令都是原子性的，包括那些一次可以执行多项操作的命令也一样。此外，在使用多命令的时候，Redis 支持事务。 你可能不知道，但是 Redis 确实是单线程的，这就是为什么每个命令都是原子性的原因。一次只能执行一个命令，其他的命令不能执行。(We’ll briefly talk about scaling in a later chapter.) 这在你考虑用那些一次可执行多项操作的命令时候特别有用。比如说: incr 实际上是一个 get 后面跟个 set getset 设置一个新值之后返回原值 setnx 首先检查 key 是否存在，当它不存在的时候设值 虽然这些命令很有用，但不可避免的，你肯定会遇到需要以组为单位执行多个命令的情况。首先你需要 multi 命令，然后接下来是你希望作为一组事务执行的所有命令，最后用 exec 来实际执行命令，或者用 discard 来放弃取消执行所有的命令。Redis 的事务可以保证什么？ 命令将被顺序执行 命令组将以单原子模式执行(命令组执行途中不会插入别的客户端的命令操作) 在事务中的命令，要么全部执行成功，要么全部执行失败 你可以，也应该，在命令行界面测试一下这个。Also note that there’s no reason why you can’t combine pipelining and transactions. 1234multihincrby groups:1percent balance -9000000000hincrby groups:99percent balance 9000000000exec 最后，Redis 允许你指定监视一个 key(或一组 key)，如果 key(s) 改变了，那将根据情况选择执行事务。这可以用于当你在同一个事务中需要取值，并基于取得结果执行操作的情况。上面的代码中，我们不能自己实现 incr 命令，因为命令总是在 exec 执行之后一起顺序执行。用代码来说就是，我们不能这样: 1234redis.multi()current = redis.get(&#x27;powerlevel&#x27;)redis.set(&#x27;powerlevel&#x27;, current + 1)redis.exec() 这不在 Redis 事务的责任范围之内。但是，如果我们加上 watch 给 powerlevel，我们可以这样: 12345redis.watch(&#x27;powerlevel&#x27;)current = redis.get(&#x27;powerlevel&#x27;)redis.multi()redis.set(&#x27;powerlevel&#x27;, current + 1)redis.exec() 如果另一个客户端在调用 watch 之后，改变了 powerlevel 的话，我们的事务将会失败。如果值没有变化，那么 set 将会起作用。我们可以在循环中不断执行这段代码直到它成功为止。 Keys Anti-Pattern在下一章中，我们将讨论一些和具体数据结构无关的命令。某些是关于管理或者调试工具的。不过在这里有一个我特别想说一说的是: keys 命令。该命令通过指定模式返回所有匹配的 key。这个命令看起来在某些情况下很适用，但是它绝对不应当用在产品代码中。为什么？因为它为了查找匹配的 key 会对所有的 key 做一个线性扫描。或者，简单的说，它慢死了。 那为什么有人会尝试用它？比如说你在做一个 bug 跟踪服务。每个账户有字段 id ，并且你想把每个 bug 存到一个字符串值里面去，对应的 key 看起来像 bug:account_id:bug_id。如果你需要找出一个账号下所有的 bug (显示它们，或者删除账号之后把 bug 一同删除),你应该试试 (因为我就这样!) 使用 keys 命令: 1keys bug:1233:* 好一点的解决案是用哈希结构。就像我们可以用哈希来暴露二级索引那样，所以我们也可以用它来组织我们的数据: 12hset bugs:1233 1 &#x27;&#123;&quot;id&quot;:1, &quot;account&quot;: 1233, &quot;subject&quot;: &quot;...&quot;&#125;&#x27;hset bugs:1233 2 &#x27;&#123;&quot;id&quot;:2, &quot;account&quot;: 1233, &quot;subject&quot;: &quot;...&quot;&#125;&#x27; 为了取得一个账户下的所有 bug 标识符，我们只需要调用 hkeys bugs:1233。要删除指定 bug 我们可以 hdel bugs:1233 2，要删除账户的话我们可以通过 del bugs:1233 来删除 key。 小结通过本章以及前一章，希望你已经开始有感觉知道应当怎么用 Redis 去处理实际问题了。还有很多的方式，你可以用来处理所有类型的东西，不过真正的关键是理解基础数据结构，并拥有那么一种视野，知道如何摆脱原有观念，利用它们来处理新问题。 第四章 - 数据结构以外在 Redis 五种基本数据结构以外，还有一些命令是和数据结构没有关系的。我们已经看过一些了: info, select, flushdb, multi, exec, discard, watch 和 keys。本章再来看看其他的重要的一些。 ExpirationRedis 允许你指定 key 的存活时间。你可以以 Unix 时间戳格式指定一个具体的时间 (从1970年01月01日开始的秒数)或指定以秒为单位的存活时间。这是一个基于 key 的命令，和 key 所对应的数据结构无关。 12expire pages:about 30expireat pages:about 1356933600 第一个命令会在三十秒后删除 key (当然包括关联的值) 。第二个会在2012年12月31日上午 12:00 删除 key。 这让 Redis 成为一个理想的缓存引擎。通过 ttl 命令，你可以找出一条数据还能活多久。通过 persist 命令你可以删除那些过期的数据: 12ttl pages:aboutpersist pages:about 最后，还有一个特殊的字符串命令, setex 允许你在一个单独的原子命令中设置一个字符串并指定它的存活时间 (这比什么都方便): 1setex pages:about 30 &#x27;&lt;h1&gt;about us&lt;/h1&gt;....&#x27; 发布订阅(Publication and Subscriptions)Redis 的列表结构有 blpop 和 brpop 命令，可以从列表中返回并删除第一个(或最后一个)元素，或者堵塞到有可用元素为止。这可以用于作成简单的队列。 除此之外，Redis 对发布信息/订阅频道有着一流的支持。你可以打开第二个 redis-cli 窗口自己试试。首先在第一个窗口中订阅频道。(我们假设它叫 warnings): 1subscribe warnings 命令返回你订阅的信息。然后，在另外一个窗口中，发布一条信息到 warnings 频道: 1publish warnings &quot;it&#x27;s over 9000!&quot; 如果你切回你的第一个窗口，你会发现接收到了 warnings 频道的消息。 你可以订阅多个频道 (subscribe channel1 channel2 ...)，订阅某种模式的一组频道 (psubscribe warnings:*) 或用 unsubscribe 和 punsubscribe 命令来停止监听一个，多个，或者某种模式的一组频道。 最后，注意 publish 命令的返回值 1。这是收到消息的客户端的个数。 Monitor and Slow Logmonitor 命令让你监控 Redis 的状态。它是一个很棒的调试工具，能让你深入了解你的应用是怎样和 Redis 交互的。在你的两个 redis-cli 窗口中的一个 (如果它还在订阅状态，你可以用 unsubscribe 命令或者直接关掉窗口然后再开一个新的) 输入 monitor 命令。在另一个，执行其他的任意类型的命令 (比如 get 或者 set)。你可以看到这些命令，以及它们的参数，会在第一个窗口中显示。 你应该注意不要在生产环境中使用监控命令，它就是一个调试和开发的工具而已。除此之外，没得说。它就是一个很棒的开发工具。 和 monitor一起的，Redis 还有一个 slowlog ，也是一个很棒的性能分析工具。它会记录所有执行时间超过指定 微秒 的命令。在下一章我们会概述怎样配置 Redis，不过现在你可以像这样配置 Redis ，对所有的命令做日志记录: 1config set slowlog-log-slower-than 0 然后，执行几个命令。然后你可以检索所有日志，或者最新日志，通过: 12slowlog getslowlog get 10 你可以获取 slow log 中记录条数，通过 slowlog len 对你执行的每个命令，你可以看到四个参数: 一个自增的 id 一个 Unix 时间戳，表示命令开始时间 执行时间，用微秒表示的, 记录了命令执行总时间 命令和它的参数 slow log 在内存中维护，所以在生产环境中执行，即使使用低阈值，应该也没问题。默认，它会跟踪最新 1024 条日志。 排序(Sort)sort 是Redis 最强力命令之一。它允许你对列表，集合，有序集合中的值进行排序 (有序集是依照权重排序的，而不是集合中的成员)。最简单的情况，它允许我们这样: 12rpush users:leto:guesses 5 9 10 2 4 10 19 2sort users:leto:guesses 将会返回从低到高顺序排列的值。还有一些更高级例子: 12sadd friends:ghanima leto paul chani jessica alia duncansort friends:ghanima limit 0 3 desc alpha 上面的命令演示了怎么对已排序记录分页 (通过 limit)，如何以降序返回结果 (通过 desc) 以及如何按照字典序排序而不是按照数值 (通过 alpha). sort 真正强力的地方在于它可以对基于引用的对象进行排序。之前我们演示了列表，集合和有序集合是怎样用于引用其他 Redis 对象的。sort 命令可以解引用这些关系，并且根据值进行排序。比如，假设我们有一个 bug 跟踪系统，可以让用户查看异常。我们会用一个集合来跟踪被监控的异常: 1sadd watch:leto 12339 1382 338 9338 可能通过 id 对异常进行排序很不错 (默认就是就这样做的)，可是我们也希望能按照严重度来排序的。于是，我们得告诉 Redis 用什么模式来排序。首先，让我们添加一些数据，这样可以让我们看到比较有意义的测试结果: 1234set severity:12339 3set severity:1382 2set severity:338 5set severity:9338 4 然后按照 bug 的严重度来排序，从高到低，你可以这样: 1sort watch:leto by severity:* desc Redis 会将我们指定的模式(用 by 标记部分) 中的 * ，用我们的列表/集合/有序集的值来替换。然后 Redis 会以此创建 key 名，查询实际值之后再根据结果进行排序。 虽然你可以有上百万的 key 在 Redis 中，但是我觉得上面还是有点乱了。不过好在 sort 对哈希结构和它的字段也有用。你可以利用哈希结构取代一堆顶级 key: 123456789101112131415hset bug:12339 severity 3hset bug:12339 priority 1hset bug:12339 details &#x27;&#123;&quot;id&quot;: 12339, ....&#125;&#x27;hset bug:1382 severity 2hset bug:1382 priority 2hset bug:1382 details &#x27;&#123;&quot;id&quot;: 1382, ....&#125;&#x27;hset bug:338 severity 5hset bug:338 priority 3hset bug:338 details &#x27;&#123;&quot;id&quot;: 338, ....&#125;&#x27;hset bug:9338 severity 4hset bug:9338 priority 2hset bug:9338 details &#x27;&#123;&quot;id&quot;: 9338, ....&#125;&#x27; 不单事情变简单了，可以根据 severity 或 priority排序了，我们还可以告诉 sort 我们需要取什么值: 1sort watch:leto by bug:*-&gt;priority get bug:*-&gt;details 和刚才一样有做替换操作，不过 Redis 可以识别 -&gt; 序列，用它来查找我们哈希结构中指定的字段。我们还加入了 get 命令，同样有替换操作和字段查询，用于获取 bug 的详细信息。 对于大集合，sort 可能会慢。好消息是 sort 的输出结构可以保存起来: 1sort watch:leto by bug:*-&gt;priority get bug:*-&gt;details store watch_by_priority:leto sort 的 store 功能，以及我们已经学过的 expiration 命令，可以组成一个非常棒的组合。 扫描(Scan)在上一章，我们看到了如何使用 keys 命令，它很有用，但是不应该用到生产环境中。Redis 2.8 引入了 scan 命令，它对生产环境是无害的。虽然 scan 的目的和 keys 类似，但是它们之间还是存在一些不同。说实话，大多数 不同 应当看成是 特质，但这是作为一个有用的命令所需的开销。 首先在众多的不同中的的第一个是，一次调用 scan 无需返回所有匹配结果。没什么奇怪的，就是一个被分页的结果;但是, scan 返回的结果条数不定，它不能被精确的控制。你可以用 count 选项，默认是 10，不过它完全有可能拿到比指定的 count 更多或更少的结果。 和通过使用 limit 和 offset来实现分页不同，scan 用 cursor。第一次调用 scan ，指定 0 作为游标。下面我们看看一个初始调用 scan 的例子，它指定了匹配模式 (可选) 和计数 (可选): 1scan 0 match bugs:* count 20 作为返回值的一部分，scan 返回下一个可用游标。或者，返回 0 来表示结果扫描结束。注意下一个游标的值，不代表结果的个数，也不是服务端可用的任何东西。 一个典型的流程应该看起来像这样: 1234567scan 0 match bugs:* count 2&gt; 1) &quot;3&quot;&gt; 2) 1) &quot;bugs:125&quot;scan 3 match bugs:* count 2&gt; 1) &quot;0&quot;&gt; 2) 1) &quot;bugs:124&quot;&gt; 2) &quot;bugs:123&quot; 第一次调用返回了下个游标(3)和一个结果。第二次调用，使用了这个游标(3)，返回了结束标记(0)和最后两条数据。这是个典型的流程。由于 count 只是一个提示，有可能 scan 返回下一个(非 0) cursor 时不带任何结果。也就是说，一个空结果集并不意味着没有其他的结果存在。只有一个 0 游标，才意味着没有更多的结果。 从好的一面看，站在 Redis 的角度来看， scan 是完全无状态的。因此不需要关闭游标，而且没有完全读取结果集也是无害的。如果你想，你可以随时终止遍历结果集，即使 Redis 返回一个有效的游标。 这有两点需要牢记。首先，scan 可以多次返回相同的 key 。你需要自己处理(比如说保存一个已有值集合)。其次，scan 只保证在迭代的整个持续过程中的存在值会被返回。如果在迭代中有值被添加或者被删除，新值可能被返回，旧值可能被忽略。再强调一次，这就是 scan 所谓的无状态; 它不会对存在值做快照(就像你在许多数据库中看到的那样，提供了强一致性保证)，仅仅是遍历同一块内存空间，不管空间有没有发生变更。 除了 scan ,还添加了 hscan, sscan 和 zscan 命令。这可以让你遍历哈希，集合和有序集。为什么需要这些命令？好吧，就像因为 keys 堵塞了其他所有的调用，于是有了哈希命令 hgetall 和集合命令 smembers。如果你想遍历一个非常大的哈希或集合，你可以考虑用这些命令。zscan 看起来没什么用，因为对一个有序集合分页，通过 zrangebyscore 或 zrangebyrank 已经可以达到目的。不过，如果你真的想全遍历一个大的有序集合，zscan 也不是没有价值。 小结本章主要讲了非特定数据结构命令。和其他一样，这些命令需要按需使用。不是创建一个应用或者功能时都要用到期限，发布/订阅 和/或 排序。不过最好应当知道它们的存在。并且，我们只讲了其中一部分命令。还有更多的，当你消化了本书内容之后，应当去看看完整功能列表。 第五章 - Lua 脚本Redis 2.6 开始内置 Lua 解析器，开发者可以用来为 Redis 编写更高级的查询。没错，就像你想的那样，这种功能和大多数关系型数据库提供的存储过程是一样的。 掌握该功能最难的部分是学习 Lua。好在，Lua 和大多数通用语言一样，有好的文档，有一个活跃的社区，除了写 Redis 脚本外当然还有更强大的功能。本章不会涉及 Lua 的任何细节；不过我们看几个例子，希望可以当成是一个简单的介绍。 Why?在开始学习如何使用 Lua 脚本之前，你会想，为什么需要用它。许多开发者并不喜欢传统的存储过程，这有什么不一样吗？简单的说，没有。使用不当的 Redis Lua 脚本会导致代码测试困难，逻辑和数据访问紧耦合，甚至是重复逻辑。 但是使用得当，它就是一种能力，可以简化代码提高性能。所有这些便利，很大程度上都是通过良好的组织多命令，一些简单的逻辑，结合到自定义方法中。由于 Lua 脚本执行时不能中断，因此提供了更简洁的方式来创建自己的原子性命令 (根本上避免了使用繁琐的 watch 命令)。它可以改善性能，通过移除那些需要返回的中间临时计算结果 - 最终输出结果可以在脚本中计算。 下一节给出的例子能更好的说明这些点。 Evaleval 命令包含一个 Lua 脚本参数 (字符串形式)，我们要操作的 key 组参数，及一个附加参数。让我们看看一个简单的例子 (从 Ruby 执行，因为在 Redis 的命令行工具里面执行多行命令非常不爽): 12345678910111213script = &lt;&lt;-eos local friend_names = redis.call(&#x27;smembers&#x27;, KEYS[1]) local friends = &#123;&#125; for i = 1, #friend_names do local friend_key = &#x27;user:&#x27; .. friend_names[i] local gender = redis.call(&#x27;hget&#x27;, friend_key, &#x27;gender&#x27;) if gender == ARGV[1] then table.insert(friends, redis.call(&#x27;hget&#x27;, friend_key, &#x27;details&#x27;)) end end return friendseosRedis.new.eval(script, [&#x27;friends:leto&#x27;], [&#x27;m&#x27;]) 上面的代码获取了 Leto 的所有男性朋友。注意在我们的脚本中调用 Redis 命令，需要用 redis.call(&quot;command&quot;, ARG1, ARG2, ...) 这种方式。 如果你是 Lua 新手，你应该认真看看每一行。知道下面这些对你的理解会有帮助的，比如 &#123;&#125; 创建一个空的 table (可以把它当成一个数组或者一个字典)， #TABLE 能拿到在表中的元素个数，.. 用来链接字符串。 eval 实际上应该有四个参数。第二个实际上是 key 组参数中 key 的个数；Ruby 驱动自动为我们创建了。但为什么要这样？考虑一下上面的代码在 CLI 里应该是怎样的: 123eval &quot;.....&quot; &quot;friends:leto&quot; &quot;m&quot;vseval &quot;.....&quot; 1 &quot;friends:leto&quot; &quot;m&quot; 第一种情况 (不正确) 中，Redis 怎么知道哪些是 key 而哪些是附加参数？第二种情况中，不存在多义性。 这引出了第二个问题:为什么要显式的把 key 列出来？所有 Redis 命令，在运行时，都需要确定哪些 key 是需要的。这允许以后的一些工具，比如说 Redis 集群，可以在多个 Redis 服务器中正确分发请求。你可能已经发现，我们上面的代码其实是动态读取 key 的(没把它们传给 eval)。hget 可以拿到 Leto 的所有男性朋友。这就是为什么需要把 key 列出来，当然这更多是一个建议，而不是一个硬性规则。上述代码在一个单例中会运行得很好，或者在副本中，但是肯定不会在未发行的 Redis 集群中。 脚本管理(Script Management)尽管通过 eval 执行的脚本会被 Redis 缓存起来，但是你在执行的时候，每次把整个内容发送过去看起来会很傻。或者，你可以把脚本注册到 Redis，然后通过脚本的 key 来执行。这需要你调用 script load 命令，然后拿到脚本的 SHA1 摘要: 12redis = Redis.newscript_key = redis.script(:load, &quot;THE_SCRIPT&quot;) 一旦我们加载了脚本，我们可以用 evalsha 来执行它: 1redis.evalsha(script_key, [&#x27;friends:leto&#x27;], [&#x27;m&#x27;]) 你可以用来管理 Lua 脚本的其他三个命令是 script kill, script flush 和 script exists。它们分别用来中断执行中的脚本，移除内部缓存中的所有脚本，以及查找在缓存中是否存在一个脚本。 库(Libraries)Redis 的 Lua 实现中附带了许多有用的库。尽管 table.lib, string.lib 和 math.lib 非常棒，对我来说，在这里我想单独拿出来强调的是 cjson.lib 。首先，如果你发现你需要向脚本传入多个参数的时候，以 JSON 格式将会显得更简洁: 1redis.evalsha &quot;.....&quot;, [KEY1], [JSON.fast_generate(&#123;gender: &#x27;m&#x27;, ghola: true&#125;)] 然后你可以在 Lua 脚本中反序列化: 1local arguments = cjson.decode(ARGV[1]) 当然，这个 JSON 库还可以用来解析 Redis 自己保存的值。我们上面的例子可以这样改写: 12345678910local friend_names = redis.call(&#x27;smembers&#x27;, KEYS[1])local friends = &#123;&#125;for i = 1, #friend_names do local friend_raw = redis.call(&#x27;get&#x27;, &#x27;user:&#x27; .. friend_names[i]) local friend_parsed = cjson.decode(friend_raw) if friend_parsed.gender == ARGV[1] then table.insert(friends, friend_raw) endendreturn friends 于是我们可以从保存的朋友数据本身来查找性别，而不是从指定的哈希字段。(这个解决案相当慢，我个人更喜欢原先的那个，但是它确实演示了什么另一可行方案)。 原子性(Atomic)由于 Redis 是单线程的，你不需要担心你的 Lua 脚本会被其他的 Redis 命令打断。其中一个最明显的好处就是，有 TTL 的 key 在执行中不会半路过期。如果在脚本开始的时候，key 存在，那么它会在之后一直存在 - 除非你把它删了。 Administration下一章将更详细的讨论 Redis 的管理和配置。现在，你只要简单的知道，lua-time-limit 定义了一个 Lua 脚本最长可执行时间。默认的是五秒。考虑降低它。 小结本章介绍了 Redis 的 Lua 脚本功能。和其他任何事物一样，这个功能可能会被滥用。但是，谨慎使用，用以实现你所关注的和自定义的命令时，不但可以简化你的代码，还可以提高性能。Lua 脚本和 Redis 的其他功能/命令一样:你要作的仅仅是，如果需要，在一开始就使用它，然后你会发现它会用得越来越频繁熟练。 第六章 - Administration我们的最后一章将用来讨论 Redis 使用中的一些管理方面的内容。这是一份不完全的 Redis 管理指南。我们尽可能的回答一些 Redis 新手最有可能遇到的基本问题。 Configuration当你第一次启动 Redis 服务器，它会提醒你，redis.conf 文件找不到。这个文件用于配置 Redis 的各方面。有一份对所有版本 Redis 都可用的通用 redis.conf 文档模板。该模板中包含了默认的配置选项，对于理解各种选项的作用以及选项的默认值都很有帮助。你可以在这里找到它: http://download.redis.io/redis-stable/redis.conf。 因为这个文件定义得很详细，我们不再进行重复说明。 除了通过 redis.conf 文件对 Redis 进行配置外，我们还可以通过 config set 命令来个别值进行设置。实际上，我们已经用过它了，在之前将 slowlog-log-slower-than 设置为 0 的时候。 这里还可以通过 config get 命令来显示配置中的值。该命令支持匹配模式。所以如果你想找出所有和 logging 有关的选项，可以这样: 1config get *log* AuthenticationRedis 可以配置为需要密码才可使用。通过使用 requirepass 设置 (用 redis.conf 文件或者 config set 命令)。当 requirepass 被设置(也就是密码), 客户端将需要使用 auth password 命令。 客户端被认证后，它们可以对任何的数据库用任何的命令。包括使用 flushall 命令抹除所有数据库上的所有值。通过配置，你可以重命名混淆命令来达到一定程度的安全性: 12rename-command CONFIG 5ec4db169f9d4dddacbfb0c26ea7e5efrename-command FLUSHALL 1041285018a942a4922cbf76623b741e 或者你可以禁用一个命令，通过将命令重命名为空字符串。 Size Limitations你开始用 Redis 的时候，你肯定会想知道 “我最多能用多少 key ？”，或者是想知道一个哈希结构里面最多能有多少字段(尤其当你考虑用它组织数据的时候)，或者每个列表结构或者集合能存多少元素？对每个 Redis 实例来说，所有的这些，实际的限制都达到了上亿(hundreds of millions)级别。 ReplicationRedis 支持复制，意思是说，当你把数据写到一个 Redis 实例(主服务)上的时候，一个或者多个实例(从服务)将会保持和主服务同步更新。配置从服务，可以通过修改配置文件的 slaveof 标签或者用 slaveof 命令(没有使用该配置的实例是或可以是主服务)。 复制通过把数据拷贝到不同的服务器上达到保护目的。复制还可以用于改善性能，因为读操作可以分发到从服务上。虽然可能会返回略微过期的数据，但是对于大多数应用来说，这是一个有价值的值得考虑的折中。 不过不幸的是，Redis 的复制还没提供自动故障转移。如果主服务挂了，你需要手动提升从服务。如果你希望实现 Redis 的高可用性，还是不得不考虑用传统的高可用性工具的心跳监控(heartbeat monitoring)以及用脚本自动切换当前服务器。 备份(Backups)备份 Redis 只需要简单的将 Redis 的快照拷贝到你想要的地方(S3, FTP, …)。默认的，Redis 把它的快照保存在名为 dump.rdb 的文件中。随时，你都可以 scp, ftp 或者 cp (或别的什么) 操作这个文件。 在主服务上禁用快照或者禁用增量文件(append-only file (aof))，转而让从服务去维护，这种做法并不少见。这有助于降低主服务器上的负载，并且允许在从服务上使用更积极的备份操作，而不会影响整个系统的响应速度。 扩展和 Redis 集群(Scaling and Redis Cluster)复制是那些负荷高速成长的网站用到的第一个工具。有些命令的开销比其他高(比如说 sort )，于是可以把它们放到从服务上执行，从而保持整个系统对传入的查询的响应。 此外，真正的扩展 Redis，可以归结为，横跨多个 Redis 实体(可以执行在同一个 box 中，记住，Redis 是单线程的)使用你的 key。就目前而言，这就是你需要注意的东西(虽然许多 Redis 驱动都提供了一致性哈希算法(consistent-hashing))。考虑如何将你的数据水平切分不在本书的讨论范围之内。这些事情一时半会你也还不用担心，不过不管你用什么解决案，总是需要意识到的。 这好消息是，Redis 集群中这些都可以实现。不单止提供水平扩展，包括均衡，还提供高可用性的自动失败转移。 只要你愿意花更多的时间和精力，高可用性和扩展性在今天是完全可以做到的。以后，Redis 集群会让事情变得更简单。 小结鉴于已经开始使用 Redis 的网站以及工程的数量级，毋庸置疑，Redis 已经可用于成产，并且已经用于生产中了。但是，对于某些工具，尤其是在安全性和可用性发面，仍然略显年轻。Redis 集群，我们应该很快就可以看到的，将帮助我们解决目前管理方面的一些挑战。 总结总的来说，Redis 代表了数据处理方式的简化。它剥离了众多的复杂性和抽象性，并可与其他系统一同使用。一些场合 Redis 并不是最好选择。但在某些场合，Redis 简直就是为你的数据量身定做一样。 最后回到我一开始说的: Redis 简单易学。不停的有新技术出现，你很难说哪些值得花时间去学习。当你真正认识到 Redis 的简洁所带来的好处的时候，我由衷相信，它是你和你的团队所能做到的，在学习方面，最值得的投资之一。 1 : 中文版本 the-little-mongodb-book","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://straysh.github.life/tags/Redis/"}]},{"title":"SphinxQL","slug":"2015-SphinxQL","date":"2015-07-20T05:41:17.000Z","updated":"2019-12-01T10:13:39.416Z","comments":true,"path":"2015/07/20/2015-SphinxQL/","link":"","permalink":"https://straysh.github.life/2015/07/20/2015-SphinxQL/","excerpt":"","text":"配置文件参考页底的配置文件 命令行方式1234567891011121314151617$mysql -h127.0.0.1 -P9306mysql&gt; SELECT * FROM `restaurant_index` WHERE MATCH(&#x27;(@name a)&#x27;) ORDER BY `id` ASC LIMIT 0, 10;+------+--------------------------------+--------------------------------+-------+----------+------------------+------------------+----------+----------+------------+---------+| id | name | social_name | price | rating | collected_amount | tabletalk_amount | city | state_id | country_id | cuisine |+------+--------------------------------+--------------------------------+-------+----------+------------------+------------------+----------+----------+------------+---------+| 150 | A Bite To Eat, A Drink As Well | A Bite To Eat, A Drink As Well | 2 | 4.400000 | 691623 | 885097 | Chifley | 1 | 1 | 5,25,28 || 437 | Just a Bite | Just a Bite | 1 | 0.600000 | 835971 | 913061 | Mawson | 1 | 1 | 5,6 || 886 | Wok In A Box Canberra | Wok In A Box Canberra | 1 | 0.300000 | 80784 | 124537 | Canberra | 1 | 1 | 2,12,25 || 902 | A. Baker | A. Baker | 3 | 3.300000 | 593557 | 197168 | Canberra | 1 | 1 | 7,22 || 1007 | A Hereford Beefstouw | A Hereford Beefstouw | 4 | 2.300000 | 435698 | 401660 | Adelaide | 5 | 1 | 53 || 1043 | A Mother&#x27;s Milk | A Mother&#x27;s Milk | 1 | 2.600000 | 637110 | 825623 | Unley | 5 | 1 | 5,6,85 || 1336 | Signature A Fusion Of Coffee | Signature A Fusion Of Coffee | 1 | 3.900000 | 415182 | 771096 | Adelaide | 5 | 1 | 5,6,10 || 1347 | Zero - A little slice of Italy | Zero - A little slice of Italy | 1 | 4.600000 | 269857 | 11684 | Maylands | 5 | 1 | 6,28,37 || 1500 | Thai in a Wok | Thai in a Wok | 2 | 3.800000 | 465388 | 70699 | Adelaide | 5 | 1 | 4 || 1631 | Michelangelos Dial a Pizza | Michelangelos Dial a Pizza | 1 | 4.300000 | 325638 | 339890 | Adelaide | 5 | 1 | 28 |+------+--------------------------------+--------------------------------+-------+----------+------------------+------------------+----------+----------+------------+---------+10 rows in set (0.00 sec) 使用phphttps://github.com/FoolCode/SphinxQL-Query-Builder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425## Sphinx configuration file sample## WARNING! While this sample file mentions all available options,# it contains (very) short helper descriptions only. Please refer to# doc/sphinx.html for details.################################################################################ 商家索引 data source definition#############################################################################source restaurant_src&#123; type = mysql sql_host = localhost sql_user = root sql_pass = 123456 sql_db = yumCircle sql_port = 3306 # optional, default is 3306 sql_query_pre = SET NAMES UTF8 sql_query_range = SELECT MIN(id),MAX(id) FROM restaurant sql_range_step = 10000 sql_query = select id, id as restaurant_id, name, social_name, telephone, price, rating, collected_amount, tabletalk_amount, photo_amount, \\ locality, suburb, street, city, state_id, country_id, postcode, \\ CONCAT(suburb, &#x27; &#x27;, postcode) as suburb_postcode \\ from restaurant \\ WHERE id&gt;=$start AND id&lt;=$end sql_field_string = name sql_field_string = social_name sql_attr_multi = uint cuisine from ranged-query; \\ SELECT restaurant_id as id, cuisine_id as cuisine FROM restaurant_cuisine WHERE id&gt;=$start AND id&lt;=$end; \\ SELECT MIN(id), MAX(id) FROM restaurant_cuisine; sql_attr_uint = country_id sql_attr_uint = state_id sql_attr_string = city sql_attr_string = suburb_postcode sql_attr_uint = price sql_attr_float = rating sql_attr_uint = collected_amount sql_attr_uint = tabletalk_amount&#125;############################################################################### index definition#############################################################################index restaurant_index&#123; source = restaurant_src path = /home/straysh/softwares/sphinx/var/data/restaurant docinfo = extern #dict = keywords mlock = 0 morphology = none min_word_len = 1 min_infix_len = 1 html_strip = 0&#125;############################################################################### indexer settings#############################################################################indexer&#123; # memory limit, in bytes, kiloytes (16384K) or megabytes (256M) # optional, default is 128M, max is 2047M, recommended is 256M to 1024M mem_limit = 1024M # maximum IO calls per second (for I/O throttling) # optional, default is 0 (unlimited) # # max_iops = 40 # maximum IO call size, bytes (for I/O throttling) # optional, default is 0 (unlimited) # # max_iosize = 1048576 # maximum xmlpipe2 field length, bytes # optional, default is 2M # # max_xmlpipe2_field = 4M # write buffer size, bytes # several (currently up to 4) buffers will be allocated # write buffers are allocated in addition to mem_limit # optional, default is 1M # # write_buffer = 1M # maximum file field adaptive buffer size # optional, default is 8M, minimum is 1M # # max_file_field_buffer = 32M # how to handle IO errors in file fields # known values are &#x27;ignore_field&#x27;, &#x27;skip_document&#x27;, and &#x27;fail_index&#x27; # optional, default is &#x27;ignore_field&#x27; # # on_file_field_error = skip_document # how to handle syntax errors in JSON attributes # known values are &#x27;ignore_attr&#x27; and &#x27;fail_index&#x27; # optional, default is &#x27;ignore_attr&#x27; # # on_json_attr_error = fail_index # whether to auto-convert numeric values from strings in JSON attributes # with auto-conversion, string value with actually numeric data # (as in &#123;&quot;key&quot;:&quot;12345&quot;&#125;) gets stored as a number, rather than string # optional, allowed values are 0 and 1, default is 0 (do not convert) # # json_autoconv_numbers = 1 # whether and how to auto-convert key names in JSON attributes # known value is &#x27;lowercase&#x27; # optional, default is unspecified (do nothing) # # json_autoconv_keynames = lowercase # lemmatizer cache size # improves the indexing time when the lemmatization is enabled # optional, default is 256K # # lemmatizer_cache = 512M&#125;############################################################################### searchd settings#############################################################################searchd&#123; # [hostname:]port[:protocol], or /unix/socket/path to listen on # known protocols are &#x27;sphinx&#x27; (SphinxAPI) and &#x27;mysql41&#x27; (SphinxQL) # # multi-value, multiple listen points are allowed # optional, defaults are 9312:sphinx and 9306:mysql41, as below # # listen = 127.0.0.1 # listen = 192.168.0.1:9312 # listen = 9312 # listen = /var/run/searchd.sock listen = 9312 listen = localhost:9306:mysql41 # log file, searchd run info is logged here # optional, default is &#x27;searchd.log&#x27; log = /home/straysh/softwares/sphinx/var/log/searchd.log # query log file, all search queries are logged here # optional, default is empty (do not log queries) query_log = /home/straysh/softwares/sphinx/var/log/query.log # client read timeout, seconds # optional, default is 5 read_timeout = 5 # request timeout, seconds # optional, default is 5 minutes client_timeout = 300 # maximum amount of children to fork (concurrent searches to run) # optional, default is 0 (unlimited) max_children = 30 # maximum amount of persistent connections from this master to each agent host # optional, but necessary if you use agent_persistent. It is reasonable to set the value # as max_children, or less on the agent&#x27;s hosts. persistent_connections_limit = 30 # PID file, searchd process ID file name # mandatory pid_file = /home/straysh/softwares/sphinx/var/log/searchd.pid # seamless rotate, prevents rotate stalls if precaching huge datasets # optional, default is 1 seamless_rotate = 1 # whether to forcibly preopen all indexes on startup # optional, default is 1 (preopen everything) preopen_indexes = 1 # whether to unlink .old index copies on succesful rotation. # optional, default is 1 (do unlink) unlink_old = 1 # attribute updates periodic flush timeout, seconds # updates will be automatically dumped to disk this frequently # optional, default is 0 (disable periodic flush) # # attr_flush_period = 900 # MVA updates pool size # shared between all instances of searchd, disables attr flushes! # optional, default size is 1M mva_updates_pool = 1M # max allowed network packet size # limits both query packets from clients, and responses from agents # optional, default size is 8M max_packet_size = 8M # max allowed per-query filter count # optional, default is 256 max_filters = 256 # max allowed per-filter values count # optional, default is 4096 max_filter_values = 4096 # socket listen queue length # optional, default is 5 # # listen_backlog = 5 # per-keyword read buffer size # optional, default is 256K # # read_buffer = 256K # unhinted read size (currently used when reading hits) # optional, default is 32K # # read_unhinted = 32K # max allowed per-batch query count (aka multi-query count) # optional, default is 32 max_batch_queries = 32 # max common subtree document cache size, per-query # optional, default is 0 (disable subtree optimization) # # subtree_docs_cache = 4M # max common subtree hit cache size, per-query # optional, default is 0 (disable subtree optimization) # # subtree_hits_cache = 8M # multi-processing mode (MPM) # known values are none, fork, prefork, and threads # threads is required for RT backend to work # optional, default is threads workers = threads # for RT to work # max threads to create for searching local parts of a distributed index # optional, default is 0, which means disable multi-threaded searching # should work with all MPMs (ie. does NOT require workers=threads) # # dist_threads = 4 # binlog files path; use empty string to disable binlog # optional, default is build-time configured data directory # # binlog_path = # disable logging # binlog_path = /home/straysh/softwares/sphinx/var/data # binlog.001 etc will be created there # binlog flush/sync mode # 0 means flush and sync every second # 1 means flush and sync every transaction # 2 means flush every transaction, sync every second # optional, default is 2 # # binlog_flush = 2 # binlog per-file size limit # optional, default is 128M, 0 means no limit # # binlog_max_log_size = 256M # per-thread stack size, only affects workers=threads mode # optional, default is 64K # # thread_stack = 128K # per-keyword expansion limit (for dict=keywords prefix searches) # optional, default is 0 (no limit) # # expansion_limit = 1000 # RT RAM chunks flush period # optional, default is 0 (no periodic flush) # # rt_flush_period = 900 # query log file format # optional, known values are plain and sphinxql, default is plain # # query_log_format = sphinxql # version string returned to MySQL network protocol clients # optional, default is empty (use Sphinx version) # # mysql_version_string = 5.0.37 # trusted plugin directory # optional, default is empty (disable UDFs) # # plugin_dir = /usr/local/sphinx/lib # default server-wide collation # optional, default is libc_ci # # collation_server = utf8_general_ci # server-wide locale for libc based collations # optional, default is C # # collation_libc_locale = ru_RU.UTF-8 # threaded server watchdog (only used in workers=threads mode) # optional, values are 0 and 1, default is 1 (watchdog on) # # watchdog = 1 # costs for max_predicted_time model, in (imaginary) nanoseconds # optional, default is &quot;doc=64, hit=48, skip=2048, match=64&quot; # # predicted_time_costs = doc=64, hit=48, skip=2048, match=64 # current SphinxQL state (uservars etc) serialization path # optional, default is none (do not serialize SphinxQL state) # # sphinxql_state = sphinxvars.sql # maximum RT merge thread IO calls per second, and per-call IO size # useful for throttling (the background) OPTIMIZE INDEX impact # optional, default is 0 (unlimited) # # rt_merge_iops = 40 # rt_merge_maxiosize = 1M # interval between agent mirror pings, in milliseconds # 0 means disable pings # optional, default is 1000 # # ha_ping_interval = 0 # agent mirror statistics window size, in seconds # stats older than the window size (karma) are retired # that is, they will not affect master choice of agents in any way # optional, default is 60 seconds # # ha_period_karma = 60 # delay between preforked children restarts on rotation, in milliseconds # optional, default is 0 (no delay) # # prefork_rotation_throttle = 100 # a prefix to prepend to the local file names when creating snippets # with load_files and/or load_files_scatter options # optional, default is empty # # snippets_file_prefix = /mnt/common/server1/&#125;############################################################################### common settings#############################################################################common&#123; # lemmatizer dictionaries base path # optional, defaut is /usr/local/share (see ./configure --datadir) # # lemmatizer_base = /usr/local/share/sphinx/dicts # path to RLP root directory # optional, defaut is /usr/local/share (see ./configure --datadir) # # rlp_root = /usr/local/share/sphinx/rlp # path to RLP environment file # optional, defaut is /usr/local/share/rlp-environment.xml (see ./configure --datadir) # # rlp_environment = /usr/local/share/sphinx/rlp/rlp/etc/rlp-environment.xml&#125;# --eof--","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Sphinx","slug":"Sphinx","permalink":"https://straysh.github.life/tags/Sphinx/"}]},{"title":"Sphinx进阶","slug":"2015-Sphinx进阶","date":"2015-07-20T05:40:49.000Z","updated":"2019-12-01T10:13:39.792Z","comments":true,"path":"2015/07/20/2015-Sphinx进阶/","link":"","permalink":"https://straysh.github.life/2015/07/20/2015-Sphinx%E8%BF%9B%E9%98%B6/","excerpt":"","text":"安装 在官网下载源码包 解压,然后就是常规的源码编译: 12345$ tar xzvf sphinx-2.2.8-release.tar.gz$ cd sphinx$ ./configure$ make$ make install 在执行./configure的时候,可以指定一些选项,这些选项可以通过 --help来查看.最重要的一些选项是: --prefix 指定了要将sphinx安装到何位置,例如 --prefix=/usr/local/sphinx --with-mysql 指定了在自动检测失败时,到何处可以找到MySQL的include和library文件. --with-static-mysql 将Sphinx编译为使用静态Mysql链接. --with-pgsql 使用PostgreSQL --with-static-pgsql 使用静态PostgreSQL 编者注: 本文使用SphinxQL方式,不需要在php上再编译sphinxClient扩展,而是使用php-mysql扩展. Sphinx快速使用指南假定sphinx安装于目录/usr/local/sphinx中(下文相同) 123456789101112131415161718192021$ cd /usr/local/sphinx/etc$ cp sphinx.conf.dist sphinx.conf$ vi sphinx.conf$ mysql -u test &lt; /usr/local/sphinx/etc/example.sql$ cd /usr/local/sphinx/etc$ /usr/local/sphinx/bin/indexer --all$ mysql -h0 -P9306SELECT * FROM test1 WHERE MATCH(&#x27;my document&#x27;);INSERT INTO rt VALUES (1, &#x27;this is&#x27;, &#x27;a sample text&#x27;, 11);INSERT INTO rt VALUES (2, &#x27;some more&#x27;, &#x27;text here&#x27;, 22);SELECT gid/11 FROM rt WHERE MATCH(&#x27;text&#x27;) GROUP BY gid;SELECT * FROM rt ORDER BY gid DESC;SHOW TABLES;SELECT *, WEIGHT() FROM test1 WHERE MATCH(&#x27;&quot;document one&quot;/1&#x27;);SHOW META;SET profiling=1;SELECT * FROM test1 WHERE id IN (1,2,4);SHOW PROFILE;SELECT id, id%3 idd FROM test1 WHERE MATCH(&#x27;this is | nothing&#x27;) GROUP BY idd;SHOW PROFILE;SELECT id FROM test1 WHERE MATCH(&#x27;is this a good plan?&#x27;);SHOW PLAN;SELECT COUNT(*) c, id%3 idd FROM test1 GROUP BY idd HAVING COUNT(*)&gt;1;SELECT COUNT(*) FROM test1;CALL KEYWORDS (&#x27;one two three&#x27;, &#x27;test1&#x27;);CALL KEYWORDS (&#x27;one two three&#x27;, &#x27;test1&#x27;, 1); 全文索引|Full-text fields全文本字段(或简称字段),是被Sphinx索引的文档中的文字内容,并可以通过关键字来快速搜索. 字段是命名的,可以限制只搜索一个字段(例如搜索’title’字段)或者多个字段(如仅’title’和’abstract’字段).Sphinx的索引结构最多支持256个字段.但,&lt;=2.0.1-beta,被强制限制为32个字段. 注意,字段用之来构建全文索引(一个特殊的数据结构,用来通过关键字快速搜索), 其原始内容不会存储在索引中.之后原始的文本内容就被丢弃了.这些原始的内容需要到数据源中(MySQL)中重新查询. 另外,想要重构原始文本内容是不可能的,因为指定的(配置文件中指定的)空白字符/大小写/标点符号在建立索引时都被丢失了. 属性|Attributes属性是和文档关联的额外的值,可以用来过滤和排序. 全文检索通常不仅仅是匹配文档id和相关度的排序(rank), 还要处理基于每个文档的许多参数.例如,希望先按时间再按相关度排序来显示新闻,或者在指定的价格区间搜索商品,或限制搜索指定用户发布的博客,或者按照月份分组.为了高效的做到这一点,Sphinx允许向每个文档添加一系列的属性,并在全文索引中存储这些属性的值.然后就可以使用这些值在全文搜索时来过滤/排序/分组. 属性与索引不同,他们不是全文索引的.他们存储在索引中,但不能将他们当做全文索引来搜索,尝试这样做会引发错误. 例如,假设column列是属性,则不能使用@column 1来搜索column值为1的文档. and this is still true even if the numeric digits are normally indexed. 属性可以用来做过滤,限制返回的行数,同样也可以做排序或者结果分组.简单的基于属性的排序,且不使用相关度工具是完全可行的.另外,属性会通过search daemon返回,而索引则不会. 对于属性的最佳例子是论坛的post表.假设只有title和content字段需要全文检索,但有时也需要检索特定用户或子论坛的帖子.或者按照发布日期排序.或者按照发布日期来分组检索的结果并计算每个分组的数量. 多值属性|MVA (multi-valued attributes)多值属性是Sphinx中非常重要的文档属性.多值属性可以给文档添加数值集合.用来实现文章标签/商品分类非常合适.在多值属性上可以过滤/分组(排序不能). 集合的规模是无限制的,只要内存允许你可以添加.(包含MVA的.spm文件会在searchd预缓存到内存中).MVA的数据源可以是独立的查询或者文档的字段.对于第一种方式,查询需要返回一对值:文档id和MVA值,第二种方式字段会被解析为整数.对输入的数据的顺序完全没有要求.在构建索引时,MVA值会被自动按照文档id分组(并在内部排序). 过滤时,MVA值中的任意一个符合过滤条件,文档就会被匹配.(因此,对于排他过滤器,返回的文档不会包含任何被禁止的值). TODOhttp://wenku.baidu.com/view/4ebf3505bed5b9f3f90f1caf.htmlhttp://www.douban.com/group/topic/30286342/ 试了一下，美团前台的搜索是索引了套餐的标题（包括网站编辑加上的部分）、店名、地址，其中套餐标题拆词也能搜到相应结果，而地址拆词搜不到。应该是用了两种方式，地址部分经过了分词用的是类似Xunsearch的分词索引插件，标题和店名是Sphinx之类的全文索引。 后台借了个账号看了下，单子、商家、套餐，都是按地址和标题搜索，和前台用的一个搜索接口，只是多个了ID。 考虑到Mongo对全文检索的响应速度，折中的解决方法是分词入MySQL，准确率一般，完美的方法只能是整合搜索引擎。热门、推荐、位置类别那些都好说。 商品搜索的话，这个重点在于商品页面的主题提取 和 查询推荐 再具体点就是对每个页面抽取主题、关键词 比如上面例子如果能发现页面主题是“衣服”，归类到服饰类别 然后搜索面膜时，主动就给分类到“化妆品”类了 通过在搜索结果的相关性排序中加入类别参数，不能保证完全不出现，但是可以通过权重让靠谱的结果出现在前边 ps：如果是问sphinx里面怎么操作的话，没有具体用过 直接简单的文本聚类、分类算法就行，也可以去搞一个商品分类语料库","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Sphinx","slug":"Sphinx","permalink":"https://straysh.github.life/tags/Sphinx/"}]},{"title":"Sphinx基础","slug":"2015-Sphinx基础","date":"2015-07-16T14:00:31.000Z","updated":"2019-12-01T10:13:40.316Z","comments":true,"path":"2015/07/16/2015-Sphinx基础/","link":"","permalink":"https://straysh.github.life/2015/07/16/2015-Sphinx%E5%9F%BA%E7%A1%80/","excerpt":"","text":"什么是SphinxSphinx 是SQL Phrase Index的缩写;Sphinx是一个在GPLv2下分发的全文检索引擎;一般而言，Sphinx是一个独立的全文搜索引擎;可以非常容易的与SQL数据库和脚本语言集成;搜索API支持PHP、Python、Perl、Rudy和Java，并且也可以用作MySQL存储引擎. Sphinx 的主要特性： 索引和搜索性能优异； 先进的索引和查询工具 (灵活且功能丰富的文本分析器，查询语言，以及多种不同的排序方式等等); 先进的结果集分析处理 (SELECT 可以使用表达式, WHERE, ORDER BY, GROUP BY 等对全文搜索结果集进行过滤)； 实践证实可扩展性支持数十亿文档记录，TB级别的数据，以及每秒数千次查询; 易于集成SQL和XML数据源，并可使用SphinxAPI、SphinxQL或者SphinxSE搜索接口 易于通过分布式搜索进行扩展 Sphinx 的详细特性： 高速的索引建立(在当代CPU上，峰值性能可达到10 ~ 15MB/秒); 高性能的搜索 (在1.2G文本，100万条文档上进行搜索，支持高达每秒150~250次查询); 高扩展性 (最大的索引集群超过30亿条文档，最繁忙时刻的查询峰值达到每天5千万次); 提供了优秀的相关度算法，基于短语相似度和统计（BM25）的复合Ranking方法; 支持分布式搜索功能; 提供文档片段（摘要以及高亮）生成功能; 内建支持SphinxAPI和SphinxQL搜索箭口，也可作为MySQL的存储引擎提供搜索服务; 支持布尔、短语、词语相似度等多种检索模式; 文档支持多个全文检索字段(缺省配置下，最大不超过32个); 文档支持多个额外的属性信息(例如：分组信息，时间戳等); 支持查询停止词; 支持词形学处理; 支持特殊词汇处理; 支持单一字节编码和UTF-8编码; 内建支持英语、俄语、捷克语词干化处理; 对 法语，西班牙语，葡萄牙语，意大利语，罗马尼亚语，德国，荷兰，瑞典，挪威，丹麦，芬兰，匈牙利等语言 的支持可通过第三方的 libstemmer 库 建立); 原生的MySQL支持(同时支持MyISAM 、InnoDB、NDB、Archive等所有类型的数据表 ); 原生的PostgreSQL 支持; 原生的ODBC兼容数据库支持 (MS SQL, Oracle, 等) ; …多达50多项其他功能没有在此一一列出，请参阅API和配置手册！ 数据源索引的数据可以来自各种各样不同的来源：SQL数据库、纯文本、HTML文件、邮件等等。从CoreSeek/Sphinx的视角看，索引数据是一个结构化的文档的集合，其中每个文档是字段的集合，这种结构更利于SQL数据获取，其中每一行代表一个文档，每一列代表一个字段。 由于数据来源的不同，需要不同的代码来获取数据、处理数据以供CoreSeek/Sphinx进行索引的建立。这种代码被称之为数据源驱动程序（简称：驱动或数据源）。 在本文撰写时，CoreSeek/Sphinx中包括MySQL和PostgreSQL数据源的驱动程序，这些驱动使用数据库系统提供的C/C++原生接口连接到数据库服务器并获取数据。此外，CoreSeek/Sphinx还提供了额外的被称为xmlpipe的数据源驱动，该驱动运行某个具体的命令，并从该命令的stdout中读入数据。数据的格式在 第 3.8 节 “xmlpipe 数据源” 中有介绍。经过长足的发展，Coreseek还提供了具有特色的Python数据源驱动，可以使用Python编写数据获取脚本自定义数据源，从而得以获取任何已知世界和未知世界的数据。 如果确有必要，一个索引的数据可以来自多个数据源。这些数据将严格按照配置文件中定义的顺序进行处理。所有从这些数据源获取到的文档将被合并，共同产生一个索引，如同他们来源于同一个数据源一样。 属性属性是附加在每个文档上的额外的信息（值），可以在搜索的时候用于过滤和排序。 搜索结果通常不仅仅是进行文档的匹配和相关度的排序，经常还需要根据其他与文档相关联的值，对结果进行额外的处理。例如，用户可能需要对新闻检索结果依次按日期和相关度排序，检索特定价格范围内的产品，检索某些特定用户的blog日志，或者将检索结果按月分组。为了高效地完成上述工作，Sphinx允许给文档附加一些额外的属性，并把这些值存储在全文索引中，以便在对全文匹配结果进行过滤、排序或分组时使用。 属性与字段不同，不会被全文索引。他们仅仅是被存储在索引中，属性进行全文检索式不可能的。如果要对属性进行全文检索，系统将会返回一个错误。 例如，如果column被设置为属性，就不能使用扩展表达式@column 1去匹配column为1的文档；如果数字字段按照普通的方式被索引，那么就可以这样来匹配。 属性可用于过滤，或者限制返回的数据，以及排序或者 结果分组; 也有可能是完全基于属性排序的结果, 而没有任何搜索相关功能的参与. 此外, 属性直接从搜索服务程序返回信息, 而被索引的文本内容则没有返回. 论坛帖子表是一个很好的例子。假设只有帖子的标题和内容这两个字段需要全文检索，但是有时检索结果需要被限制在某个特定的作者的帖子或者属于某个子论坛的帖子中（也就是说，只检索在SQL表的author_id和forum_id这两个列上有特定值的那些行），或者需要按post_date列对匹配的结果排序，或者根据post_date列对帖子按月份分组，并对每组中的帖子计数。 为实现这些功能，可以将上述各列（除了标题和内容列）作为属性做索引，之后即可使用API调用来设置过滤、排序和分组。以下是一个例子： 示例： sphinx.conf 片段: 1234567...sql_query = SELECT id, title, content, \\ author_id, forum_id, post_date FROM my_forum_postssql_attr_uint = author_idsql_attr_uint = forum_idsql_attr_timestamp = post_date... 示例： 应用程序代码 (使用 PHP): 12345678// 仅搜索ID为123的作者发布的内容$cl-&gt;SetFilter ( &quot;author_id&quot;, array ( 123 ) );// 仅在id为1，3，7的子论坛中搜索$cl-&gt;SetFilter ( &quot;forum_id&quot;, array ( 1,3,7 ) );// 按照发布时间倒序排列获取的结果$cl-&gt;SetSortMode ( SPH_SORT_ATTR_DESC, &quot;post_date&quot; ); 可以通过名字来指示特定的属性，并且这个名字是大小写无关的。属性并不会被全文索引，他们只是按原封不动的存储在索引文件中。目前支持的属性类型如下： 无符号整数（1-32位宽）; UNIX 时间戳（timestamps）; 浮点值（32位，IEEE 754单精度）; 字符串序列 (专指尤其计算出来的字符串序列整数值); 字符串 (版本 1.10-beta 开始支持); 多值属性 MVA(32位无符号整型值的变长序列). 由各个文档的全部的属性信息构成了一个集合，它也被称为文档信息 docinfo. 文档信息可以按如下两种方式之一存储： 与全文索引数据分开存储（“外部存储extern”，在.spa文件中存储）, 或者 在全文索引数据中，每出现一次文档ID 就出现相应的文档信息（“内联存储inline”，在.spd文件中存储） 在大多数情况下，外部存储可令建立索引和检索的效率都大幅提高。 检索时，采用外部存储方式产生的的内存需求为 (1+属性总数)文档总数4字节，也就是说，带有两个属性和一个时间戳的1千万篇文档会消耗(1+2+1)10M4 = 160 MB的RAM。这是每个检索的守护进程（PER DAEMON）消耗的量，而不是每次查询，searchd仅在启动时分配160MB的内存，读入数据并在不同的查询之间保持这些数据。子进程并不会对这些数据做额外的拷贝。 MVA (多值属性)多值属性MVA (multi-valued attributes)是文档属性的一种重要的特例，MVA使得向文档附加一系列的值作为属性的想法成为可能。这对文章的tags，产品类别等等非常有用。MVA属性支持过滤和分组（但不支持分组排序）。 目前MVA列表项的值被限制为32位无符号整数。列表的长度不受限制，只要有足够的RAM，任意个数的值都可以被附加到文档上（包含MVA值的.spm文件会被searchd预缓冲到RAM中）。MVA的源数据来源既可以是一个单独的查询，也可以是文档属性，参考 sql_attr_multi中的来源类型。在第一种情况中，该查询须返回文档ID和MVA值的序对；而在第二种情况中，该字段被分析为整型值。对于多值属性的输入数据的顺序没有任何限制，在索引过程中这些值会自动按文档ID分组（而相同文档ID下的数据也会排序）。 在过滤过程中，MVA属性中的任何一个值满足过滤条件，则文档与过滤条件匹配（因此通过排他性过滤的文档不会包含任何被禁止的值）。按MVA属性分组时，一篇文档会被分到与多个不同MVA值对应的多个组。例如，如果文档集只包含一篇文档，它有一个叫做tag的MVA属性，该属性的值是5、7和11，那么按tag的分组操作会产生三个组，它们的@count都是1，@groupby键值分别是5、7和11。还要注意，按MVA分组可能会导致结果集中有重复的文档：因为每篇文文档可能属于不同的组，而且它可能在多个组中被选为最佳结果，这会导致重复的ID。由于历史原因，PHP API对结果集的行进行按文档ID的有序hash，因此用PHP API进行对MVA属性的分组操作时你还需要使用 SetArrayResult(). 索引为了快速地相应响应查询，Sphinx需要从文本数据中建立一种为查询做优化的特殊的数据结构。这种数据结构被称为索引（index）；而建立索引的过程也叫做索引或建立索引（indexing）。 不同的索引类型是为不同的任务设计的。比如，基于磁盘的B-Tree存储结构的索引可以更新起来比较简单（容易向已有的索引插入新的文档），但是搜起来就相当慢。 目前在Sphinx中实现的唯一一种索引类型是为最优化建立索引和检索的速度而设计的。随之而来的代价是更新索引相当的很慢。理论上讲，更新这种索引甚至可能比从头重建索引还要慢。不过大多数情况下这可以靠建立多个索引来解决索引更新慢的问题，细节请参考 第 3.11 节 “实时索引更新”. 每个配置文件都可以按需配置足够多的索引。indexer 工具可以将它们同时重新索引（如果使用了–all选项）或者仅更新明确指出的一个。 searchd工具会为所有被指明的索引提供检索服务，而客户端可以在运行时指定使用那些索引进行检索。 字符集、大小写转换和转换表当建立索引时，Sphinx从指定的数据源获得文本文档，将文本分成词的集合，再对每个词做大小写转换，于是“Abc”，“ABC”和“abc”都被当作同一个词（word，或者更学究一点，词项term） 为了正确完成上述工作，Sphinx需要知道： 源文本是什么编码的; 那些字符是字母，哪些不是; 哪些字符需要被转换，以及被转换成什么. 这些都可以用 charset_type 和 charset_table 选项为每个索引单独配置. charset_type 指定文档的编码是单字节的（SBCS）还是UTF-8的。在Coreseek中，如果通过charset_dictpath设置中文词典启动了中文分词模式后，不仅可以使用UTF-8编码的，还可以使用GBK及BIG5的编码（需要编译时提供iconv支持）；但是在内部实现中，仍然是预先转换成UTF-8编码在进行处理的. charset_table 则指定了字母类字符到它们的大小写转换版本的对应表，没有在这张表中出现的字符被认为是非字母类字符，并且在建立索引和检索时被当作词的分割符来看待。 注意，尽管默认的转换表并不包含空格符 (ASCII code 0x20, Unicode U+0020) , 但是这么做是 完全合法的。 这在某些情况下可能有用，比如在对tag云构造索引的时候，这样一个用空格分开的词集就可以被当作一个单独的查询项了. 实时索引更新(作者注:然并卵)当前主要有两种方法来维护全文索引的内容是最新的。请注意，但是，这两种处理方法 的任务是应对全文数据更新，而不是 属性更新。从版本0.9.8开始支持即时的属性更新。参看 UpdateAttributes() API 调用了解详情。 方法一，使用基于磁盘的索引，手动分区，然后定期重建较小的分区（被称为“增量”）。通过尽可能的减小重建部分的大小，可以将平均索引滞后时间降低到30~60秒。在0.9.x版本中，这是唯一可用的方法。在一个巨大的文档集上，这可能是最有效的一种方法。参看第 3.12 节 “增量索引更新”了解详情。 方法二，版本1.x（从版本1.10-beta开始）增加了实时索引（简写为Rt索引）的支持，用于及时更新全文数据。在RT索引上的更新，可以在1~2毫秒（0.001-0.002秒）内出现在搜索结果中。然而，RT实时索引在处理较大数据量的批量索引上效率并不高。参看 第 4 章 RT实时索引 了解详情。 增量索引更新有这么一种常见的情况：整个数据集非常大，以至于难于经常性的重建索引，但是每次新增的记录却相当地少。一个典型的例子是：一个论坛有1000000个已经归档的帖子，但每天只有1000个新帖子。 在这种情况下可以用所谓的“主索引＋增量索引”（main+delta）模式来实现“近实时”的索引更新。 这种方法的基本思路是设置两个数据源和两个索引，对很少更新或根本不更新的数据建立主索引，而对新增文档建立增量索引。在上述例子中，那1000000个已经归档的帖子放在主索引中，而每天新增的1000个帖子则放在增量索引中。增量索引更新的频率可以非常快，而文档可以在出现几分种内就可以被检索到。 确定具体某一文档的分属那个索引的分类工作可以自动完成。一个可选的方案是，建立一个计数表，记录将文档集分成两部分的那个文档ID，而每次重新构建主索引时，这个表都会被更新。 1234567891011121314151617181920212223242526272829303132333435363738# in MySQLCREATE TABLE sph_counter( counter_id INTEGER PRIMARY KEY NOT NULL, max_doc_id INTEGER NOT NULL);# in sphinx.confsource main&#123; # ... sql_query_pre = SET NAMES utf8 sql_query_pre = REPLACE INTO sph_counter SELECT 1, MAX(id) FROM documents sql_query = SELECT id, title, body FROM documents \\ WHERE id&lt;=( SELECT max_doc_id FROM sph_counter WHERE counter_id=1 )&#125;source delta : main&#123; sql_query_pre = SET NAMES utf8 sql_query = SELECT id, title, body FROM documents \\ WHERE id&gt;( SELECT max_doc_id FROM sph_counter WHERE counter_id=1 )&#125;index main&#123; source = main path = /path/to/main # ... all the other settings&#125;# note how all other settings are copied from main,# but source and path are overridden (they MUST be)index delta : main&#123; source = delta path = /path/to/delta&#125; 请注意，上例中我们显示设置了数据源delta的sql_query_pre选项，覆盖了全局设置。必须显示地覆盖这个选项，否则对delta做索引的时候也会运行那条REPLACE查询，那样会导致delta源中选出的数据为空。可是简单地将delta的sql_query_pre设置成空也不行，因为在继承来的数据源上第一次运行这个指令的时候，继承来的所有值都会被清空，这样编码设置的部分也会丢失。因此需要再次显式调用编码设置查询。 索引合并合并两个已有的索引比重新对所有数据做索引更有效率，而且有时候必须这样做（例如在“主索引＋增量索引”分区模式中应合并主索引和增量索引，而不是简单地重新索引“主索引对应的数据）。因此indexer有这个选项。合并索引一般比重新索引快，但在大型索引上仍然不是一蹴而就。基本上，待合并的两个索引都会被读入内存一次，而合并后的内容需要写入磁盘一次。例如，合并100GB和1GB的两个索引将导致202GB的IO操作（但很可能还是比重新索引少） 基本的命令语法如下： 1indexer --merge DSTINDEX SRCINDEX [--rotate] SRCINDEX的内容被合并到DSTINDEX中，因此只有DSTINDEX索引会被改变。 若DSTINDEX已经被searchd于提供服务，则–rotate参数是必须的。 最初设计的使用模式是，将小量的更新从SRCINDEX合并到DSTINDEX中。 因此，当属性被合并时，一旦出现了重复的文档ID，SRCINDEX中的属性值更优先（会覆盖DSTINDEX中的值）。 不过要注意，“旧的”关键字在这个过程中并不会被自动删除。 例如，在DSTINDEX中有一个叫做“old”的关键字与文档123相关联，而在SRCINDEX中则有关键字“new”与同一个文档相关，那么在合并后用这两个关键字都能找到文档123。 您可以给出一个显式条件来将文档从DSTINDEX中移除，以便应对这种情况，相关的开关是–merge-dst-range: 1indexer --merge main delta --merge-dst-range deleted 0 0 这个开关允许您在合并过程中对目标索引实施过滤。过滤器可以有多个，只有满足全部过滤条件的文档才会在最终合并后的索引中出现。在上述例子中，过滤器只允许“deleted”为0的那些条件通过，而去除所有标记为已删除（“deleted”）的记录（可以通过调用UpdateAttributes()设置文档的属性）。 这个开关允许您在合并过程中对目标索引实施过滤。过滤器可以有多个，只有满足全部过滤条件的文档才会在最终合并后的索引中出现。在上述例子中，过滤器只允许“deleted”为0的那些条件通过，而去除所有标记为已删除（“deleted”）的记录（可以通过调用UpdateAttributes()设置文档的属性）。 RT实时索引RT实时索引概览RT实时索引使用注意事项RT实时索引原理二进制日志搜索##SphinxQL 指南(作者注:很重要,更何况search工具在新版后不这么好用了)sphinx.conf/csft.conf 配置选项参考 参考资料: sphinx官方wiki(较老,建议只看介绍部分) CoreSeek中文文档(CoreSeek是基于sphinx的中文分词引擎)","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Sphinx","slug":"Sphinx","permalink":"https://straysh.github.life/tags/Sphinx/"}]},{"title":"oAuth授权的过程","slug":"2014-oAuth授权的过程","date":"2014-11-28T14:35:02.000Z","updated":"2019-12-01T10:13:39.508Z","comments":true,"path":"2014/11/28/2014-oAuth授权的过程/","link":"","permalink":"https://straysh.github.life/2014/11/28/2014-oAuth%E6%8E%88%E6%9D%83%E7%9A%84%E8%BF%87%E7%A8%8B/","excerpt":"","text":"查看原文，点击这里 ####使用OAuth 2.0访问Google APIs#### Google APIs使用OAuth 2.0来认证和授权。Google APIs支持常用的OAuth 2.0场景，例如网页应用，需要安装的本地应用，和客户端应用。 OAuth 2.0 是一个相对简单的协议。首先，你需要从Google Developers Console获取OAuth 2.0的凭证（译注：client_id、client_secret、redirect_uri等）。然后你的客户端应用程序从Google身份认证服务器请求一个accesss token，从http响应中解析出一个token，最后将携带这个token去访问google API。 这个页面给出了Google支持的OAuth 2.0身份认证场景的概览，并提供了更详细的链接。OAuth 2.0身份认证的详细文档，请点击OpenID Connect。 既要正确实现功能又要保证安全，外面强烈推荐您在与Google OAuth 2.0终端交互时使用OAuth 2.0的库文件。使用这些比人well-debug过的代码是最佳实践，并且它能保护你的服务器和你的用户。更多信息，参考Client libraries。 ####基础步骤#### 所有的应用在访问Google OAuth 2.0时都遵循一个模式。从高层次上来说，你需要4个步骤： 1. 从Google开发者控制台获取到凭证#####至少你需要获取到client_id，可能也需要client_secret（译注：详细请参看上一篇文章创建Google App）。需要哪些值取决于你在Google开发者控制台创建的是何种app。例如，一个javascript应用不需要client_secret，但是网页应用需要。 2. 从Google身份认证服务器上获取一个access token#####在你的应用程序能使用Google API访问私有数据之前，你需要获取一个准许访问该API的token。单单一个access token能准许对多个API各种等级的访问权限。变量参数scope控制access token准许访问的一系列资源和操作。在获取access-token期间，你的应用程序会发送一个或者多个scope值。 有多种方法来发送这个请求，并且它们根据你创建的应用而变化。例如，一个javascript应用会使用浏览器跳转到google来获取token，而在设备上安装的应用程序则因没有浏览器转而使用web service请求token。 在用户使用google账户登陆时，需要数个授权步骤。在正确登陆之后，用户会被询问是否准许应用程序获取这些权限。这个过程称之为user consent。 若用户准许了这些权限，Google身份认证服务器会给你的应用程序发送一个access token（或者你可以用来获取access token的一个授权code）。若用户拒绝，则服务器返回一个error。 通常，增量的请求scope是最佳的实践：在需要时请求相应的scope，而不是在最开始就请求。例如，一个需要支持支付的应用就应该在用户点击”购买”时才请求访问Google钱包的权限。详情查看Incremental authorization。 3. 将access token 发送给一个API#####应用获取到access token之后，它会在http身份认证头中发送token到Google API上。在URI的查询字符串中发送token也是可以的，单并不推荐。因为URI参数会被记录到日志文件中因此并不安全。一个良好的REST实践是：避免非必须的URI参数。 access token只能访问在获取该token时指定的scope中描述的资源和操作。例如，若token是为Google+ API产生的，它不被准许访问Google 联系人API。但是，你可以向Google+ API多次发送同一个token。 4. 如果需要，刷新access token#####access token有有限的生命时间。若你的应用程序需要在超出生命时间之后访问Google API，它可以获取一个refresh token。refresh token允许你的应用程序获取一个新的access token。 将refresh token保存在一个长期的存储介质中，并在它有效时继续使用它。refresh token的数量是有限制的(这句不会翻译 囧)。若应用程序获取的refresh token超量，旧的refresh token将会失效。 to be continued … …","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"创建Google App","slug":"2014-创建Google-App","date":"2014-11-28T13:09:15.000Z","updated":"2019-12-01T10:13:39.448Z","comments":true,"path":"2014/11/28/2014-创建Google-App/","link":"","permalink":"https://straysh.github.life/2014/11/28/2014-%E5%88%9B%E5%BB%BAGoogle-App/","excerpt":"","text":"第一步，打开https://console.developers.google.com，点击create project 点击app进入管理页面，下图中3处均需要修改 首先，点击APIs，找到Google+ API 将off修改为on 进入Consent screen 选择邮箱并设置app的名称 进入 Credentials，点击 Create new Client ID ok，到这里，就可以使用网页登陆google+了。 下一章：oAuth授权的过程","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"Google+ oAuth 2.0 登陆","slug":"2014-Google-oAuth-2-0-登陆","date":"2014-11-28T12:43:36.000Z","updated":"2019-12-01T10:13:40.124Z","comments":true,"path":"2014/11/28/2014-Google-oAuth-2-0-登陆/","link":"","permalink":"https://straysh.github.life/2014/11/28/2014-Google-oAuth-2-0-%E7%99%BB%E9%99%86/","excerpt":"","text":"今天要写的是一个系列，主题是使用google+账号登陆自己的网站。 创建Goolge app oAuth授权的过程 一个简单的网页登陆 进阶：让服务器替代用户请求google API (js版本的例子) 使用命令行调试google API（oAuth 授权过程为例） Final：使用ios上使用google+账号登陆自己的网站 参考文献： https://console.developers.google.com https://developers.google.com/analytics/devguides/reporting/core/v2/gdataAuthentication https://developers.google.com/+/mobile/ios/sign-in https://developers.google.com/+/quickstart/php https://developers.google.com/accounts/docs/OAuth2ForDevices","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"PHP 5.4 on CentOS/RHEL 7.0, 6.5 and 5.10 via Yum","slug":"2014-PHP-5-4-on-CentOS-RHEL-7-0-6-5-and-5-10-via-Yum","date":"2014-11-17T05:46:09.000Z","updated":"2019-12-01T10:13:39.532Z","comments":true,"path":"2014/11/17/2014-PHP-5-4-on-CentOS-RHEL-7-0-6-5-and-5-10-via-Yum/","link":"","permalink":"https://straysh.github.life/2014/11/17/2014-PHP-5-4-on-CentOS-RHEL-7-0-6-5-and-5-10-via-Yum/","excerpt":"","text":"###PHP 5.4 on CentOS/RHEL 7.0, 6.5 and 5.10 via Yum###原文 PHP 5.4.33 has been released on PHP.net on 18th September 2014, and is also available for CentOS/RHEL 5.10 and 6.5 at Webtatic via Yum. Update 2013-07-21 – A new package “php54w-mysqlnd” has been added as an alternative to “php54w-mysql”. This will instead provide mysql, mysqli, and pdo_mysql built against the PHP MySql native driver rather than the system default libmysqlclient. It will replace “php54w-mysql55″, as it will work with MySQL 5.0/5.1/5.5 server) Update 2013-06-20 – Webtatic now has released PHP 5.5.0 for CentOS/RHEL 5 and 6 Update 2013-05-26 – CentOS/RHEL 5.x now supported. Update 2013-05-18 – A new package “php54w-pecl-zendopcache” has been added, Zend Optimizer Plus opcode cache. Update 2012-08-26 – APC is stable enough now and so the extension has been added Update 2012-07-22 – memcache and xdebug extensions have been added Update 2012-04-29 – mcrypt, tidy, mssql, interbase have been added back in to the repository. PHP 5.4.0 adds new features such as: Traits Built-in web server Array short notation Array return value de-referencing Finally killing off magic-quotes and safe-mode To see what else has been added, check out the changelog. To install, first you must add the Webtatic EL yum repository information corresponding to your CentOS/RHEL version to yum: CentOS/RHEL 7.x: 12rpm -Uvh https://mirror.webtatic.com/yum/el7/epel-release.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm CentOS/RHEL 6.x: 1rpm -Uvh https://mirror.webtatic.com/yum/el6/latest.rpm CentOS/RHEL 5.x: 1rpm -Uvh https://mirror.webtatic.com/yum/el5/latest.rpm Now you can install php by doing: 1yum install php54w If you would like to upgrade php to this version it is recommended that you check that your system will support the upgrade, e.g. making sure any CPanel-like software can run after the upgrade. Unless you know what you are doing, it is risky upgrading an existing system. It’s much safer to do this by provisioning a separate server to perform the upgrade as a fresh install instead. If you know what you are doing, you can upgrade PHP by: 123yum install yum-plugin-replace yum replace php-common --replace-with=php54w-common It will likely give you a message “WARNING: Unable to resolve all providers …”. This is normal, and you can continue by tying “y“. You will be given a chance to see what packages will be installed and removed before again being given a chance to confirm.Packages Package Providesphp54w mod_php, php54w-ztsphp54w-bcmath**php54w-cli php-cgi, php-pcntl, php-readline**php54w-common php-api, php-bz2, php-calendar, php-ctype, php-curl, php-date, php-exif, php-fileinfo, php-ftp, php-gettext, php-gmp, php-hash, php-iconv, php-json, php-libxml, php-openssl, php-pcre, php-pecl-Fileinfo, php-pecl-phar, php-pecl-zip, php-reflection, php-session, php-shmop, php-simplexml, php-sockets, php-spl, php-tokenizer, php-zend-abi, php-zip, php-zlibphp54w-dbaphp54w-develphp54w-embedded php-embedded-develphp54w-enchantphp54w-fpmphp54w-gdphp54w-imapphp54w-interbase php_database, php-firebirdphp54w-intlphp54w-ldapphp54w-mbstringphp54w-mcryptphp54w-mssqlphp54w-mysql php-mysqli, php_databasephp54w-mysqlnd php-mysqli, php_databasephp54w-odbc php-pdo_odbc, php_databasephp54w-pdophp54w-pecl-apcphp54w-pecl-gearmanphp54w-pecl-geoipphp54w-pecl-memcachephp54w-pecl-zendopcachephp54w-pecl-xdebugphp54w-pgsql php-pdo_pgsql, php_databasephp54w-process php-posix, php-sysvmsg, php-sysvsem, php-sysvshmphp54w-pspellphp54w-recodephp54w-snmpphp54w-soapphp54w-tidyphp54w-xml php-dom, php-domxml, php-wddx, php-xslphp54w-xmlrpc Opcode Caches A precompiled PHP APC package is available as an opcode cache, which is recommended for performance reasons. It can be installed via: 1yum install php54w-pecl-apc Zend have now released Zend Optimizer Plus opcode cache as open source, and is now known as Zend OPcache. As it’s more actively maintained than APC, it has been added as a package to the Webtatic EL6 repository. It can be installed via: 1yum install php54w-pecl-zendopcache #####error_reporting E_ALL now includes E_STRICT##### You may get a lot more errors coming out of your error logs if by default your error_reporting is set to E_ALL now without explicitly turning off E_STRICT. The default php.ini that comes with the PHP package turns this off by default, but if you are upgrading from an existing installation, your php.ini may not be updated, meaning this will likely be turned on. Cent-OS 6 安装LAMP 精简版本 12345678910111213141516171819202122232425# 更新Linux到最新内核yum update -y# 安装时间同步yum install ntpdate# 同步时间ntpdate 210.72.145.44# CentOS6 增加PHP5.4源（推荐）rpm -Uvh http://mirror.webtatic.com/yum/el6/latest.rpm# CentOS5 增加PHP5.4源rpm -Uvh http://mirror.webtatic.com/yum/el5/latest.rpm# 安装LAMPyum install php54w.x86_64 php54w-cli.x86_64 php54w-devel.x86_64 php54w-intl.x86_64 php54w-mysqlnd.x86_64 php54w-pdo.x86_64 php54w-soap.x86_64 php54w-tidy.x86_64 php54w-xml.x86_64 php54w-xmlrpc.x86_64 php54w-zts.x86_64 php54w-gd.x86_64 php54w-mbstring.x86_64 php54w-pecl-apc.x86_64 php54w-mcrypt.x86_64 httpd.x86_64 httpd-devel.x86_64 httpd-tools.x86_64 mysql.x86_64 mysql-devel.x86_64 mysql-server.x86_64 vsftpd.x86_64 unzip.x86_64 -y# 添加自启动chkconfig httpd onchkconfig mysqld on# 启动Apache和mysqlservice httpd startservice mysqld start# 当然要简单直接reboot 参考：sudo apt-get install lamp-server^CentOS6.3关于LAMP的配置apache-2.4.3、php-5.4.7、phpMyAdmin3.5(系列博文)","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"搭建Nginx图片服务器","slug":"2014-搭建Nginx图片服务器","date":"2014-11-13T02:46:55.000Z","updated":"2019-12-01T10:13:39.428Z","comments":true,"path":"2014/11/13/2014-搭建Nginx图片服务器/","link":"","permalink":"https://straysh.github.life/2014/11/13/2014-%E6%90%AD%E5%BB%BANginx%E5%9B%BE%E7%89%87%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"Part-I 安装Nginx 安装PCRE 下载 ngx_cache_purge 并解压，用来清除缓存 下载Nginx并解压 cd nginx-1.7.7 编译，–prefix使用默认值，则nginx安装在/usr/local/nginx123./configure --user=www --group=www --add-module=../ngx_cache_purge-1.0 --with-http_stub_status_module --with-http_ssl_modulemake &amp;&amp; make install ###Part-II 配置###vim /usr/local/nginx/conf/nginx.conf，并编辑如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191user www www;worker_processes 8;error_log /data3/nginx/error.log crit;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid /usr/local/nginx/nginx.pid;events &#123; use epoll; worker_connections 65535;&#125;http &#123; include mime.types; default_type application/octet-stream; charset utf-8; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 300m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; client_body_buffer_size 512k; proxy_connect_timeout 5; proxy_read_timeout 60; proxy_send_timeout 5; proxy_buffer_size 16k; proxy_buffers 4 64k; proxy_busy_buffers_size 128k; proxy_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.1; gzip_comp_level 2; gzip_types text/plainapplication/x-javascript text/css application/xml; gzip_vary on;# proxy_temp_path 和 proxy_cache_path 必须在同一分区 proxy_temp_path /data0/proxy_temp_dir;# 设置web缓存区名称为cahche_one，内存缓存空间大小为200M,1天没有被访问的内容自动清除硬盘缓存空间大小为300G proxy_cache_path /data0/proxy_cache_dir levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=30g;# upstream backend_server&#123;# server 192.168.1.121:80 weight=1 max_fail=2 fail_timeout=30s; # server 192.168.1.122:80 weight=1 max_fail=2 fail_timeout=30s; # server 192.168.1.123:80 weight=1 max_fail=2 fail_timeout=30s; # &#125;#以下为缓存服务器 log_format cache &#x27;***$time_local \\n&#x27; &#x27; $upstream_cache_status \\n&#x27; &#x27; $remote_addr, $http_x_forwarded_for \\n&#x27; &#x27; Cache-Control: $upstream_http_cache_control \\n&#x27; &#x27; Expires: $upstream_http_expires \\n&#x27; &#x27; &quot;$request&quot;($status) \\n&#x27; &#x27; &quot;$http_user_agent&quot; \\n&#x27;; server &#123; listen 80; server_name 192.168.1.120; location / &#123; proxy_cache cache_one; # 对不同的HTTP状态码设置不同的缓存时间 proxy_cache_valid 200 304 12h; #以域名、URI、参数组合成web服务器的key值，Ngnix根据key值哈希，存储缓存内容到二级缓存目录内 proxy_cache_key $host$uri$is_args$args; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; #此处跳转到真实图片服务器 proxy_pass http://192.168.1.120:8080; access_log /data3/nginx/cache.log cache; expires 1d; &#125; location ~ /purge(/.*)&#123; #设置只允许指定的ip或ip段才可以清除url缓存 #allow 127.0.0.1; #allow 192.168.0.0/16; #deny all; proxy_cache_purge cache_one $host$1$is_args$args; &#125;# #扩展名为.php、.jsp、.cig结尾的动态应用程序不缓存# location ~.*\\.(php|jsp|cgi)?$# &#123;# proxy_set_header Host $host;# proxy_set_header X-Forwarded-For $remote_addr;# proxy_pass http://backend_server;# &#125; access_log off; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&#x27;s document root # concurs with nginx&#x27;s one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125; #真实的图片服务器 server&#123; listen 8080; server_name localhost; location /&#123; root /data0/images/; &#125; #访问日志，一般都off掉 access_log /data3/nginx/access.log combined; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; 到此，Nginx图片服务区搭建完毕。在/data0/images/下放入一张图片 200.jpg测试之访问 http://192.168.1.120/200.jpg，cache_log记录如下： 123456789101112131415***12/Nov/2014:16:15:26 +0800 MISS 192.168.1.19, - Cache-Control: - Expires: - &quot;GET /200.jpg HTTP/1.1&quot;(200) &quot;Mozilla/5.0 (X11; Linux x86_64; rv:30.0) Gecko/20100101 Firefox/30.0&quot; ***12/Nov/2014:16:15:38 +0800 HIT 192.168.1.19, - Cache-Control: - Expires: - &quot;GET /200.jpg HTTP/1.1&quot;(200) &quot;Mozilla/5.0 (X11; Linux x86_64; rv:30.0) Gecko/20100101 Firefox/30.0&quot; 访问 http://192.168.1.120/purge/200.jpg 清除缓存 【参考资料】：http://nginx.org/en/docs/configure.htmlhttp://nginx.org/cn/#basic_http_features","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://straysh.github.life/tags/Nginx/"}]},{"title":"MySQL分区","slug":"2014-MySQL分区","date":"2014-11-12T08:50:53.000Z","updated":"2019-12-01T10:13:40.424Z","comments":true,"path":"2014/11/12/2014-MySQL分区/","link":"","permalink":"https://straysh.github.life/2014/11/12/2014-MySQL%E5%88%86%E5%8C%BA/","excerpt":"","text":"http://dev.mysql.com/doc/refman/5.5/en/partitioning-limitations-partitioning-keys-unique-keys.html 19.5.1 分区索引，主键索引，和唯一索引 这一节讨论了分区索引同主键索引、唯一索引的关系。 掌握这一关系的基本原则可以表述如下：在分区表的分区表达式中所用到的列必须是该表的每一个唯一索引的一部分。 换而言之，该表的每一个唯一索引都必须使用了分区表达式中的每一列。（唯一索引也包括主键索引，因为从定义上将主键也是唯一索引。这个特殊的例子在后面后讨论。）如下例，下面的建表语句都是无效的。 1234567891011121314151617181920CREATE TABLE t1 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, UNIQUE KEY (col1, col2))PARTITION BY HASH(col3)PARTITIONS 4;CREATE TABLE t2 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, UNIQUE KEY (col1), UNIQUE KEY (col3))PARTITION BY HASH(col1 + col3)PARTITIONS 4; 上述两例中，目标表将至少有一个唯一索引不是分区表达式中指定的列。 下面的例子时有效的，并指出了正确修改上述无效例子的方法： 12345678910111213141516171819CREATE TABLE t1 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, UNIQUE KEY (col1, col2, col3))PARTITION BY HASH(col3)PARTITIONS 4;CREATE TABLE t2 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, UNIQUE KEY (col1, col3))PARTITION BY HASH(col1 + col3)PARTITIONS 4; 下面的例子是错误的： 1234567891011mysql&gt; CREATE TABLE t3 ( -&gt; col1 INT NOT NULL, -&gt; col2 DATE NOT NULL, -&gt; col3 INT NOT NULL, -&gt; col4 INT NOT NULL, -&gt; UNIQUE KEY (col1, col2), -&gt; UNIQUE KEY (col3) -&gt; ) -&gt; PARTITION BY HASH(col1 + col3) -&gt; PARTITIONS 4;ERROR 1491 (HY000): A PRIMARY KEY must include all columns in the table&#x27;s partitioning function 上述建表语句失败了，原因在于col1和col3这两列都在目标分区索引中，但是，没有任何一列出现在该表的全部唯一索引中。如下时一种可能的改进方案： 1234567891011mysql&gt; CREATE TABLE t3 ( -&gt; col1 INT NOT NULL, -&gt; col2 DATE NOT NULL, -&gt; col3 INT NOT NULL, -&gt; col4 INT NOT NULL, -&gt; UNIQUE KEY (col1, col2, col3), -&gt; UNIQUE KEY (col3) -&gt; ) -&gt; PARTITION BY HASH(col3) -&gt; PARTITIONS 4;Query OK, 0 rows affected (0.05 sec) 这个例子中，col3是该表每一个唯一索引的一部分。因此上述语句成功了。 下面这个建表语句时无法分区的，因为分区索引中不能有某列出现的该表的每一个唯一索引中。 12345678CREATE TABLE t4 ( col1 INT NOT NULL, col2 INT NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, UNIQUE KEY (col1, col3), UNIQUE KEY (col2, col4)); 由于主键也是被定义为唯一索引的，若表有主键，上述的限制页适用与主键。下述两例均是无效的： 1234567891011121314151617181920CREATE TABLE t5 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, PRIMARY KEY(col1, col2))PARTITION BY HASH(col3)PARTITIONS 4;CREATE TABLE t6 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, PRIMARY KEY(col1, col3), UNIQUE KEY(col2))PARTITION BY HASH( YEAR(col2) )PARTITIONS 4; 两例中，主键都未包含分区表达式中的列。然而，下面的两例都是有效的： 1234567891011121314151617181920CREATE TABLE t7 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, PRIMARY KEY(col1, col2))PARTITION BY HASH(col1 + YEAR(col2))PARTITIONS 4;CREATE TABLE t8 ( col1 INT NOT NULL, col2 DATE NOT NULL, col3 INT NOT NULL, col4 INT NOT NULL, PRIMARY KEY(col1, col2, col4), UNIQUE KEY(col2, col1))PARTITION BY HASH(col1 + YEAR(col2))PARTITIONS 4; 如果一张表没有唯一索引（该情形包括没有主键），则上述限制不再生效，那么只要列类型兼容分区类型，可以在分区表达式中适用任何列。 基于同样的道理，不能在分区之后给分区表加唯一索引，除非该索引包含分区表达式中所有的列。思考下面的例子： 12345678mysql&gt; CREATE TABLE t_no_pk (c1 INT, c2 INT) -&gt; PARTITION BY RANGE(c1) ( -&gt; PARTITION p0 VALUES LESS THAN (10), -&gt; PARTITION p1 VALUES LESS THAN (20), -&gt; PARTITION p2 VALUES LESS THAN (30), -&gt; PARTITION p3 VALUES LESS THAN (40) -&gt; );Query OK, 0 rows affected (0.12 sec) 使用下面的ALTER语句给t_no_pk表增加主键是可行的： 12345678910111213141516171819# possible PKmysql&gt; ALTER TABLE t_no_pk ADD PRIMARY KEY(c1);Query OK, 0 rows affected (0.13 sec)Records: 0 Duplicates: 0 Warnings: 0# drop this PKmysql&gt; ALTER TABLE t_no_pk DROP PRIMARY KEY;Query OK, 0 rows affected (0.10 sec)Records: 0 Duplicates: 0 Warnings: 0# use another possible PKmysql&gt; ALTER TABLE t_no_pk ADD PRIMARY KEY(c1, c2);Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0# drop this PKmysql&gt; ALTER TABLE t_no_pk DROP PRIMARY KEY;Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0 但是，下面这条语句是无效的，因为c1是分区索引的一部分，却不是主键的一部分： 123# fails with error 1503mysql&gt; ALTER TABLE t_no_pk ADD PRIMARY KEY(c2);ERROR 1503 (HY000): A PRIMARY KEY must include all columns in the table&#x27;s partitioning function 因为t_no_pk表的分区索引中只含有c1列，因此尝试将c2设置为唯一索引是无效的。但是，你可以设置一个同时使用c1和c2的主键。 上述规则也适用于你期望使用 ALTER TABLE … PARTITION BY 来分区的当前非分区表。思考如下建表语句： 1234567mysql&gt; CREATE TABLE np_pk ( -&gt; id INT NOT NULL AUTO_INCREMENT, -&gt; name VARCHAR(50), -&gt; added DATE, -&gt; PRIMARY KEY (id) -&gt; );Query OK, 0 rows affected (0.08 sec) 下面的ALTER语句是无效的，因为added列不是该表中任何唯一索引的一部分： 1234mysql&gt; ALTER TABLE np_pk -&gt; PARTITION BY HASH( TO_DAYS(added) ) -&gt; PARTITIONS 4;ERROR 1503 (HY000): A PRIMARY KEY must include all columns in the table&#x27;s partitioning function 但是，使用id列作为分区索引时可行的，如下例： 12345mysql&gt; ALTER TABLE np_pk -&gt; PARTITION BY HASH(id) -&gt; PARTITIONS 4;Query OK, 0 rows affected (0.11 sec)Records: 0 Duplicates: 0 Warnings: 0 在np_pk这个例子中，能被用于分区表达式中的只有id列；若你想在分区表达式中使用任何表中其他列，你必须先修改表，或将待分区的列加入主键，或直接删除主键。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"pptp配置","slug":"2014-pptp配置","date":"2014-03-13T05:25:26.000Z","updated":"2020-02-28T10:54:46.744Z","comments":true,"path":"2014/03/13/2014-pptp配置/","link":"","permalink":"https://straysh.github.life/2014/03/13/2014-pptp%E9%85%8D%E7%BD%AE/","excerpt":"","text":"123456789101112131415161718192021222324252627281.yum -y install ppp pptp2.vim /etc/ppp/chap-secrets# Secrets for authentication using CHAP# client server secret IP addressesvpn帐号 vpnIP vpn密码 *3.vim /etc/ppp/peers/phpcurlpty &#x27;pptp vpnIP --nolaunchpppd&#x27;name vpn帐号remotename vpnIPrequire-mppe-128file /etc/ppp/options.pptpipparam phpcurl4.vim /etc/ppp/options.pptplocknoauthrefuse-paprefuse-eaprefuse-chapnobsdcompnodeflaterequire-mppe-1285.pppd call phpcurl6.route del default dev eth07.route add default dev ppp0","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"用Linux命令行生成随机密码的十种方法","slug":"2014-用Linux命令行生成随机密码的十种方法","date":"2014-03-05T01:27:37.000Z","updated":"2019-12-01T10:13:39.324Z","comments":true,"path":"2014/03/05/2014-用Linux命令行生成随机密码的十种方法/","link":"","permalink":"https://straysh.github.life/2014/03/05/2014-%E7%94%A8Linux%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E5%AF%86%E7%A0%81%E7%9A%84%E5%8D%81%E7%A7%8D%E6%96%B9%E6%B3%95/","excerpt":"","text":"Linux操作系统的一大优点是对于同样一件事情，你可以使用高达数百种方法来实现它。例如，你可以通过数十种方法来生成随机密码。本文将介绍生成随机密码的十种方法。 这些方法均收集于Command-Line Fu，并且在我们自己的Linux PC机上测试过。这十种方法的一些在安装了Cygwin的Windows下也是可以运行的，特别是最后一种方法。 生成一个随机密码 对于下面的任何一种方法，你可以通过简单的修改来生成特定长度的密码，或者只使用其输出结果的前N位。希望你正在使用一些类似于LastPass的密码管理器，这样你就不用自己记住这些随机生成的密码了。 这种方法使用SHA算法来加密日期，并输出结果的前32个字符： 1date +%s | sha256sum | base64 | head -c 32 ; echo 这种方法使用内嵌的/dev/urandom，并过滤掉那些日常不怎么使用的字符。这里也只输出结果的前32个字符： 1&lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c$&#123;1:-32&#125;;echo; 这种方法使用openssl的随机函数。如果你的系统也许没有安装openssl，你可以尝试其它九种方法或自己安装openssl。 1openssl rand -base64 32 这种方法类似于之前的urandom，但它是反向工作的。Bash的功能是非常强大的！ 1tr -cd &#x27;[:alnum:]&#x27; &lt; /dev/urandom | fold -w30 | head -n1 这种方法使用string命令，它从一个文件中输出可打印的字符串： 1strings /dev/urandom | grep -o &#x27;[[:alnum:]]&#x27; | head -n 30 | tr -d &#x27;\\n&#x27;; echo 这是使用urandom的一个更简单的版本： 1&lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c6 这种方法使用非常有用的dd命令： 1dd if=/dev/urandom bs=1 count=32 2&gt;/dev/null | base64 -w 0 | rev | cut -b 2- | rev 你甚至可以生成一个只用左手便可以输入的密码： 1&lt;/dev/urandom tr -dc &#x27;12345!@#$%qwertQWERTasdfgASDFGzxcvbZXCVB&#x27; | head -c8; echo &quot;&quot; 如果每次都使用上述某种方法，那更好的办法是将它保存为函数。如果这样做了，那么在首次运行命令之后，你便可以在任何时间只使用randpw就可以生成随机密码。或许你可以把它保存到你的~/.bashrc文件里面。 1randpw()&#123; &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c$&#123;1:-16&#125;;echo;&#125; 最后这种生成随机密码的方法是最简单的。它同样也可以在安装了Cygwin的Windows下面运行。在Mac OS X下或许也可以运行。我敢肯定会有人抱怨这种方法生成的密码没有其它方法来的随机。但实际上如果你使用它生成的全部字符串作为密码，那这个密码就足够随机了。 1date | md5sum 是的，这种方法也极其好记。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"mysql_faq","slug":"2013-mysql-faq","date":"2013-12-22T13:15:48.000Z","updated":"2019-12-01T10:13:39.292Z","comments":true,"path":"2013/12/22/2013-mysql-faq/","link":"","permalink":"https://straysh.github.life/2013/12/22/2013-mysql-faq/","excerpt":"","text":"int(5) 的含义，存储时使用多岁字节？最大能存放的数是多少？以前，为一直认为int(5)表示最多能存放5个字符长的数字，即99,999能存进去，但是100,000会被截断。又例如当我们有一个字段is_onuse(1)，它有2个值0/1表示禁用/启用，我也一直认为这样定义字段会节省空间。 但实际上，这些“认为”都是想当然的，都是错误的。在MySQL中数值类型的存储空间是预定义的，例如对于tinyint(1)来说，不论你存储的是0还是1还是100，它都是占用一个字节的空间（当然超过127会被截断）。对于int(5)，同样的，不论99,999还是100,000都是占用4个字节的空间。从这点上看，与字符串的定义varchar(5)是截然不同的两个概念（varchar(5)的确是只能存储5个字符）。详细的官方文档，请点击查看 mysql 不区分声调 ‘e’ ‘é’遇到这个问题的通常情况是数据字符涉及到多国语言。大家往往认为在创建数据库或表格时指定charset=utf8，数据库就能识别所有字符，结果实际情况却并非这样 mysql使用charset指定字符如果存储，却依靠另外一个参数–collation–来指定排序规则。这个collation正是mysql是否区分大小学，是否区分声调字符的关键。每个charset有一个默认collation。utf8的默认collation是utf8_general_ci，ci表示case insensitive，即不区分大小写。通过测试，该collation也不区分’e’ 和’é’，’ü’和’u’。 而实际上绝大多collation都不区分，或者部分区分这些带声调或特殊地区的特殊字符。如果一个数据集必须保证区分这类字符，可以指定charset=utf8 collation=utf8_bin 例如： 1234567891011121314CREATE TABLE data_state(id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,country VARCHAR(32) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,state VARCHAR(32) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,url VARCHAR(255) NOT NULL,c_time INT UNSIGNED DEFAULT &#x27;0&#x27;)ENGINE=MYISAM DEFAULT CHARSET=utf8;CREATE TABLE data_city(id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,state_id INT UNSIGNED NOT NULL,city VARCHAR(32) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,url VARCHAR(255) NOT NULL,c_time INT UNSIGNED DEFAULT &#x27;0&#x27;)ENGINE=MYISAM DEFAULT CHARSET=utf8; 查看MySQL当前连接数 12straysh ~]$mysqladmin statusUptime: 17863 Threads: 1 Questions: 2932 Slow queries: 0 Opens: 57 Flush tables: 1 Open tables: 50 Queries per second avg: 0.164 12345678910111213mysql&gt; show status like &#x27;Conn%&#x27;;+-----------------------------------+-------+| Variable_name | Value |+-----------------------------------+-------+| Connection_errors_accept | 0 || Connection_errors_internal | 0 || Connection_errors_max_connections | 0 || Connection_errors_peer_address | 0 || Connection_errors_select | 0 || Connection_errors_tcpwrap | 0 || Connections | 281 |+-----------------------------------+-------+7 rows in set (0.00 sec) ERROR 1054 (42S22): Unknown column ‘plugin’ in ‘mysql.user’see MySQL service stops after trying to grant privileges to a user to be continue. .. …","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"Linux shell用法和技巧","slug":"2013-Linux-shell用法和技巧","date":"2013-12-21T07:33:44.000Z","updated":"2019-12-01T10:13:39.340Z","comments":true,"path":"2013/12/21/2013-Linux-shell用法和技巧/","link":"","permalink":"https://straysh.github.life/2013/12/21/2013-Linux-shell%E7%94%A8%E6%B3%95%E5%92%8C%E6%8A%80%E5%B7%A7/","excerpt":"","text":"检查远程端口是否对bash开放： 1echo &gt;/dev/tcp/8.8.8.8/53 &amp;&amp; echo &quot;open&quot; 让进程转入后台： 1Ctrl + z 将进程转到前台： 1$&gt;fg 产生随机的十六进制数，其中n是字符数： 1openssl rand -hex n 在当前shell里执行一个文件里的命令： 1source /home/user/file.name 截取前5个字符： 1$&#123;variable:0:5&#125; SSH debug 模式: 1ssh -vvv user@ip_address SSH with pem key: 1ssh user@ip_address -i key.pem 用wget抓取完整的网站目录结构，存放到本地目录中： 1wget -r --no-parent --reject &quot;index.html*&quot; http://hostname/ -P /home/user/dirs 一次创建多个目录： 1mkdir -p /home/user/&#123;test,test1,test2&#125; 列出包括子进程的进程树： 1ps axwef 创建 war 文件: 1jar -cvf name.war file 测试硬盘写入速度： 1dd if=/dev/zero of=/tmp/output.img bs=8k count=256k; rm -rf /tmp/output.img 测试硬盘读取速度： 1hdparm -Tt /dev/sda 获取文本的md5 hash： 1echo -n &quot;text&quot; | md5sum 检查xml格式： 1xmllint --noout file.xml 将tar.gz提取到新目录里： 1tar zxvf package.tar.gz -C new_dir 使用curl获取HTTP头信息： 1curl -I http://www.example.com 修改文件或目录的时间戳(YYMMDDhhmm): 1touch -t 0712250000 file 用wget命令执行ftp下载： 1wget -m ftp://username:password@hostname 生成随机密码(例子里是16个字符长): 1LANG=c &lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c$&#123;1:-16&#125;;echo; 快速备份一个文件： 1cp some_file_name&#123;,.bkp&#125; 访问Windows共享目录： 1smbclient -U &quot;DOMAIN\\user&quot; //dc.domain.com/share/test/dir 执行历史记录里的命令(这里是第100行): 1!100 解压: 1unzip package_name.zip -d dir_name 输入多行文字(CTRL + d 退出): 1cat &gt; test.txt 创建空文件或清空一个现有文件： 1&gt; test.txt 与Ubuntu NTP server同步时间： 1ntpdate ntp.ubuntu.com 用netstat显示所有tcp4监听端口： 1netstat -lnt4 | awk &#x27;&#123;print $4&#125;&#x27; | cut -f2 -d: | grep -o &#x27;[0-9]*&#x27; qcow2镜像文件转换： 12qemu-img convert -f qcow2 -O raw precise-server-cloudimg-amd64-disk1.img \\ precise-server-cloudimg-amd64-disk1.raw 重复运行文件，显示其输出（缺省是2秒一次）： 1watch ps -ef 所有用户列表： 1getent passwd Mount root in read/write mode: 1mount -o remount,rw / 挂载一个目录（这是不能使用链接的情况）: 1mount --bind /source /destination 动态更新DNS server: 1234nsupdate &amp;gt; &amp;gt;EOFupdate add $HOST 86400 A $IPsendEOF 递归grep所有目录： 1grep -r &quot;some_text&quot; /path/to/dir 列出前10个最大的文件： 1lsof / | awk &#x27;&#123; if($7 &gt; 1048576) print $7/1048576 &quot;MB &quot;$9 &#125;&#x27; | sort -n -u | tail 显示剩余内存(MB): 1free -m | grep cache | awk &#x27;/[0-9]/&#123; print $4&quot; MB&quot; &#125;&#x27; 打开Vim并跳到文件末： 1vim + some_file_name Git 克隆指定分支(master): 1git clone git@github.com:name/app.git -b master Git 切换到其它分支(develop): 1git checkout develop Git 删除分支(myfeature): 1git branch -d myfeature Git 删除远程分支 1git push origin :branchName Git 将新分支推送到远程服务器： 1git push -u origin mynewfeature 打印历史记录中最后一次cat命令： 1!cat:p 运行历史记录里最后一次cat命令： 1!cat 找出/home/user下所有空子目录: 1find /home/user -maxdepth 1 -type d -empty 获取test.txt文件中第50-60行内容： 1&lt; test.txt sed -n &#x27;50,60p&#x27; 运行最后一个命令(如果最后一个命令是mkdir /root/test, 下面将会运行: sudo mkdir /root/test)： 1sudo !! 创建临时RAM文件系统 – ramdisk (先创建/tmpram目录): 1mount -t tmpfs tmpfs /tmpram -o size=512m Grep whole words: 1grep -w &quot;name&quot; test.txt 在需要提升权限的情况下往一个文件里追加文本： 1echo &quot;some text&quot; | sudo tee -a /path/file 列出所有kill signal参数: 1kill -l 在bash历史记录里禁止记录最后一次会话： 1kill -9 $$ 扫描网络寻找开放的端口： 1nmap -p 8081 172.20.0.0/16 设置git email: 1git config --global user.email &quot;me@example.com&quot; To sync with master if you have unpublished commits: 1git pull --rebase origin master 将所有文件名中含有”txt”的文件移入/home/user目录: 1find -iname &quot;*txt*&quot; -exec mv -v &#123;&#125; /home/user \\; 将文件按行并列显示： 1paste test.txt test1.txt shell里的进度条: 1pv data.log 使用netcat将数据发送到Graphite server: 1echo &quot;hosts.sampleHost 10 `date +%s`&quot; | nc 192.168.200.2 3000 将tabs转换成空格： 1expand test.txt &gt; test1.txt Skip bash history: 1&lt; space &gt;cmd 去之前的工作目录： 1cd - 拆分大体积的tar.gz文件(每个100MB)，然后合并回去： 12split –b 100m /path/to/large/archive /path/to/output/filescat files* &gt; archive 使用curl获取HTTP status code: 1curl -sL -w &quot;%&#123;http_code&#125;\\\\n&quot; www.example.com -o /dev/null 设置root密码，强化MySQL安全安装: 1/usr/bin/mysql_secure_installation 当Ctrl + c不好使时: 1Ctrl + \\ 获取文件owner: 1stat -c %U file.txt block设备列表： 1lsblk -f 找出文件名结尾有空格的文件： 1find . -type f -exec egrep -l &quot; +$&quot; &#123;&#125; \\; 找出文件名有tab缩进符的文件 1find . -type f -exec egrep -l $&#x27;\\t&#x27; &#123;&#125; \\; 用”=”打印出横线: 1printf &#x27;%100s\\n&#x27; | tr &#x27; &#x27; =","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"mysql 不区分声调 'e' 'é'","slug":"2013-mysql-不区分声调-e-e","date":"2013-12-17T15:26:53.000Z","updated":"2019-12-01T10:13:39.284Z","comments":true,"path":"2013/12/17/2013-mysql-不区分声调-e-e/","link":"","permalink":"https://straysh.github.life/2013/12/17/2013-mysql-%E4%B8%8D%E5%8C%BA%E5%88%86%E5%A3%B0%E8%B0%83-e-e/","excerpt":"","text":"遇到这个问题的通常情况是数据字符涉及到多国语言。大家往往认为在创建数据库或表格时指定charset=utf8，数据库就能识别所有字符，结果实际情况却并非这样 mysql使用charset指定字符如果存储，却依靠另外一个参数--collation--来指定排序规则。这个collation正是mysql是否区分大小学，是否区分声调字符的关键。每个charset有一个默认collation。utf8的默认collation是utf8_general_ci，ci表示case insensitive，即不区分大小写。通过测试，该collation也不区分'e' 和'é'，'ü'和'u'。 而实际上绝大多collation都不区分，或者部分区分这些带声调或特殊地区的特殊字符。如果一个数据集必须保证区分这类字符，可以指定charset=utf8 collation=utf8_bin","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"mysql导出select结果到文件","slug":"2013-mysql导出select结果到文件","date":"2013-12-16T01:54:22.000Z","updated":"2019-12-01T10:13:40.012Z","comments":true,"path":"2013/12/16/2013-mysql导出select结果到文件/","link":"","permalink":"https://straysh.github.life/2013/12/16/2013-mysql%E5%AF%BC%E5%87%BAselect%E7%BB%93%E6%9E%9C%E5%88%B0%E6%96%87%E4%BB%B6/","excerpt":"","text":"1mysql -h&#123;ip&#125; -u&#123;account&#125; -p -e &quot;query statement&quot; db &gt; file 例如： 1mysql -uroot -p -e &quot;select * from a&quot; test &gt; 1.txt 这样会输出列名信息，如果不想输出列名信息： 1234mysql -uroot -p -Ne &quot;select * from a&quot; test &gt; 1.txt 或root&gt;mysql -uroot -p test &gt;select * from table into outfile &#x27;1.txt&#x27;; 两种方法效果一样的第二种方式的mysql文档： 123SELECT [select options go here] INTO &#123;OUTFILE | DUMPFILE&#125; filename EXPORT_OPTIONS FROM table_references [additional select options go here] 例如： 12mysql -uroot -p select * from a into outfile &quot;1.txt&quot; fields terminated by &#x27;\\t&#x27; lines terminated by &#x27;\\r\\n&#x27; 第一种方法和第二种方法的结合：使用 mysql -e执行导出到文件的sql语句 mysql -hxx -uxx -pxx -e &quot;query statement&quot; db 例如: 1mysql -uroot -p -e&quot;select * from a into outfile &#x27;1.txt&#x27; fields terminated by &#x27;,&#x27; lines terminated by &#x27;\\r\\n&#x27;&quot; test 如果不想输出列名信息： 1mysql -uroot -p -Ne&quot;select * from a into outfile &#x27;1.txt&#x27; fields terminated by &#x27;,&#x27; lines terminated by &#x27;\\r\\n&#x27;&quot; test 默认情况下， mysql -e导出的文件，列是用”\\t”分隔，行是用”\\r\\n”分隔(windows)，行是用”\\n”分隔(unix) 追加一种方式： 123select col002,col005,col004,col008 into outfile &#x27;e:/mysql/i0812.txt&#x27; fields terminated by &#x27;|&#x27; lines terminated by &#x27;\\r\\n&#x27; from test where col003 in (select col001 from qdbm) order by col005;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"表单按回车就自动提交的问题","slug":"2013-表单按回车就自动提交的问题","date":"2013-12-13T09:53:04.000Z","updated":"2020-02-28T10:56:17.392Z","comments":true,"path":"2013/12/13/2013-表单按回车就自动提交的问题/","link":"","permalink":"https://straysh.github.life/2013/12/13/2013-%E8%A1%A8%E5%8D%95%E6%8C%89%E5%9B%9E%E8%BD%A6%E5%B0%B1%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"当form表单中只有一个&lt;input type=&quot;text&quot; name=&quot;name&quot; /&gt;时按回车键将会自动将表单提交。123&lt;form id=&quot;form1&quot; action=&quot;post.php&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; /&gt; &lt;/form&gt; 再添加一个 &lt;input type=&quot;text&quot; /&gt; 按下回车将不会自动提交，但是页面上显示一个不知所云的输入框挺别扭，后从网上搜到两个解决办法： 1) 添加一个 &lt;input style=&quot;display: none;&quot; type=&quot;text&quot; /&gt;不显示输入框，然后回车之后也不会提交： 1234&lt;form id=&quot;form1&quot; action=&quot;post.php&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; /&gt; &lt;input style=&quot;display:none&quot; /&gt; &lt;/form&gt; 2) 添加一个onkeydown事件，然后回车之后也不会显示：123&lt;form id=&quot;form1&quot; action=&quot;post.php&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; onkeydown=&quot;if(event.keyCode==13) return false;&quot;/&gt; &lt;/form&gt; 如果想添加回车事件可以在onkeydown事件中添加判断提交表单： 1234&lt;form id=&quot;form1&quot; action=&quot;post.php&quot; method=&quot;post&quot;&gt; &lt;input style=&quot;display:none&quot; /&gt; &lt;input type=&quot;text&quot; name=&quot;name&quot; onkeydown=&quot;if(event.keyCode==13)&#123;gosubmit();&#125;&quot; /&gt; &lt;/form&gt; 我们有时候希望回车键敲在文本框（input element）里来提交表单（form），但有时候又不希望如此。比如搜索行为，希望输入完关键词之后直接按回车键立即提交表单，而有些复杂表单，可能要避免回车键误操作在未完成表单填写的时候就触发了表单提交。 要控制这些行为，不需要借助JS，浏览器已经帮我们做了这些处理，这里总结几条规则： 如果表单里有一个type=\"submit\"的按钮，回车键生效。 如果表单里只有一个type=\"text\"的input，不管按钮是什么type，回车键生效。 如果按钮不是用input，而是用button，并且没有加type，IE下默认为type=button，FX默认为type=submit。 其他表单元素如textarea、select不影响，radio checkbox不影响触发规则，但本身在FX下会响应回车键，在IE下不响应。 type=\"image\"的input，效果等同于type=\"submit\"，不知道为什么会设计这样一种type，不推荐使用，应该用CSS添加背景图合适些。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://straysh.github.life/tags/Javascript/"}]},{"title":"Configuring MySQL To Use UTF-8","slug":"2013-Configuring-MySQL-To-Use-UTF-8","date":"2013-12-10T02:02:26.000Z","updated":"2019-12-01T10:13:39.656Z","comments":true,"path":"2013/12/10/2013-Configuring-MySQL-To-Use-UTF-8/","link":"","permalink":"https://straysh.github.life/2013/12/10/2013-Configuring-MySQL-To-Use-UTF-8/","excerpt":"","text":"A project I’m working on at the moment is going to have multiple language options available, not all of which use the same alphabet (e.g. Russian and Chinese). To lessen the pain commonly associated with internationalisation on the web, it’s beneficial to use the UTF-8 character set. This short summary from the Unicode Consortium may help explain better; Unicode provides a unique number for every character, no matter what the platform, no matter what the program, no matter what the language. … Unicode enables a single software product or a single website to be targeted across multiple platforms, languages and countries without re-engineering. It allows data to be transported through many different systems without corruption Thankfully MySQL has supported Unicode for quite some time now, even if it’s not configured to use it by default. First, let’s check what our settings are at the moment; 1234567891011121314151617181920212223mysql&gt; SHOW VARIABLES LIKE &#x27;collation%&#x27;;+----------------------+-------------------+| Variable_name | Value |+----------------------+-------------------+| collation_connection | latin1_swedish_ci || collation_database | latin1_swedish_ci || collation_server | latin1_swedish_ci |+----------------------+-------------------+3 rows in set (0.01 sec)mysql&gt; SHOW VARIABLES LIKE &#x27;character_set%&#x27;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | latin1 | | character_set_connection | latin1 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | latin1 | | character_set_server | latin1 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+8 rows in set (0.00 sec) That’s to be expected, but it’s not really what we wanted. Find your MySQL configuration file (on most Linux/BSD systems it’s /etc/my.cnf) and make sure it’s got the following statements under the relevant headers. 123456789[mysqld]default-character-set=utf8default-collation=utf8_general_cicharacter-set-server=utf8collation-server=utf8_general_ciinit-connect=&#x27;SET NAMES utf8&#x27;[client]default-character-set=utf8 Restart MySQL and make sure it’s working; service mysql restart 123456789mysql&gt; SHOW VARIABLES LIKE &#x27;collation%&#x27;;+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_general_ci | | collation_database | utf8_general_ci | | collation_server | utf8_general_ci | +----------------------+-----------------+3 rows in set (0.00 sec) 1234567891011121314mysql&gt; SHOW VARIABLES LIKE &#x27;character_set%&#x27;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+8 rows in set (0.00 sec) Update: Demonstrating setting the charset and collation when creating tables, as suggested by Mo: 123456CREATE TABLE `content` ( `id` int(11) NOT NULL auto_increment, `language` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `title` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL default &#x27;&#x27;, PRIMARY KEY (`id`)) ENGINE=MyISAM CHARACTER SET utf8 COLLATE utf8_general_ci;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"favorite_CPanel|服务器文件及目录","slug":"2013-favorite-CPanel-服务器文件及目录","date":"2013-12-10T01:52:02.000Z","updated":"2020-02-28T10:56:30.106Z","comments":true,"path":"2013/12/10/2013-favorite-CPanel-服务器文件及目录/","link":"","permalink":"https://straysh.github.life/2013/12/10/2013-favorite-CPanel-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%E5%8F%8A%E7%9B%AE%E5%BD%95/","excerpt":"","text":"Apache 123456789101112/usr/local/apache+ bin- apache binaries are stored here – httpd, apachectl, apxs+ conf – configuration files – httpd.conf+ cgi-bin+ domlogs – domain log files are stored here+ htdocs+ include – header files+ libexec – shared object (.so) files are stored here – libphp4.so,mod_rewrite.so+ logs – apache logs – access_log, error_log, suexec_log+ man – apache manual pages+ proxy -+ icons - CPanel 12345678/usr/local/cpanel+ 3rdparty/ – tools like fantastico, mailman files are located here+ addons/ – AdvancedGuestBook, phpBB etc+ base/ – phpmyadmin, squirrelmail, skins, webmail etc+ bin/ – cpanel binaries+ cgi-sys/ – cgi files like cgiemail, formmail.cgi, formmail.pl etc+ logs/ – cpanel access log and error log+ whostmgr/ – whm related files WHM 12345678/var/cpanel – whm files+ bandwidth/ – rrd files of domains+ username.accts – reseller accounts are listed in this files+ packages – hosting packages are listed here+ root.accts – root owned domains are listed here+ suspended – suspended accounts are listed here+ users/ – cpanel user file – theme, bwlimit, addon, parked, sub-domains all are listed in this files+ zonetemplates/ – dns zone template files are taken from here Important CPanel/WHM files 1234567891011121314151617181920212223242526272829303132333435/etc/domainips 独立ip 如果共享ip被当成了独立ip删除这里的就可以了/etc/httpd/conf/httpd.conf – apache configuration file/etc/exim.conf – mail server configuration file/etc/named.conf – name server (named) configuration file/etc/proftpd.conf – proftpd server configuration file/etc/pure-ftpd.conf – pure-ftpd server configuration file/etc/valiases/domainname – catchall and forwarders are set here/etc/vfilters/domainname – email filters are set here/etc/userdomains – all domains are listed here – addons, parked,subdomains along with their usernames/etc/localdomains – exim related file – all domains should be listed here to be able to send mails/var/cpanel/users/username – cpanel user file/var/cpanel/cpanel.config – cpanel configuration file ( Tweak Settings )/etc/cpbackup-userskip.conf -/etc/sysconfig/network – Networking Setup/etc/hosts -/var/spool/exim -/var/spool/cron -/etc/resolv.conf – Networking Setup Resolver Configuration/etc/nameserverips – Networking Setup Nameserver IPs ( For resellers to give their nameservers )/var/cpanel/resellers – For addpkg, etc permissions for resellers./etc/chkserv.d – Main &gt;&gt; Service Configuration &gt;&gt; Service Manager/var/run/chkservd – Main &gt;&gt; Server Status &gt;&gt; Service Status/var/log/dcpumon – top log process/root/cpanel3-skel – skel directory. Eg: public_ftp, public_html./etc/wwwacct.conf – account creation defaults file in WHM (Basic cPanel/WHMSetup)/etc/cpupdate.conf – Update Config/etc/cpbackup.conf – Configure Backup/etc/clamav.conf – clamav (antivirus configuration file )/etc/my.cnf – mysql configuration file/usr/local/Zend/etc/php.ini OR /usr/local/lib/php.ini – php configuration file/etc/ips – ip addresses on the server (except the shared ip)/etc/ipaddrpool – ip addresses which are free/etc/ips.dnsmaster – name server ips/var/cpanel/Counters – To get the counter of each users./var/cpanel/bandwidth – To get bandwith usage of domains PHP 12Program :/usr/local/bin/php, /usr/bin/phpini file: /usr/local/lib/php.ini – apache must be restarted after any change to this file Exim 123456789101112131415Conf : /etc/exim.conf – exim main configuration file/etc/localdomains – list of domains allowed to relay mailLog : /var/log/exim_mainlog – incoming/outgoing mails are logged here/var/log/exim_rejectlog – exim rejected mails are reported here/var/log/exim_paniclog – exim errors are logged hereMail queue: /var/spool/exim/inputCpanel script to restart exim – /scripts/restartsrv_eximEmail forwarders and catchall address file – /etc/valiases/domainname.comEmail filters file – /etc/vfilters/domainname.comPOP user authentication file – /home/username/etc/domainname/passwdcatchall inbox – /home/username/mail/inboxPOP user inbox – /home/username/mail/domainname/popusername/inboxPOP user spambox – /home/username/mail/domainname/popusername/spamProgram : /usr/sbin/exim (suid – -rwsr-xr-x 1 root root )Init Script: /etc/rc.d/init.d/exim MySQL 1234567Program : /usr/bin/mysqlInit Script : /etc/rc.d/init.d/mysqlConf : /etc/my.cnf, /root/.my.cnfData directory – /var/lib/mysql – Where all databases are stored.Database naming convention – username_dbname (eg: john_sales)Permissions on databases – drwx 2 mysql mysqlSocket file – /var/lib/mysql/mysql.sock, /tmp/ mysql.sock","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"HOWTO: prevent 'stdin: is not a tty' errors when using ssh","slug":"2013-HOWTO-prevent-stdin-is-not-a-tty-errors-when-using-ssh","date":"2013-12-10T01:00:04.000Z","updated":"2020-02-28T10:56:34.960Z","comments":true,"path":"2013/12/10/2013-HOWTO-prevent-stdin-is-not-a-tty-errors-when-using-ssh/","link":"","permalink":"https://straysh.github.life/2013/12/10/2013-HOWTO-prevent-stdin-is-not-a-tty-errors-when-using-ssh/","excerpt":"","text":"link:http://www.vpshostingforum.com/howto-prevent-stdin-not-tty-errors-when-using-ssh-t4152.html When you are connecting to a remote server using ssh (for rsync, or other remote functions), and are receiving 'stdin: is not a tty' error messages in your log files, it is most likely because your remote shell is trying to execute something which needs to run interactively, when you are not in an interactive mode. I was experiencing this problem myself, while re-creating my rsync backup system on my new local server, and after looking all over the place for info - I finally found a solution. Of course, the rsync was still working properly, but having it generate errors was keeping me from sleeping Simply add the following line to the top of your .bashrc file (in the home dir of your user account): Code: [ -z \"$PS1\" ] && return This will cause bash to check and see if it is running interactively or not, and if it is not, it will halt processing of the configuration file, preventing the rest of the file from executing code which could possibly generate this error when in non-interactive mode. This must be done for each of your user accounts, but if you also make the change in /etc/skel/.bashrc, the 'fixed' version will be used for all new accounts in the future. Note: This is the first time I have done anything with bash configuration files, so feel free to let me know if this method will cause any bad results","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://straysh.github.life/tags/Linux/"}]},{"title":"检测上传的文件类型","slug":"2013-检测上传的文件类型","date":"2013-10-30T08:08:52.000Z","updated":"2019-12-01T10:13:40.356Z","comments":true,"path":"2013/10/30/2013-检测上传的文件类型/","link":"","permalink":"https://straysh.github.life/2013/10/30/2013-%E6%A3%80%E6%B5%8B%E4%B8%8A%E4%BC%A0%E7%9A%84%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"在$_FILES数组中，type字段中的图片类型不能作为检测的途径。应为这个值是浏览器通过HTTP头发送过来的，是可以伪造的。例如： 首先在test.php中写入，然后修改文件名为test.jpg，并通过表单添加该文件。服务端接收的数据如下： 123456789array ( &#x27;pic&#x27; =&gt; array ( &#x27;name&#x27; =&gt; &#x27;test.jpg&#x27;, &#x27;type&#x27; =&gt; &#x27;image/jpeg&#x27;, &#x27;tmp_name&#x27; =&gt; &#x27;/tmp/phpogAh98&#x27;, &#x27;error&#x27; =&gt; 0, &#x27;size&#x27; =&gt; 0, ),) 网络上也有一些通过读取文件前2字节，来判断文件类型的代码。但这些方法既不严谨也不够安全。 12345678910111213141516171819202122232425function file_type($filename) &#123; $file = fopen($filename, &quot;rb&quot;); $bin = fread($file, 2); //只读2字节 fclose($file); $strInfo = @unpack(&quot;C2chars&quot;, $bin); $typeCode = intval($strInfo[&#x27;chars1&#x27;].$strInfo[&#x27;chars2&#x27;]); $fileType = &#x27;&#x27;; switch ($typeCode) &#123; case 7790: $fileType = &#x27;exe&#x27;;break; case 7784: $fileType = &#x27;midi&#x27;;break; case 8297: $fileType = &#x27;rar&#x27;;break; case 8075: $fileType = &#x27;zip&#x27;;break; case 255216: $fileType = &#x27;jpg&#x27;;break; case 7173: $fileType = &#x27;gif&#x27;;break; case 6677: $fileType = &#x27;bmp&#x27;;break; case 13780: $fileType = &#x27;png&#x27;;break; default: $fileType = &#x27;unknown: &#x27;.$typeCode; &#125; //Fix if ($strInfo[&#x27;chars1&#x27;]==&#x27;-1&#x27; AND $strInfo[&#x27;chars2&#x27;]==&#x27;-40&#x27; ) return &#x27;jpg&#x27;; if ($strInfo[&#x27;chars1&#x27;]==&#x27;-119&#x27; AND $strInfo[&#x27;chars2&#x27;]==&#x27;80&#x27; ) return &#x27;png&#x27;; return $fileType; &#125; 其实，在php中就有检测图片类型的方法。getimagesize。这个函数不需要GD等画图库的支持。打开文件失败或者不是有效的图片时返回false。 123456789array ( 0 =&gt; 460, 1 =&gt; 406, 2 =&gt; 2, 3 =&gt; &#x27;width=&quot;460&quot; height=&quot;406&quot;&#x27;, &#x27;bits&#x27; =&gt; 8, &#x27;channels&#x27; =&gt; 3, &#x27;mime&#x27; =&gt; &#x27;image/jpeg&#x27;,) 正确的放回值中mime亦可用作进一步过滤图片类型。另外需要注意，当被检测的文件为空时，会产生警告： 12Notice: getimagesize(): Read error! in /fileupload/sample5/file_type_detect.php on line 2false 可以使用filezie()函数先过滤掉空的文件。或者使用@getimagesize()抑制错误输出。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"结合HTML5的纯js图片上传","slug":"2013-结合HTML5的纯js图片上传","date":"2013-10-20T12:28:08.000Z","updated":"2019-12-01T10:13:39.836Z","comments":true,"path":"2013/10/20/2013-结合HTML5的纯js图片上传/","link":"","permalink":"https://straysh.github.life/2013/10/20/2013-%E7%BB%93%E5%90%88HTML5%E7%9A%84%E7%BA%AFjs%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0/","excerpt":"","text":"首次发长文，文辞简陋加之能力有限，若有错漏，望不吝赐教。 本文探讨的是图片（文件）上传的前端技术，涉及到html5（FileRader API）、js（常规DOM操作）以及ajax（模拟表单提交multipart/form-data数据）。后端使用的php语言，为简便，这里仅仅接收了ajax提交的数据。 首先让我们来回顾一下传统的使用表单上传文件的技术。 12345678&lt;!-- The data encoding type, enctype, MUST be specified as below --&gt;&lt;form enctype=&quot;multipart/form-data&quot; action=&quot;__URL__&quot; method=&quot;POST&quot;&gt; &lt;!-- MAX_FILE_SIZE must precede the file input field --&gt; &lt;input type=&quot;hidden&quot; name=&quot;MAX_FILE_SIZE&quot; value=&quot;30000&quot; /&gt; &lt;!-- Name of input element determines name in $_FILES array --&gt; Send this file: &lt;input name=&quot;userfile&quot; type=&quot;file&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;Send File&quot; /&gt;&lt;/form&gt; 传统表单上传的要点是：必须使用method=”POST”；必须指定enctype=”multipart/form-data”；包含&lt;input type=”file” /&gt;表单元素。文件上传必须使用POST，这个不必多说，若有疑问请自行google或者点击RFC2616。enctype指定了http协议头中的contentType值，它的缺省值为application/x-www-form-urlencoded，而multipart/form-data则是上传文件专用的编码方式，它指定了requestBody中的数据拼接和拆解的格式。关于协议的具体内容，请点击multipart/form-data。更多表单细节请点击 后端接收文件使用$_FILES即可。 在上传多个文件时，就需要动态的添加&lt;input type=”file” /&gt;，而取消某个文件时又要删除对应的表单元素。不论现实它是否复杂，传统的表单上传都有一个缺点：一次只能添加一个文件。如果要上传20个文件，就要至少点击20次，如果更多呢，例如100，想想就崩溃了。 在HTML5出现之前，就已经有大量的插件能够解决这个问题。例如我之前一直使用的swfupload。在线demo,点击标签微博发布器-&gt;定时微博。如果你能找到这个文件/app/m_attention/modules/clocksend/common/js/main.js，重点看看L16~L48以及L269，这是插件的调用方式。 上面说的这些插件都用到了flash或者其它技术来实现多文件选取，无法定制。好消息是，在HTML5中提供了一些API，使得单纯的使用JavaScript就可以访问选取的文件，不仅可以获得文件的文件名，物理路径，还能直接读取文件的内容。关于FileReader请点击。请复制下面的代码体验FileReader： 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;body&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;form enctype=&quot;multipart/form-data&quot; method=&#x27;post&#x27;&gt;&lt;input type=&#x27;file&#x27; name=&quot;userfile&quot; id=&quot;upfile&quot; onchange=&quot;handleFiles(this.files)&quot; /&gt;&lt;input type=&quot;submit&quot; value=&quot;upload&quot; id=&quot;submit&quot;/&gt;&lt;/form&gt;&lt;div id=&quot;imgcont&quot;&gt;&lt;/div&gt;&lt;script&gt;function handleFiles(files) &#123; if (files.length) &#123; var file = files[0]; var reader = new FileReader(); if (/text\\/\\w+/.test(file.type)) &#123; reader.onload = function() &#123; $(&#x27;&lt;pre&gt;&#x27; + this.result + &#x27;&lt;/pre&gt;&#x27;).appendTo(&#x27;#imgcont&#x27;); &#125; reader.readAsText(file); &#125;else if(/image\\/\\w+/.test(file.type))&#123; reader.onload=function()&#123; $(&quot;&lt;img src=&#x27;&quot;+this.result+&quot;&#x27; /&gt;&quot;).appendTo(&#x27;#imgcont&#x27;); &#125; reader.readAsDataURL(file); &#125; &#125;&#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 请注意上面代码中的第22行，这是一个技巧。传统的图片显示需要指定src为一个网络路径或者相对路径，而现代浏览器都支持一种新的方式，“data” url。它以消耗客户端性能为代价，将图片的base64编码替换成src的值，省去了一次对图片地址的http请求。然后，在我们的例子中，它实现了 图片预览 的功能。 ####Ajax上传文件现在，我们已经使用JavaScript获取了待上传的文件，只需要点击Submit按钮，图片就会发送到服务器。接下来，我们来尝试使用ajax来上传文件。 在讲解技术之前，我先来科普几个Tips。 &lt;input type=&quot;file&quot; /&gt; 的value值是只读的，无法修改，使用JavaScript修改该值会被告知一个SecurityError: The operation is insecure.的错误。 前面我们已经提过，文件上传的时候需要指定http协议头的contentType=’multipart/form-data’，因此简单的向服务器post数据，服务器是无法正常接收图片的。即便使用jQuery，在(.ajax()中使用)(‘form’).serialize()方法直接提交整个表单，也是无效的，即使同时指定ContentType:’multipart/form-data’同样无效。当然，你可以将图片的二进制数据（使用FileReader.readAsBinaryString()）post到服务器，当作普通的变量来接收，再将二进制数据写入到文件中来生成图片，但是这样做就不是上传图片了。 这里插入一段个人提示。网络上有一篇文章《基于HTML5的可预览多图片Ajax上传》。我摘录了这边文章中的核心代码，然后我要指出这些技术中的不可取之处以及为何下面的代码是不建议使用的。 123456789101112131415161718192021222324252627282930313233343536373839//文件上传funUploadFile: function() &#123; var self = this; if (location.host.indexOf(&quot;sitepointstatic&quot;) &amp;gt;= 0) &#123; //非站点服务器上运行 return; &#125; for (var i = 0, file; file = this.fileFilter[i]; i++) &#123; (function(file) &#123; var xhr = new XMLHttpRequest(); if (xhr.upload) &#123; // 上传中 xhr.upload.addEventListener(&quot;progress&quot;, function(e) &#123; self.onProgress(file, e.loaded, e.total); &#125;, false); // 文件上传成功或是失败 xhr.onreadystatechange = function(e) &#123; if (xhr.readyState == 4) &#123; if (xhr.status == 200) &#123; self.onSuccess(file, xhr.responseText); self.funDeleteFile(file); if (!self.fileFilter.length) &#123; //全部完毕 self.onComplete(); &#125; &#125; else &#123; self.onFailure(file, xhr.responseText); &#125; &#125; &#125;; // 开始上传 xhr.open(&quot;POST&quot;, self.url, true); xhr.setRequestHeader(&#x27;content-type&#x27;, &#x27;multipart/form-data&#x27;); xhr.setRequestHeader(&quot;X_FILENAME&quot;, file.name); xhr.send(file); &#125; &#125;)(file); &#125; &#125;, 123456789$fn = (isset($_SERVER[&#x27;HTTP_X_FILENAME&#x27;]) ? $_SERVER[&#x27;HTTP_X_FILENAME&#x27;] : false);if ($fn) &#123; file_put_contents( &#x27;uploads/&#x27; . $fn, file_get_contents(&#x27;php://input&#x27;) );echo &quot;http://www.zhangxinxu.com/study/201109/uploads/$fn&quot;;exit();&#125; 作者使用了ajax来发送数据。注意L34～L37,打开一个post连接，指定contentType为multipart-form-data，然后发送了一个额外的头部信息X_FILENAME=file.name，最后发送了图片的二进制代码触发发送动作。上述代码的特点：1、只能发送一张图片；2、requestBody中不能发送其它信息；3、若有额外信息需要发送，可以使用额外的头部新来装载，并在服务端使用$_SERVER[‘HTTP_变量名’]来接收。上述代码是有很大不足的，首先使用php://input来接收数据倒还不如使用post来接收的方便（最少只需要简单的数行代码就能实现），另外根据php官方文档中php://input一节有这么一句：php://input is not available with enctype=”multipart/form-data”。经测试，注释L35上述代码不受影响。当然我也很佩服作者的编码能力，和想象力。但是同时我也要指出这篇文章中的错误，希望后来的读者在借鉴的同时能仔细甄别。 解决这个问题的方法有两种 第一种思路，使用XMLHttpRequest对象的FormData对象。请点击FormData以及使用FormData，或者请看一篇中文博客文档XMLHttpRequest Level 2 使用指南。这个思路的实现是比较简单的，但是它仍然有缺点：需要&lt;input type=”file” /&gt;这个表单元素来存放图片，而且在向FormData对象添加文件时，必须使用该表单元素的DOM对象。使用jQuery的同学一定要注意这个问题。 第二种思路，自己拼装http协议。按照multipart/form-data的格式自己拼装http协议的内容。看上去这个思路颇具难度。但，相信稍稍了解http协议和ajax技术的同学都可以写出来。不过，好消息是，这部分内容早就有人实现并封装了，而且HTML5的drag和drop方法也一并封装了，我们只需要实现页面的DOM操作就可以了。传送门 下面我会给出一demo，介绍这个jq插件的使用方法。暂时放出项目链接，有空再写","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://straysh.github.life/tags/PHP/"}]},{"title":"博文测试","slug":"2013-博文测试","date":"2013-10-13T06:20:40.000Z","updated":"2019-12-01T10:13:40.108Z","comments":true,"path":"2013/10/13/2013-博文测试/","link":"","permalink":"https://straysh.github.life/2013/10/13/2013-%E5%8D%9A%E6%96%87%E6%B5%8B%E8%AF%95/","excerpt":"","text":"Github提交中文乱码这个状况是由于cygwin的中文乱码导致，解决办法：vim ~/.inputrc 添加 1234set meta-flag onset convert-meta offset input-meta onset output-meta on 重启console Github公钥安装确认已经安装了ssh， 在console输入ssh -v检查没有ssh 需要启动setup.exe选择ssh安装即可。 #mkdir ~/.ssh #ssh-keygen -t rsa (密码可以不设置) 登录github网站，选择account settings（网站最上面）-&gt;SSH Keys(侧边栏)-&gt;Add SSH key,title任意，key需要vim ~/.ssh/id_rsa.pub复制出来 最后，git push 你的项目的ssh路径 例如：git push git@github:straysh/test.git 输入刚才设置密钥时设置的密码就ok了。 mintty中vim无彩色 检查.vimrc 中 syntax on 检查TERM变量，在console里输入echo $TERM如果输出的不是xterm，需要修改mintty的配置：在控制台右键选options-&gt;termibal-&gt;type 选择xterm并在console里输入 export TERM=xterm vim中文乱码最简单的办法 设置console字符集为utf8，打开vim之后set encoding=utf-8或者修改.vimrc，添加 123456789&quot; 设置编码set fenc=utf-8set encoding=utf-8set fileencodings=utf-8,gbk,cp936,latin-1&quot; 解决consle输出乱码language messages zh_CN.utf-8 warning: CRLF will be replaced by LFgit config --global core.autocrlf false 禁止Vimwiki自动套用p标签vimwiki会自动套用p标签，当我们使用自定义的html标签时，页面就会变形。我将段落改为=tab激活(段落以一个=号和tab键开头)，找到autoload/vimwiki_html.vim文件，修改L856-L871：有点hack的味道，不过暂时解决了这个难题 1234567891011121314function! s:process_tag_para(line, para) &quot;&#123;&#123;&#123; let lines = [] let para = a:para let processed = 0 if a:line =~ &#x27;^=\\t\\S&#x27; if !para&quot; call add(lines, &quot;&quot;) let para = 1 endif let processed = 1 call add(lines, substitute(a:line,&#x27;^=\\t&#x27;, &#x27;&lt; p&gt;&#x27;,&#x27;&#x27;)) elseif para &amp;&amp; a:line =~ &#x27;^\\s*$&#x27; call add(lines, &quot;&quot;) let para = 0 endif return [processed, lines, para]endfunction &quot;&#125;&#125;&#125; 上次修改完之后发现一个bug，wiki的超链接语法无法转换成html链接。再次修改代码如下：1234567891011121314151617function! s:process_tag_para(line, para) &quot;&#123;&#123;&#123; let lines = [] let para = a:para let processed = 0 if a:line =~ &#x27;^=\\t\\S&#x27; if !para&quot; call add(lines, &quot;&quot;) call add(lines, substitute(a:line,&#x27;^=\\t&#x27;,&#x27;&#x27;,&#x27;&#x27;)) let para = 1 endif let processed = 1 elseif para &amp;&amp; a:line =~ &#x27;^\\s*$&#x27; call add(lines, &quot;&quot;) let para = 0 else call add(lines, a:line) let processed = 1 endif return [processed, lines, para]endfunction &quot;&#125;&#125;&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Yii","slug":"Yii","permalink":"https://straysh.github.life/tags/Yii/"}]},{"title":"Yii_Page_Caching","slug":"2013-Yii-Page-Caching","date":"2013-10-07T12:06:37.000Z","updated":"2019-12-01T10:13:39.336Z","comments":true,"path":"2013/10/07/2013-Yii-Page-Caching/","link":"","permalink":"https://straysh.github.life/2013/10/07/2013-Yii-Page-Caching/","excerpt":"","text":"译文1.输出缓存 2.HTTP缓存 页面缓存指的是将整个页面缓存住。页面缓存可以在不同的地方进行。例如，通过选取合适的HTTP头，客户端浏览器可以将浏览的页面在限定的时间内缓存住。web应用本身也可将页面存储在缓存中。 1.输出缓存(Output Caching) 页面缓存可以当作是片段缓存的特例。因为页面内容通常是由试图上应用布局来生成的，如果只是简单的在布局上调用beginCache()和endCache()是没有效果的。原因是，在视图被计算出来之后，再会在CController::render()方法中应用布局文件。 为了将整个页面缓存住，就要跳过action生成页面内容的代码。我们可以使用COutputCache作为action过滤器来完成这个任务。下面的代码显示了如何配置缓存过滤器： 12345678910public function filters()&#123; return array( array( &#x27;COutputCache&#x27;, &#x27;duration&#x27; =&gt; 100, &#x27;varyByParam&#x27; =&gt; array(&#x27;id&#x27;), ), );&#125; 上面的过滤器配置会应用到控制器中的所有方法上。我们可以使用加号操作符(+)，限制它只发生在一个或几个方法上。更多的细节可以在控制器中找到。 Tip：COutputCache可以作为过滤器是因为它继续自CFileterWidget，而CFilterWidget既是过滤器又是挂件。事实上，挂件的运作方式跟过滤器是很像的：一个挂件（过滤器）会在任何内嵌的内容（方法）被计算之前开始运行，并且挂件（过滤器）会在任何内嵌的内容（方法）被计算之后结束。 2.HTTP缓存 作为简单的缓存方法输出内容的补充，Yii在1.1.11版本引入了CHttpCacheFilter。这个过滤器有助于设置前述的HTTP头来通知客户端在前次请求之后页面内容没有发生改变，这样，服务器不需要重新传输页面内容。CHttpCacheFilter的设置和COutputCache相似： 12345678910public function filter()&#123; return array( array( &#x27;CHttpCacheFilter + index&#x27;, &#x27;lastModified&#x27; =&gt; Yii::app()-&gt;db-&gt;createCommand(&quot;SELECT MAX(update_time) FROM post&quot;)-&gt;queryScalar(), ), );&#125; 上面的代码将HTTP头的Last-Modified设置为文章最后修改的时间。你也可以使用CHttpCacheFilter::lastModifiedExpression（php表达式）来设置HTTP头的Last-Modified。 Tip：CHttpCacheFilter::lastModifiedExpression和CHttpCacheFilter::lastModified都能使用整数表示Unix时间戳或者任意的字符串表示可人识别的时间。后一种方式可以使用strtotime()解析，不许要其它转换。 SEO提示搜索引擎机器人倾向于考虑缓存中的HTTP头信息。由于一些爬虫对单位时间内处理的一个域的页面数量有限制，引入缓存HTTP头或许有助于爬虫索引你的站点，因为这样做减少了爬虫处理的页面数量。 原文Yii Page Caching 12Output CachingHTTP Caching Page caching refers to caching the content of a whole page. Page caching can occur at different places. For example, by choosing an appropriate page header, the client browser may cache the page being viewed for a limited time. The Web application itself can also store the page content in cache. Output Caching Page caching can be considered as a special case of fragment caching. Because the content of a page is often generated by applying a layout to a view, it will not work if we simply call beginCache() and endCache() in the layout. The reason is because the layout is applied within the CController::render() method AFTER the content view is evaluated. To cache a whole page, we should skip the execution of the action generating the page content. We can use COutputCache as an action filter to accomplish this task. The following code shows how we configure the cache filter: 12345678910public function filters()&#123; return array( array( &#x27;COutputCache&#x27;, &#x27;duration&#x27;=&gt;100, &#x27;varyByParam&#x27;=&gt;array(&#x27;id&#x27;), ), );&#125; The above filter configuration would make the filter to be applied to all actions in the controller. We may limit it to one or a few actions only by using the plus operator. More details can be found in filter. Tip: We can use COutputCache as a filter because it extends from CFilterWidget, which means it is both a widget and a filter. In fact, the way a widget works is very similar to a filter: a widget (filter) begins before any enclosed content (action) is evaluated, and the widget (filter) ends after the enclosed content (action) is evaluated. HTTP Caching In addition to simply caching the output of an action, Yii introduced CHttpCacheFilter in version 1.1.11. This filter aids in setting the aforementioned headers to notify a client that a page’s content has not been changed since the last request, so the server will not have to re-transmit the content. CHttpCacheFilter can be set up similar to COutputCache: public function filters(){ return array( array( 'CHttpCacheFilter + index', 'lastModified'=>Yii::app()->db->createCommand(\"SELECT MAX(update_time) FROM post\")->queryScalar(), ), ); } The above code will set the Last-Modified header to the last date at which a post was updated. You can also use CHttpCacheFilter::lastModifiedExpression to set the Last-Modified header using a php expression. Tip: Both, CHttpCacheFilter::lastModifiedExpression and CHttpCacheFilter::lastModified can take either an integer representing an epochal Unix timestamp or an arbitrary string representing a human-readable date. As long as later one can be parsed by strtotime(), no further conversion is necessary. The “Entity Tag” (or ETag for short) header can be set in a similar fashion through CHttpCacheFilter::etagSeed and CHttpCacheFilter::etagSeedExpression , respectively. Both will be serialized (so you can use either a single value or an entire array) and are used to generate a quoted, base64-encoded SHA1 hash serving as content for the ETag header. This differs from the way the Apache Webserver and others are generating their ETags. However, this method is perfectly in line with the RFC and turned out to be more feasible for use in a framework. Note: In order to comply with RFC 2616, section 13.3.4, CHttpCacheFilter will send out ETag and Last-Modified headers if they can both be generated. Consequently, both will be used for cache validation if sent by the client. Since entity tags are hashes, they allow more complex and/or more precise caching strategies than Last-Modified headers. For instance, an ETag can be invalidated if the site has switched to another theme. Tip: Expensive expressions for CHttpCacheFilter::etagSeedExpression may defeat the purpose of CHttpCacheFilter and introduce unnecessary overhead, since they need to be re-evaluated on every request. Try to find a simple expression that invalidates the cache if the page content has been modified. SEO Implications Search engine bots tend to respect cache headers. Since some crawlers have a limit on how many pages per domain they process within a certain time span, introducing caching headers may help indexing your site as they reduce the number of pages that need to be processed.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"}]},{"title":"Yii_Dynamic_Content","slug":"2013-Yii-Dynamic-Content","date":"2013-10-04T03:27:10.000Z","updated":"2019-12-01T10:13:40.024Z","comments":true,"path":"2013/10/04/2013-Yii-Dynamic-Content/","link":"","permalink":"https://straysh.github.life/2013/10/04/2013-Yii-Dynamic-Content/","excerpt":"","text":"译文动态内容 使用片段缓存或者页面缓存时，我们经常遇到这样的情境：除了在个别地方，整个输出的内容都是相对静态的。例如，帮助页面要显示静态的帮助信息，同时，页面顶部要显示当前登录用户的用户名。 解决这个问题，我们可以通过用户名来变换缓存的内容，但这样对，同时，页面顶部要显示当前登录用户的用户名。 解决这个问题，我们可以通过用户名来变换缓存的内容，但除了用户名，缓存中绝大部分内容都是相同的，这对我们珍贵的缓存空间是巨大的浪费。我们也可以将页面划分成几个片段，并分别缓存起来，但是这么做使得视图和代码都变得更加的复杂。更好的实现方式是使用CController提供的动态内容特性。 动态内容指的是输出当中不应该被缓存的片段，即使它被内嵌在片段缓存中。要使得这部分内容一直是动态的，那么每一次请求都要重新生成之，即使嵌套在它上面的这部分内容是从缓存中读取的。因此，我们需要这部分动态的内容有某个方法或者函数来生成。 调用CController::renderDynamic()方法在期望的地方插入动态内容。 1234567... 其它HTML内容 ...&lt;?php if($this-&gt;beginCache($id)&#123; ?&gt;... 片段缓存中的内容 ... &lt;?php $this-&gt;renderDynamic($callback); ?&gt;... 片段缓存中的内容 ...&lt;?php $this-&gt;endCache(); &#125; ?&gt;... 其它HTML内容 ... 上面的代码中，$callback指的是有效的php回调函数。它可以是指向当前控制器类的方法名的字符串，或者全局的函数。也可以是指向对象方法的数组。renderDynamic()任何附加的参数都会被传递给回调函数。回调函数会将动态内容return回来而不是直接输出。 原文 Yii Dynamic Content Dynamic Content When using fragment caching or page caching, we often encounter the situation where the whole portion of the output is relatively static except at one or several places. For example, a help page may display static help information with the name of the user currently logged in displayed at the top. To solve this issue, we can variate the cache content according to the username, but this would be a big waste of our precious cache space since most content are the same except the username. We can also divide the page into several fragments and cache them individually, but this complicates our view and makes our code very complex. A better approach is to use the dynamic content feature provided by CController. A dynamic content means a fragment of output that should not be cached even if it is enclosed within a fragment cache. To make the content dynamic all the time, it has to be generated every time even when the enclosing content is being served from cache. For this reason, we require that dynamic content be generated by some method or function. We call CController::renderDynamic() to insert dynamic content at the desired place. 1234567891011...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id)) &#123; ?&gt;...fragment content to be cached...&lt;blockquote&gt;&lt;?php \\(this-&gt;renderDynamic(\\)callback); ?&gt;&lt;/blockquote&gt;...fragment content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... In the above, $callback refers to a valid PHP callback. It can be a string referring to the name of a method in the current controller class or a global function. It can also be an array referring to a class method. Any additional parameters to renderDynamic() will be passed to the callback. The callback should return the dynamic content instead of displaying it.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"}]},{"title":"Yii_Fragment_Caching","slug":"2013-Yii-Fragment-Caching","date":"2013-10-04T03:20:42.000Z","updated":"2019-12-01T10:13:39.484Z","comments":true,"path":"2013/10/04/2013-Yii-Fragment-Caching/","link":"","permalink":"https://straysh.github.life/2013/10/04/2013-Yii-Fragment-Caching/","excerpt":"","text":"译文缓存选项嵌套缓存 片段缓存指的是缓存页面的片段。例如，一个显示了年度销售报表的页面中，我们可以把这个报表存储在缓存中，这样每次请求都节约了生成该报表的时间。 在视图文件中通过调用CController::beginCache()和CController::endCache()来使用片段缓存。这两个方法分别标记出了待缓存的页面内容的开始和结束位置。类似变量缓存，我们需要一个ID来唯一标识被缓存的片段。 12345... 其它HTML内容 ...&lt;?php if($this-&gt;beginCache($id))&#123; ?&gt;... 被缓存的内容 ...&lt;?php $this-&gt;endCache(); ?&gt;... 其它HTML内容 ... 上面的代码中，如果beginCache()返回false，已缓存的内容会被自动插入到其中；否则，当endCache()被调用时if语句块内的代码会被执行。 1.缓存选项 调用beginCache()时，我们可以通过使用数组来作为组成缓存选项的第二个参数来定制片段缓存。事实上，beginCache()和endCache()方法是对COutputCache挂件的两个实用的包装。因此，缓存选项可以是用来初始化COutputCache挂件的任意属性。 属性duration 或许最常用的选项是有效期(duration)，它指定了缓存在多长时间内有效。类似于CCache::set()的过期参数(expiration)。下面这段代码将缓存约一个小时： 12345... 其它HTML内容 ...&lt;?php if($this-&gt;beginCache($id, array(&#x27;duration&#x27;=&gt;3600))&#123; ?&gt;... 缓存的内容 ...&lt;?php $this-&gt;endCache();&#125; ?&gt;... 其它HTML内容 ... 如果不设置duration值，默认会是60，即缓存将会在60秒后失效。 自1.1.8版本起，若设置duration为0，所有现有的缓存都将被删除。若datation是负值，缓存将被禁用，但已经存在的缓存仍然有效。而在1.1.8版本之前，daration为0或负数将使缓存被(完全,译者注)禁用。 属性dependency 类似变量缓存，被缓存的内容片段也可以有依赖(dependencies)。例如，展示出来的一篇已发布的文章取决于发布的内容是否修改过。 为了指定依赖关系，我们使用denpendency选项，它既可以是一个实现了ICacheDenpendcy的对象，也可以是一个可用来生成denpendency对象的配置数组。下面的代码显示了缓存片段依赖于lastModified列的修改： 1234567... 其它HTML内容 ...&lt;?php if($this-&gt;beginCach($id, array(&#x27;dependency&#x27;=&gt;array( &#x27;class&#x27;=&gt;&#x27;system.caching.dependencies.CDbCacheDependency&#x27;, &#x27;sql&#x27;=&gt;&#x27;SELECT MAX(lastModified) FROM `post`))) &#123; ?&gt;... 缓存的内容 ...&lt;?php $this-&gt;endCache(); &#125; ?&gt;... 其它HTML内容 ... 属性variation 缓存的内容可能随一些参数而变化。例如，个人的profile对不同的用户是不一样的。为了缓存个人的profile，我们希望缓存的副本随着用户ID而改变。本质上来说，就是在调用beginCache()方法时，要使用不同的ID参数。 在一些方案中需要开发人员来变更ID参数，作为一种替代的方案，COutputCache内置了这个特性。(Instead of asking developers to variate the IDs according to some scheme, COutputCache is built-in with such a feature)。下面是对variation的归纳。 varyByRoute:设置这个选项为TRUE，缓存将会根据route而改变。因此，每一个由controller和action组成的请求都将被缓存为单独的内容。 varyBySession:设置这个选项为TRUE，缓存将会根据session ID而改变。因此，每一个用户session看到的内容都是不同的，并且他们都使用缓存。 varyByParam:设置这个选项为名称数组(关联数组，字符串为键，译者注)，我们可以根据指定的GET参数值来读取不同的缓存内容。例如，如果一个页面根据id参数来展示不同的文章，我们可以通过设置varyByParam参数为array(‘id’)来缓存每一篇文章。若没有这样一个变换的手段，我们将只能缓存主一篇文章。 varyByExpression:设置该选项为PHP表达式，我们可以是缓存根据表达式的值而改变。 请求类型(Request Types) 有时，我们希望缓存只在特定的HTTP请求时有效。例如，一个表单页，我们只希望在页面初始化时将表单缓存住，这是一个GET请求。其它一系列(通过POST请求)的表单都不应该被缓存，因为这些表单中肯能含有用户输入信息。可以指定requestTypes选项来达到目的： 12345... 其它HTML内容 ...&lt;?php if($this-&gt;beginCache($id, array(&#x27;requestTypes&#x27;=&gt;array(&#x27;GET&#x27;)))&#123; ?&gt;... 缓存的HTML内容 ...&lt;?php $this-&gt;endCache(); &#125; ?&gt;... 其它HTML内容 ... 2.嵌套缓存 片段缓存是可嵌套的。也就是说，一个片段缓存可以被包装在一个更大的片段缓存中。例如，评论被缓存在一内层的缓存中，并且这些片段缓存同文章内容一起被缓存在外层的片段缓存中。 123456789... 其它HTML内容 ...&lt;?php if($this-&gt;beginCache($id1)&#123; ?&gt;... 外层的缓存内容 ... &lt;?php if($this-&gt;beginCache($id2) &#123; ?&gt; ... 内层的缓存内容 ... &lt;?php $this-&gt;endCache(); &#125; ?&gt;... 外层的缓存内容 ...&lt;?php $this-&gt;endCache(); &#125; ?&gt;... 其它HTML内容 ... 嵌套缓存中可以使用不同的缓存参数。例如，上例中的内外层缓存中可以使用不同的duration参数。设置当外层缓存数据失效时，内层缓存仍然能提供有效的片段缓存内容。但是，反过来却不行。如果外层缓存有效，即时内层缓存过期了，它还是会继续提供相同的缓存副本。在设置duration或者dependency参数时，务必谨慎，否则内层的缓存将会被外层缓存覆盖掉。 原文Yii Fragment Caching 12Caching OptionsNested Caching Fragment caching refers to caching a fragment of a page. For example, if a page displays a summary of yearly sale in a table, we can store this table in cache to eliminate the time needed to generate it for each request. To use fragment caching, we call CController::beginCache() and CController::endCache() in a controller’s view script. The two methods mark the beginning and the end of the page content that should be cached, respectively. Like data caching, we need an ID to identify the fragment being cached. 12345...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id)) &#123; ?&gt;...content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... In the above, if beginCache() returns false, the cached content will be automatically inserted at the place; otherwise, the content inside the if-statement will be executed and be cached when endCache() is invoked. Caching Options When calling beginCache(), we can supply an array as the second parameter consisting of caching options to customize the fragment caching. As a matter of fact, the beginCache() and endCache() methods are a convenient wrapper of the COutputCache widget. Therefore, the caching options can be initial values for any properties of COutputCache.Duration Perhaps the most commonly option is duration which specifies how long the content can remain valid in cache. It is similar to the expiration parameter of CCache::set(). The following code caches the content fragment for at most one hour: 12345...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id, array(&#x27;duration&#x27;=&gt;3600))) &#123; ?&gt;...content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... If we do not set the duration, it would default to 60, meaning the cached content will be invalidated after 60 seconds. Starting from version 1.1.8, if the duration is set 0, any existing cached content will be removed from the cache. If the duration is a negative value, the cache will be disabled, but existing cached content will remain in the cache. Prior to version 1.1.8, if the duration is 0 or negative, the cache will be disabled.Dependency Like data caching, content fragment being cached can also have dependencies. For example, the content of a post being displayed depends on whether or not the post is modified. To specify a dependency, we set the dependency option, which can be either an object implementing ICacheDependency or a configuration array that can be used to generate the dependency object. The following code specifies the fragment content depends on the change of lastModified column value: 12345678...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id, array(&#x27;dependency&#x27;=&gt;array(&#x27;class&#x27;=&gt;&#x27;system.caching.dependencies.CDbCacheDependency&#x27;,&#x27;sql&#x27;=&gt;&#x27;SELECT MAX(lastModified) FROM Post&#x27;)))) &#123; ?&gt;...content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... Variation Content being cached may be variated according to some parameters. For example, the personal profile may look differently to different users. To cache the profile content, we would like the cached copy to be variated according to user IDs. This essentially means that we should use different IDs when calling beginCache(). Instead of asking developers to variate the IDs according to some scheme, COutputCache is built-in with such a feature. Below is a summary. 1varyByRoute: by setting this option to true, the cached content will be variated according to route. Therefore, each combination of the requested controller and action will have a separate cached content. varyBySession: by setting this option to true, we can make the cached content to be variated according to session IDs. Therefore, each user session may see different content and they are all served from cache. 1varyByParam: by setting this option to an array of names, we can make the cached content to be variated according to the values of the specified GET parameters. For example, if a page displays the content of a post according to the id GET parameter, we can specify varyByParam to be array(&#x27;id&#x27;) so that we can cache the content for each post. Without such variation, we would only be able to cache a single post. varyByExpression: by setting this option to a PHP expression, we can make the cached content to be variated according to the result of this PHP expression. Request Types Sometimes we want the fragment caching to be enabled only for certain types of request. For example, for a page displaying a form, we only want to cache the form when it is initially requested (via GET request). Any subsequent display (via POST request) of the form should not be cached because the form may contain user input. To do so, we can specify the requestTypes option: 12345...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id, array(&#x27;requestTypes&#x27;=&gt;array(&#x27;GET&#x27;)))) &#123; ?&gt;...content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... Nested Caching Fragment caching can be nested. That is, a cached fragment is enclosed within a bigger fragment that is also cached. For example, the comments are cached in an inner fragment cache, and they are cached together with the post content in an outer fragment cache. 123456789101112...other HTML content...&lt;?php if(\\(this-&gt;beginCache(\\)id1)) &#123; ?&gt;...outer content to be cached...&lt;?php if(\\(this-&gt;beginCache(\\)id2)) &#123; ?&gt;...inner content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...outer content to be cached...&lt;?php $this-&gt;endCache(); &#125; ?&gt;...other HTML content... Different caching options can be set to the nested caches. For example, the inner cache and the outer cache in the above example can be set with different duration values. Even when the data cached in the outer cache is invalidated, the inner cache may still provide the valid inner fragment. However, it is not true vice versa. If the outer cache is evaluated to be valid, it will continue to provide the same cached copy even after the content in the inner cache has been invalidated. You must be careful in setting the durations or the dependencies of the nested caches, otherwise the outdated inner fragments may be kept in the outer fragment.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"}]},{"title":"translation_Versioning_REST_Services","slug":"2013-translation-Versioning-REST-Services","date":"2013-10-04T03:11:51.000Z","updated":"2019-12-01T10:13:40.064Z","comments":true,"path":"2013/10/04/2013-translation-Versioning-REST-Services/","link":"","permalink":"https://straysh.github.life/2013/10/04/2013-translation-Versioning-REST-Services/","excerpt":"","text":"译文Scott Seely解释了何时发人员需要创建一个新的endpoint。这篇高水准的、非语言相关的文章解释了怎样给REST服务打上版本号以及各个选项在何时才有意义。 在一个多层次的应用程序开始过程中，版本号是一个反复出现的问题。无论你正在更新一个远程调用，或改变一个COM对象，或更新一个web服务，在增加新功能的同时你必须考虑到怎么样兼容现有的用户（consumers）(开发人员或用户)。每一项新的技术的提出都促使我们重新定制版本号并提出处理系统变更的建议。 在这篇文章中，我们着眼于这些事件类型，它们促使REST服务中新版本号的生成。我们只考虑HTTP协议的REST。REST可以在多种不同的协议上以建筑学的风格来实现。在实践中，大部分人选择HTTP技术作为REST架构的基石。 在创建一个REST服务时，在项目的首轮迭代中通常将次原开放为只读的。在HTTP术语中，这意味着项目支持通过URL的GET方式被请求。之后，你可能会增加通过POST添加诸如资源的方法,通过PUT方法修改资源，通过DELTE方法删除资源。支持更多HTTP方法的同时，你不能中断现有的用户。换句话说，支持DELETE方法删除资源的同时，不能改变旧有的GET获取资源的功能。 那么，哪些事件类型会破坏现有的客户端？ 从数据类型中移除字段会破坏现有客户端。客户端应用的开发人员应该处理丢失的值，但事实上他们很少这么做。结果他们的代码失效了，产生各种bug。 对现有的字段排序或者新增字段会破坏依赖字段位置的客户端。例如，客户端依赖于FirstName这个字段，而这个字段在位置0上，但是布局把FirstName更改成了其它位置，客户端崩溃了，用户抱怨（系统不能正常工作）。再次声明：客户端代码要基于名字查找，虽然基于索引的代码更容易编写。 重命名一个存在的字段也会导致客户端崩溃。将FirstName修改成firstname会使大小写敏感的客户端崩溃。大多数人使用XML或者JSON传递数据，它们都是大小写敏感的。若你使用其它方式（非大小写敏感）传递数据，牢记这一点尤为重要。 更改URL结构同样导致客户端崩溃。在你移动的资源之后，客户端将无法使用旧的url请求到它们。 处理以上问题的最简单的方式是改变URL结构。”正确”的处理方式是使用HTTP状态码301，表示永久重定向。这么做通过响应HTTP Location头信息会告诉请求端（通常是浏览器）资源的新位置。 这么说吧，闭关不是所有的HTTP客户端都足够智能到自动跟随跳转。如果你无法控制现有的HTTP客户但，一些，通常是许多，会被写成禁止跟随跳转。在这个例子中，我建议尼保留旧的URL架构不动同时用代码来实现基本的新旧URL结构。旧的URL结构能被网络设备处理，同时也能被代码处理。如果你们有IT部门，跟负责负载均衡和web服务器的团队合作找到正确的方案。 在重命名，重排序，新增，或者删除字段上，我有两个建议，很大程度上取决于谁在使用你们的服务。我们首先看看REST主义者：他们阅读过Roy Fielding的博士论文而且真正懂得Web的工作原理。第二条建议着重于怎么处理版本控制，当你的目标客户对简单的实现更感兴趣而不是最大化互联网架构的使用。 一种学派支持所谓的HATEOAS：超媒体（Hypermedia）或者说应用引擎状态（Engine of Application State）。HATEOAS学派认为应该使用HTTP Accept和Content-Type头信息来处理和描述版本控制下的数据。Accept信息描述了请求这需要的内容的类型。而HTTP 消息（HTTP message）包含了数据，Content-Type描述了消息中数据的类型。 这些头部信息的值是MIME类型,互联网名称与数字地址分配机构维护着可接收的MIME类型列表。作为供应商，你也可以使用vnd前缀创建自己的MIME类型。例如，假设尼打算暴露foo这个数据类型，并且你的域名是example.com，你可以为该数据定义如下的MIME类型： 12vnd.example-com.foo+xml 在XML中表示foo这个数据vnd.example-com.foo+json 在JSON中表示foo这个数据 之后，任何人在请求你的服务时，只要他们建立了HTTP连接并且设置了正确的Accept头部MIME类型，以用户请求的格式返回的响应内容中会包含foo值。如果你对foo数据做了版本控制，那么MIME类型中就包含版本信息。 例如，对于版本1.0,1.1和1.2，JSON格式的foo数据需要参照如下方式设置Accept/Content-type头信息： 1231.0:vnd.example-com.foo+json;version=1.01.1:vnd.example-com.foo+json;version=1.12.0:vnd.example-com.foo+json;version=2.0 所有的HTTP协议栈（HTTP stacks）都有读写HTTP Accept和Content-Type头信息的技术。例如，jQuery中我会这么写来请求版本1.1的foo数据JSON对象: 1234567891011$.ajax(&#123; beforeSend: function(req)&#123; req.setRequestHeader(&#x27;Accept&#x27;, &#x27;vnd.example-com-foo+json;version=1.1&#x27;); &#125;, type: &#x27;GET&#x27;, url: &#x27;http://http:/~/12&#x27;, success: function(data)&#123; /* code elided */ &#125;, dataType: &#x27;json&#x27;&#125;); 在服务端，你的代码需要查看accept类型并根据指定的请求版本写出客户端期望的字段。服务端必须基于URL和返回的Content-Type设置vary header来说明响应是可缓存的，例如： 1Vary: Content-Type 通过使用Vary header，确保互联网架构能够精确的缓存响应。否则，依赖这种架构的服务器会缓存XML表现形式并在用户请求JSON的时候返回它。使用MIME类型，你可以处理任何版本的资源表现形式，仅需要在一个url上支持一个大家都知道的MIME类型。在你变更实现方式的时候，接收端（receiving endpoint）需要知道怎样读写返回的表现形式。 不幸的是，这最后一个bit对代码相当的敏感。用来接收的应用程序需要深入哇据HTTP Accept头并决定以何种格式书写响应。在一次请求中，在解码数据之前，例如在PUT或POST时，接收端需要检查Content-Type。 NOTE 只发送一种Accept类型的消息发送者通常不许要设置Accept类型，因为响应的Content-Type中应该包含了Accept type。 多数常用的web框架例如Django，Microsoft ASP.NET, Microsoft WCF,以及基于PHP的框架，没有这样的技术来自动处理基于Content-Type的序列操作(serialization)。作为替换手段，开发人员不得不手写那些代码。客户端框架来发送和接收消息也成为可能，但设置HTTP Accept头不总是简单的,但设置url确实简单多了。 这是我们回到我的第二个建议上 —— 易用至上。URL最大。 创建web services的第二种方式是保持URL至上（TUK原则）。遵循这一模式的开发者称他为REST，因为他们听说过REST并且在一定程序上它和HTTP相关，但他们并不烦恼于阅读Fielding论文。幸运的是，这个学派唯一的过错在于把他们所做的东西叫做REST。为避免刺激REST主义者，我称之为TUK。 在TUK中，也需要通过url在标识资源。在管理资源时，使用表转的HTTP方法：读取使用GET，新建使用POST，更新使用PUT，删除使用DELETE 在这一点上，和REST是不同的。我们诸如上文提到的因改而产生的崩溃（breaking changes）。新增，删除，重排序，和重命名字段造成了一个某处的更改崩溃（breaking change）。你在这个世界创建一个新的版本，要更改URL结构。通常，你需要同时对大量成块的（largely chunks of）对象做版本控制。相应的，foo对象的1.0，1.1，2.0版本长的像这样： 1231.0 http://www.example.com/~/foo1.1 http://www.example.com/~/foo2.0 http://www.example.com/~/foo 对本版号来说，一个可接受的选择是使用时间戳。如果你们使用时间来做版本控制，这样的结构也可以工作： 123June 2008: http://www.example.com/~/fooOctober 2009: http://www.example.com/~/fooFebruary 2010: http://www.example.com/~/foo 如果你使用时间做版本控制，记住把年放在月的前面，并使用2个数字的月份。这样对版本排序会简单的多。 TUK风格有另外一个特征：Accept很少使用。相反，终端（endpoint）依赖查询字符串中的特定的格式来决定请求的Content-Type和期望返回的Content-Type。 为了便利，默认的格式是JSON。例如： 12Request the resource as XML: http://www.example.com/~/foo?format=xmlRequest the resource as JSON: http://www.example.com/~/foo?format=json Portable Contacts和开放社区都使用这种模式，因为理解起来十分简单。 开发人员倾向于使用url做版本控制。在url上标记版本使人一眼就能知道当前使用的服务版本号。只需要看一下请求的http url，什么都清楚了。 在实现你的代码的时候，应该在一个中心区域来实现业务逻辑。不同版本的监听者应该知道在对象的业务逻辑表现形式和对外的表现形式之间进行转换。简而言之，保持HTTP部分的简洁以便统一的修改代码和更容易支持完全不同的客户端 Summary 在你重排序，重命名，新增或删除字段时，需要新建版本。通过改变表现形式，使得用户原有的解析数据的方式失效了。如果你的客户有结构话的想法并且对REST很了解，你应该使用应用程序接收的MIME类型对数据的表现形式做版本控制。如果你的客户将url作为最重要的一面，将url的设计作为版本控制的核心。那么熟悉WS-* web services的人倾向于在变更版本时改变url。 两种技术都是可行的。你要了解你的客户以便选中哪一种方式。一句话，企业和学术范倾向于REST版本控制。如果你的客户是小型业务，而用户都是hacker心理的人，追随TUK的道路。 原文Versioning REST ServicesScott Seely explains when developers need to create a new endpoint versus just adding data. This higher level, non-language specific article explains how to version REST services and when each choice makes sense. Versioning is a perennial issue in the development of multi-tier applications. Whether you are updating a remote procedure call, changing a COM object, or updating a Web service, you need to think about how to support existing consumers while providing new functionality. With each new technology, we have to revisit versioning and come up with new recommendations for how to handle change. In this article, we look at the types of events that cause you to create a new version in a REST service. We then look at two approaches to deploying versions. We only consider HTTP REST. REST can be implemented as an architectural style on many different protocols. As a practical matter, most of us choose HTTP technology as a cornerstone in our REST architectures.Creating New Versions When creating a REST service, the first iteration in the project likely exposes the resource as read-only. In HTTP terms, this means that the project called for you to support GET on the URL. Later on, you might add the ability to add new resources through POST, update resources through PUT, and delete resources through DELETE. By supporting more HTTP methods, you cannot break existing clients. Put another way, the act of supporting DELETE does not change an older client’s ability to GET the resource. What types of events do break existing clients, then? Removing a field from a data type breaks clients. Developers who write client apps should handle missing values, but they frequently don’t. Their code breaks. And they file bugs against you. Repositioning existing fields or adding a field to a data type. This will break existing clients that rely on the field position. For example, if the client relies on the field FirstName being in position 0 and a layout changes that field to any other position, clients break and users complain. Again: The client code should do name-based lookups, but code that uses indexes may be easier to write for some folks. Renaming an existing field. Changing FirstName to firstName will break clients that rely on case sensitivity. Most of you transmit data as XML or JSON; both of these are case sensitive. If you transmit data in some other (non-case sensitive) form, this is even more important to keep In mind. Updating your URL structure. If you move the resource, existing clients won’t be able to get to them. The easiest one of these to handle is changes to URL structure. The “right way” to handle this is to use the HTTP status code 301, Moved Permanently. Doing so tells the caller the new home for the resource via the HTTP Location header in the response. That said, not all HTTP clients are smart enough to automatically follow redirects. If you do not control the existing HTTP clients, some, perhaps many, were written to not follow redirects. In this case, I advise you to keep the old URL structure in place and write the underlying implementation to handle the old and new URL structure with the same code. The old URL structure can be handled with networking equipment as well as with code. If you have an IT department to call on, work with the teams that own the load balancer and web servers to get the right rules in place. I have two recommendations for renaming, reordering, adding, or deleting fields. They largely depend on who consumes your service. We first look at the RESTafarians: folks who read Roy Fielding’s PhD dissertation and really understand how the Web works. The second recommendation addresses how to handle versioning when your target clients are more interested in simplicity of implementation than in maximizing their use of the architecture of the Internet.Hypermedia as the Engine of Application State One school of thought follows what is called HATEOAS: Hypermedia as the Engine of Application State. HATEOAS says that you should use the HTTP Accept and Content-Type headers to handle versioning of data as well as describing data. Accept states the type of content the requester would like to get. When an HTTP message contains data, Content-Type states the type of content in that message. The values in these headers are Multipurpose Internet Mail Extensions (MIME) types. The Internet Corporation for Assigned Names and Numbers maintains the list of accepted MIME types. As a vendor, you can also create your own MIME types using the vnd prefix. For example, if you are exposing the foo data type and your company is example.com, you can define the following MIME types for the data: 12vnd.example-com.foo+xml for the XML representation of foo datavnd.example-com.foo+json for the JSON representation of foo data Then, whenever anyone requests data from your service, they create an HTTP request and set the Accept header to the correct MIME type. The response contains the data in the user requested format. As you version the foo data type, allow for the MIME type information to include version data. For example, for versions 1.0, 1.1, and 2.0 of the foo data type as JSON set the Accept/Content-Type header as follows: 1231.0: vnd.example-com.foo+json; version=1.01.1: vnd.example-com.foo+json; version=1.12.0: vnd.example-com.foo+json; version=2.0 All the HTTP stacks have a mechanism to read and set the HTTP Accept and Content-Type headers. For example, in jQuery I would write the following to request version 1.1 of the foo object as JSON: 1234567891011$.ajax(&#123; beforeSend: function (req) &#123; req.setRequestHeader(&quot;Accept&quot;, &quot;vnd.example-com.foo+json; version=1.1&quot;); &#125;, type: &quot;GET&quot;, url: &quot;&lt;a href=&#x27;http://http://www.example.com/foo/12&#x27;&quot;, success: function (data) &#123; /* code elided */ &#125;, dataType: &quot;json&quot;&#125;); On the server, your code needs to look at the accept type and handle writing out only the fields that the client expects, depending on which version of foo was requested. The server has to set the HTTP Vary header to say that the response is cacheable based on the URL plus the returned Content-Type, as follows: Vary: Content-Type By using the Vary header, you make sure that the Internet architecture can accurately cache the response. Otherwise, servers that rely on the architecture could cache the XML representation and return that when the caller asks for JSON. Using the MIME type, you can handle any version a resource representation by supporting a well know set of MIME types on a single URL. As you change the implementation, the receiving endpoint needs to know how to read and write the representations as requested. Unfortunately, this last bit can be fairly code intensive. The receiving application needs to dig into the HTTP Accept header and determine which formatting should be used to write the response. Before decoding data in a request, such as in a PUT or POST, the receiver needs to look at the Content-Type. NOTE a message sender that only sends one Accept type typically does not need to do this, since the response Content-Type should match the Accept type. Many popular web frameworks such as Django, Microsoft ASP.NET, Microsoft WCF, and those built on PHP do not have mechanisms to handle serialization based on Content-Type automatically. Instead, the developer has to write that code. The client frameworks to send and receive messages also make it possible, but not always simple, to set the HTTP Accept header. They do make it easy to set the URL. This brings us to my second recommendation – where ease of use is paramount.The URL is King A second way to create Web services is to observe that The URL is King (TUK). Developers who follow this pattern call it REST because they heard that REST and HTTP are somehow related, but they couldn’t be bothered to read the Fielding dissertation. Fortunately, the only thing this school is guilty of is calling what they do REST. So as to not incite the RESTafarians, I call it TUK. In TUK, you still identify resources by their URLs. When manipulating resources, use the standard HTTP methods: 1234GET to readPOST to createPUT to updateDELETE to remove At this point, we depart from REST. We have the same breaking changes as before. Adding, removing, reordering, and renaming fields constitutes a breaking change for someone somewhere. When you create a new version in this world, change the URL structure. Typically, you version large chunks of your objects at a time. Our foo object version for 1.0, 1.1, and 2.0 looks like this, instead: 1231.0: &lt;a href=&quot;http://www.example.com/app/1.0/foo&quot;&gt;http://www.example.com/app/1.0/foo&lt;/a&gt;1.1: &lt;a href=&quot;http://www.example.com/app/1.1/foo&quot;&gt;http://www.example.com/app/1.1/foo&lt;/a&gt;2.0: &lt;a href=&quot;http://www.example.com/app/2.0/foo&quot;&gt;http://www.example.com/app/2.0/foo&lt;/a&gt; An acceptable alternative to version numbers are date stamps. If your organization handles versioning by date, the following would also work: 123June 2008: &lt;a href=&quot;http://www.example.com/app/2008/06/foo&quot;&gt;http://www.example.com/app/2008/06/foo&lt;/a&gt;October 2009: &lt;a href=&quot;http://www.example.com/app/2009/10/foo&quot;&gt;http://www.example.com/app/2009/10/foo&lt;/a&gt;February 2010: &lt;a href=&quot;http://www.example.com/app/2010/02/foo&quot;&gt;http://www.example.com/app/2010/02/foo&lt;/a&gt; If you version by date, always put the year before the month, and use two digit months. This makes it easy to sort the versions visually. The TUK style has another characteristic: Accept is used rarely, if ever. Instead, endpoints rely on a format parameter in the query string to determine the content type of the request and the desired response content type. By convention, the default value for format is json. Examples: 12Request the resource as XML: &lt;a href=&quot;http://www.example.com/app/1.0/foo?format=xml&quot;&gt;http://www.example.com/app/1.0/foo?format=xml&lt;/a&gt;Request the resource as JSON: &lt;a href=&quot;http://www.example.com/app/1.0/foo?format=json&quot;&gt;http://www.example.com/app/1.0/foo?format=json&lt;/a&gt; Both Portable Contacts and OpenSocial use this pattern because it is so easy for people to understand. Developers tend to prefer versioning by URL. Versioning by URL allows them to figure out which version of the service is in use at a glance. Just look at the HTTP request URL, and you know everything! When implementing your code, you should keep the business logic in one central location. The various listeners for each version should know how to transpose between the business logic representation of the object and the external representation. In general, keep the HTTP part fairly thin and simple so that you can fix code centrally and support disparate clients easily.Summary You have a new version of your service whenever you reorder, rename, add, or delete fields. By changing the representation, you invalidate the assumptions consumers have already made about how to interpret the data. If your audience is architecturally minded and aware of REST, you should version data representations in the MIME types your application accepts. If your clients view the URL as the most important facet, make the URL the center of your versioning efforts. Folks who are familiar with versioning with WS-* Web services tend to be more comfortable with changing the URL when versions change. Both mechanisms are valid. You need to know your consumer to know which path to follow. In general, working with enterprises and academically-minded folks tends to point developers towards REST versioning. If your clients are smaller businesses and users with a hacker mentality, follow the TUK approach.","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Translation","slug":"Translation","permalink":"https://straysh.github.life/tags/Translation/"}]},{"title":"mysql_MySQL权威指南笔记（二）","slug":"2013-mysql-MySQL权威指南笔记（二）","date":"2013-10-03T06:09:06.000Z","updated":"2019-12-01T10:13:39.540Z","comments":true,"path":"2013/10/03/2013-mysql-MySQL权威指南笔记（二）/","link":"","permalink":"https://straysh.github.life/2013/10/03/2013-mysql-MySQL%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/","excerpt":"","text":"接着刚才的话题。FROM子句中的表引用table reference是可以用表表达式table expression来替换的(这两者本身也是等价的) 123456SELECT * FROM `stu` WHERE `name`=&#x27;Jack&#x27;等价于：SELECT * FROM (SELECT * FROM `stu`) tmp WHERE `name`=&#x27;Jack&#x27;//注意别名的使用上面用法是毫无意义的,只是为说明语法，看下面的例子：SELECT tmp.`name`,`course_name`FROM (SELECT * FROM `stu` WHERE `age`&amp;gt;30) tmp,`course` cWHERE tmp.`id`=c.`sid` SELECT语句：WHERE子句FROM子句结束后，执行的就是WHERE子句。WHERE子句过滤出了符合条件的临时表中的行。说的更仔细：WHERE子句过滤掉（删除）了谓词为false或者unknown的行。 假设stu表中有一行记录 12 Yoyo (NULL) 即id字段为数值12，name字段为字符’Yoyo’，age字段为NULL。NULL值在MySQL中就是不可测定的，不可知的。因此在执行WHERE age&gt;30 的测试时，返回的是unknown，既不是true也不是false。 WHERE子句可以包含以下条件： 比较运算符 AND、OR、XOR、NOT 带有列表的IN BETWEEN LIKE REGEXP MATCH NULL ANY、ALL EXISTS 比较运算符包括：=等于、&lt;小于、&gt;大于、&lt;=小于等于、&gt;=大于等于、&lt;&gt;不等于，以及不常用的!=不等于、&lt;=&gt;相等或者都等于空。对于带有列表的比较运算符：例如(x,y)=(1,3)，MySQL在内部将其转化为 (x=1) AND (y=3);(x,y)&gt;(1,3),内部转化为(x&gt;1) OR (x=1 AND y&gt;3)。MySQL在内部的转换和我们自然的认为(x&gt;1) AND (y&gt;3)是不同的！！。鉴于这种列表式的写法最终还是会被MySQL转换为常规的AND、OR，建议直接使用这种常规的写法，也不容易出现错误。 有一种比较特殊的关联子查询(correlated subquery)，看例子： 123SELECT c.course_nameFROM course cWHERE 30 &amp;lt; (SELECT s.age FROM stu s WHERE s.id=c.sid) 关联子查询的特点是，子查询的结果依赖主表的行。例如上例中的SELECT s.age FROM stu s WHERE s.id=c.sid，年龄的查询依赖于主表的sid，即主表没扫描一行，子查询就要重新计算一次。 逻辑AND、OR、XOR、NOT就不需要多说了。 IN操作符后面需要跟一个列表，一个使用（）圆括号包起来的以,逗号分隔的值 MATCH用在全文检索，不讨论。 前面说过，NULL在MySQL中是不可测定的，未知的值。所以诸如WHERE col=NULL这样的语法是错误的。而应该写作：WHERE col IS NULL、WHERE col IS NOT NULL ALL、ANY、SOME通常用在比较运算符有操作数为列表的情形。其中SOME和ANY是同义词。看下面的例子： 12345SELECT s.id,s.name FROM stu s WHERE s.id &amp;gt; ALL (1,2,3)//条件为s.id大于(1,2,3)中的每一个值SELECT s.id,s.name FROM stu s WHERE s.id &amp;gt; ANY (1,2,3)//条件为s.id大于(1,2,3)中的任意一个值SELECT s.id,s.name FROM stu s WHERE s.id &amp;gt; ALL (SELECT c.sid FROM course c WHERE c.course_name=&#x27;Math&#x27;)//条件为s.id大于子查询中的每一个值 EXISTS通常用在关联子查询中。用来作为测试条件。我们改造一下之前correlated subquery的例子： 123SELECT c.course_nameFROM course cWHERE EXISTS (SELECT * FROM stu s WHERE s.id=c.sid AND s.age&amp;gt;30) 最终的结果是一样的。当WHERE测试为true时，主表的select就会返回相应的结果。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]},{"title":"demo blog入口分析","slug":"2013-demo-blog入口分析","date":"2013-09-29T02:44:48.000Z","updated":"2020-02-26T04:16:54.407Z","comments":true,"path":"2013/09/29/2013-demo-blog入口分析/","link":"","permalink":"https://straysh.github.life/2013/09/29/2013-demo-blog%E5%85%A5%E5%8F%A3%E5%88%86%E6%9E%90/","excerpt":"","text":"首先搭建demo blog环境，更改数据库连接如下 12345678&#x27;db&#x27;=&gt;array( &#x27;connectionString&#x27; =&gt; &#x27;mysql:host=localhost;dbname=blog&#x27;, &#x27;emulatePrepare&#x27; =&gt; true, &#x27;username&#x27; =&gt; &#x27;root&#x27;, &#x27;password&#x27; =&gt; &#x27;123456&#x27;, &#x27;charset&#x27; =&gt; &#x27;utf8&#x27;, &#x27;tablePrefix&#x27; =&gt; &#x27;tbl_&#x27;,), 从http://www.blog.test/index.php?r=post/index访问demo blog。下面我们来一步步分解yii的执行流程： 12345$yii=dirname(__FILE__).&#x27;/../yii/framework/yii.php&#x27;; //加载yii框架$config=dirname(__FILE__).&#x27;/protected/config/main.php&#x27;; //指定配置文件路径// defined(&#x27;YII_DEBUG&#x27;) or define(&#x27;YII_DEBUG&#x27;,true); //debug模式，默认关闭require_once($yii); //包含yii核心文件Yii::createWebApplication($config)-&gt;run(); //这里是关键，每一次访问都从这里开始 最后一行代码分解为2部分来看:Yii::createWebApplication()和run()。Yii这个类定义在框架目录的根上yii.php。很明显这个类只是对YiiBase.php的一个包装，我们可以在yii.php按自己的需求定制。追着YiiBase.php，首先请大家快速的预览一下该文件。发现从L14-L43有着大量的常量定义。 12345678910111213141516//这里定义了app的开始时间。注意microtime(true)返回浮点数，省去了自己拼接的麻烦defined(&#x27;YII_BEGIN_TIME&#x27;) or define(&#x27;YII_BEGIN_TIME&#x27;,microtime(true));//是否开启debug模式，默认关闭defined(&#x27;YII_DEBUG&#x27;) or define(&#x27;YII_DEBUG&#x27;,false);//定义了Yii::trace()需要记录的堆栈调用(call stack information)信息(文件名和行号)。//默认0，即不记录任何回溯信息(backtrace information)，大于0时，至多记录到该定义//级别的call stacks信息(详细参看YiiBase::L460 log函数)defined(&#x27;YII_TRACE_LEVEL&#x27;) or define(&#x27;YII_TRACE_LEVEL&#x27;,0);//是否开启异常处理，默认开启defined(&#x27;YII_ENABLE_EXCEPTION_HANDLER&#x27;) or define(&#x27;YII_ENABLE_EXCEPTION_HANDLER&#x27;,true);//是否开启错误处理，默认开启defined(&#x27;YII_ENABLE_ERROR_HANDLER&#x27;) or define(&#x27;YII_ENABLE_ERROR_HANDLER&#x27;,true);//定义框架根路径defined(&#x27;YII_PATH&#x27;) or define(&#x27;YII_PATH&#x27;,dirname(__FILE__));//定义zii根路径defined(&#x27;YII_ZII_PATH&#x27;) or define(&#x27;YII_ZII_PATH&#x27;,YII_PATH.DIRECTORY_SEPARATOR.&#x27;zii&#x27;); 接下来，继续追查YiiBase::createWebApplication() 12345678public static function createWebApplication($config=null)&#123; return self::createApplication(&#x27;CWebApplication&#x27;,$config);&#125;public static function createApplication($class,$config=null)&#123; return new $class($config);&#125; 最终产生了一个CWebApplication实例，并调用了父类CApplication的构造方法 1234567891011121314151617181920212223242526272829303132333435public function __construct($config=null)&#123; Yii::setApplication($this); //奇特的单例方法 // set basePath at early as possible to avoid trouble if(is_string($config)) $config=require($config);//包含配置文件 if(isset($config[&#x27;basePath&#x27;])) &#123; $this-&gt;setBasePath($config[&#x27;basePath&#x27;]);//设置Yii::app()-&gt;basePath属性 unset($config[&#x27;basePath&#x27;]); &#125; else $this-&gt;setBasePath(&#x27;protected&#x27;);//注意：这里表明最好在配置文件中显示的定义basePath,否则系统会使用默认值 Yii::setPathOfAlias(&#x27;application&#x27;,$this-&gt;getBasePath());//3个别名设置，又认识了几个别名 ^_^ Yii::setPathOfAlias(&#x27;webroot&#x27;,dirname($_SERVER[&#x27;SCRIPT_FILENAME&#x27;])); Yii::setPathOfAlias(&#x27;ext&#x27;,$this-&gt;getBasePath().DIRECTORY_SEPARATOR.&#x27;extensions&#x27;); $this-&gt;preinit();//父类CModule中的一个空protect方法 $this-&gt;initSystemHandlers();//初始化异常句柄和错误句柄 TODO待研究 $this-&gt;registerCoreComponents();//初始化核心组件。CPhpMessageSource、CDbConnection、CPhpMessageSource、 //CErrorHandler、CSecurityManager、CStatePersister、CUrlManager、CHttpRequest、CFormatter TODO待研究 $this-&gt;configure($config);//call CModule::configure() 迭代剩余的配置信息 $this-&gt;attachBehaviors($this-&gt;behaviors);//绑定事件 TODO待研究 $this-&gt;preloadComponents();//加载静态的app组件。不知道干嘛的 TODO待研究 $this-&gt;init();//调用了实例CWebApplication::init()&#125;protected function init()&#123; parent::init();//调用爷爷类CModule::init(),这是一个空方法。即这行代码执行后 什么都没发生。 // preload &#x27;request&#x27; so that it has chance to respond to onBeginRequest event. $this-&gt;getRequest();//调用父类CApplication::getRequest()&#125;public function getRequest()&#123; return $this-&gt;getComponent(&#x27;request&#x27;);//返回一个CHttpRequest组件(核心组件)的实例&#125; 到现在，Yii::createWebapplication执行完毕，接下来执行了父类CApplication::run()方法 1234567891011121314151617181920public function run()&#123; if($this-&gt;hasEventHandler(&#x27;onBeginRequest&#x27;)) $this-&gt;onBeginRequest(new CEvent($this));//绑定onBeginRequest方法 $this-&gt;processRequest();//开始处理http请求 if($this-&gt;hasEventHandler(&#x27;onEndRequest&#x27;)) $this-&gt;onEndRequest(new CEvent($this));//绑定onEndRequest方法&#125;public function processRequest()&#123; if(is_array($this-&gt;catchAllRequest) &amp;&amp; isset($this-&gt;catchAllRequest[0])) &#123; $route=$this-&gt;catchAllRequest[0]; foreach(array_splice($this-&gt;catchAllRequest,1) as $name=&gt;$value) $_GET[$name]=$value; &#125; else $route=$this-&gt;getUrlManager()-&gt;parseUrl($this-&gt;getRequest()); $this-&gt;runController($route);//解析完url，执行post/index，到此结束&#125;","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"Yii","slug":"Yii","permalink":"https://straysh.github.life/tags/Yii/"}]},{"title":"mysql_MySQL权威指南笔记（一）","slug":"2013-mysql-MySQL权威指南笔记（一）","date":"2013-09-29T02:44:10.000Z","updated":"2019-12-01T10:13:40.452Z","comments":true,"path":"2013/09/29/2013-mysql-MySQL权威指南笔记（一）/","link":"","permalink":"https://straysh.github.life/2013/09/29/2013-mysql-MySQL%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"SELECT语句的常用元素 Literal Expression Column specification User variable System variable Case expression Scalar function Null value Cast expression Compaund expression Row expression Table expression Aggregation function 一个table expression通常得到一个值，该表达式用在SELECT中以及SELECT语句的WHERE子句中。 我们通常按照表达式的数据类型或其值的复杂性或其形式来给表达式分类。表达式的数据类型和MySQL的数据类型是一致的，也包含数值型、字符型、时间型、十六进制和布尔类型。这时，表达式有一个确定的值，我们称其为标量scalar value。在按复杂性分类时，这些scalar value所表示的表达式被成为scalar expression。除此之外，还有row expression和table expression。需要注意的是，row expression是列表(有序)，而table expression是集合(无序) 表达式的第三种划分方式是以上面两种为基础的。通常来说，带有运算符的表达式就是compaund expression。与之对应的是单一表达式singular expression。在MySQL中table expression是可复合的(如只用UNION关键字)，而row expression是不可复合的。 SELECT语句的定义12345678910111213&lt;select statement&gt; ::= &lt;table expression&gt;&lt;table expression&gt; ::= &lt;select block head&gt; [&lt;select block tail&gt;]&lt;select block head&gt; ::= &lt;select clause&gt; [&lt;from clause&gt; [&lt;where clause&gt;][&lt;group by clause&gt;][&lt;having clause&gt;]]&lt;select block tail&gt; ::= [&lt;order by clause&gt;][limit clause] 下面我们来分析一下SQL语句的执行过程。 1234567SELECT `id`,`name`//最后一步，select子句从结果集中抽取指定的列，形成最终的结果集并返回FROM `stu`//第一步，from子句指定了查询范围，并copy出一张临时表WHERE `age`&gt;18//第二步，where子句过滤出符合条件的row，缩小的临时表GROUP BY `classid`//第三步，group by子句按指定的字段分组，这些字段在临时表中是唯一的，其他字段会形成列表HAVING count(*)&gt;1//第四步，having子句再次过滤出符合条件的row，缩小了临时表ORDER BY `id`//第五步，order by子句把临时表按指定的字段排序，只改变了临时表的row顺序，不改变最终的结果集LIMIT 10//第六步，limit子句从结果集中抽取指定的rows 上面的SQL语句最终的结果集就是一个table expression，就像之前说的，a table expression is a collection of rows。table expression的定义如下： 1234567&lt;table expression&gt; ::= &#123;&lt;select block head&gt;|(&lt;table expression&gt;)|&lt;compaund table expression&gt;&#125;[select block tail]&lt;compaund table expression&gt; ::= &lt;table expression&gt;&lt;set operator&gt;&lt;table expression&gt;&lt;set operator&gt; ::= UNION 一个table expression可以是一条SELECT语句(singular expression)或者使用()括号将自身包起来，再或者是多个table expression使用一些关键字来复合(eg:UNION)，也就是所谓的compaund expression。而在使用UNION时，必须保证两个表表达式的度是相同的(列数相同)。 一个table expression可以调用另一个table expression(subquery或subselect)。理论上讲，table expression的嵌套可以是无限的。 12345SELECT * FROM (SELECT * FROM (SELECT * FROM (SELECT * FROM `stu`) AS tmp1) AS tmp2) AS tmp3 根据子查询返回的结果集的类型，子查询又被分为table subquery(返回的是table expression)、row subquery(返回的row expression)、column subquery(返回的是只有一列的row expression)、scalar subquery(返回值是一行一列，即一个标量) SELECT语句：FROM子句一个table expression通常是从FROM子句开始执行的。除非它没有FROM子句，例如：select 1+2。FROM子句的定义如下： 1234567&lt;from clause&gt; ::= FROM &lt;table reference&gt;[,&lt;table reference&gt;]...&lt;table reference&gt; ::= &lt;table specification&gt;[[AS] &lt;pseudonym&gt;]&lt;table specification&gt; ::=[&lt;database name&gt; . ]&lt;table name&gt;&lt;pseudonym&gt; ::=&lt;alias name&gt; 定义看起来很复杂，用过MySQL的同学都知道这是很简单的。上面这段相当于：FROM 数据库名.表明 [AS] 别名。AS关键字可以省略。 上面这些规则同样适用于SELECT子句中的列。例如：SELECT 数据库名.表名.列名。需要注意的是，这些限定词都是可以使用别名替代的。例如： 12345SELECT `testdb`.`stu`.`id` FROM `testdb`.`stu` WHERE `id`=1SELECT s.`id` FROM `testdb`.`stu` &#x27;s&#x27; WHERE `id`=1SELECT s.`id` FROM `testdb`.`stu` AS &#x27;s&#x27; WHERE `id`=1错误：SELECT s.`id` &#x27;sid&#x27; FROM `testdb`.`stu` AS &#x27;s&#x27; WHERE sid=1//在同一个select block head中列别名是不可用的 最后，剩下FROM子句的一个难点，JOIN。具体看下图的分析，应该比较明了了。页面太长了 - -、，换下一章。","categories":[{"name":"博文","slug":"博文","permalink":"https://straysh.github.life/categories/%E5%8D%9A%E6%96%87/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://straysh.github.life/tags/MySQL/"}]}]}